{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reOSqhDYGkS7"
      },
      "source": [
        "# Qwen/Qwen2.5-3B-Instruct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBdlmwE51TGE"
      },
      "source": [
        "### Google colab code for rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIuo2nMA1X4r"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# 🔎 Search terms (edit if you renamed the file)\n",
        "TARGET_BASENAME = \"Qwen2.5-3B-Instruct.ipynb\"   # exact filename you told me\n",
        "ALSO_MATCH_SUBSTR = \"qwen\"                      # fallback: find any .ipynb containing this\n",
        "\n",
        "roots = [\"/content/drive/MyDrive\", \"/content/drive/Shared drives\"]\n",
        "hits_exact, hits_sub = [], []\n",
        "\n",
        "def walk(root):\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        # skip super noisy system dirs\n",
        "        if any(x in dirpath for x in (\"/.Trash\", \"/.ipynb_checkpoints\", \"/System Volume\", \"/.shortcut-targets-by-id\")):\n",
        "            continue\n",
        "        for f in filenames:\n",
        "            if f.lower().endswith(\".ipynb\"):\n",
        "                p = Path(dirpath) / f\n",
        "                # exact (case-insensitive) filename match\n",
        "                if f.casefold() == TARGET_BASENAME.casefold():\n",
        "                    hits_exact.append(p)\n",
        "                # substring match for exploration\n",
        "                if ALSO_MATCH_SUBSTR and ALSO_MATCH_SUBSTR.casefold() in f.casefold():\n",
        "                    hits_sub.append(p)\n",
        "\n",
        "for r in roots:\n",
        "    if os.path.exists(r):\n",
        "        walk(r)\n",
        "\n",
        "print(\"=== Exact filename matches ===\")\n",
        "if hits_exact:\n",
        "    for i, p in enumerate(hits_exact, 1):\n",
        "        print(f\"{i}. {p}\")\n",
        "else:\n",
        "    print(\"None\")\n",
        "\n",
        "print(\"\\n=== Substring matches (to help you spot it) ===\")\n",
        "if hits_sub:\n",
        "    for i, p in enumerate(hits_sub, 1):\n",
        "        print(f\"{i}. {p}\")\n",
        "else:\n",
        "    print(\"None\")\n",
        "\n",
        "print(\"\\nTIP: In the left Files pane, right-click your notebook → Copy path, and paste it here if needed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvyV7UvCHDKZ"
      },
      "source": [
        "###Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvWBeDlGGHUT",
        "outputId": "95c7a985-defc-43a1-9fb4-083b84881de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Sep  5 09:01:37 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!pip -q install \"transformers>=4.43.3\" \"trl>=0.9.6\" \"peft>=0.12.0\" \"accelerate>=0.33.0\" bitsandbytes datasets sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fe_24GWeGdR_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojlG2PJ4GyER"
      },
      "source": [
        "###Load the base model (Qwen/Qwen2.5-3B-Instruct) + setup LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKSuZFvRGzMe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8_0x829G1D6"
      },
      "outputs": [],
      "source": [
        "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnFEl5FHI0fJ"
      },
      "source": [
        "**🔹 Why we picked this config**\n",
        "\n",
        "We are on Colab Free (Tesla T4, 16 GB VRAM).\n",
        "A normal Qwen2.5-3B model in half precision (fp16) needs ~12–14 GB just to load. That leaves almost no room for training (gradients, optimizer, dataset). You’d hit OOM (out of memory) quickly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e292JN6I40B"
      },
      "source": [
        "**So we do:**\n",
        "\n",
        "load_in_4bit=True → shrinks model weights by ~4×.\n",
        "\n",
        "bnb_4bit_quant_type=\"nf4\" → keeps accuracy high, better than plain fp4.\n",
        "\n",
        "bnb_4bit_use_double_quant=True → squeezes memory further, at tiny cost.\n",
        "\n",
        "bnb_4bit_compute_dtype=torch.bfloat16 → still computes in higher precision so training doesn’t collapse.\n",
        "\n",
        "Result: the 3B model fits comfortably in T4 memory, and you can actually fine-tune it with LoRA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_CegVFHI-_A"
      },
      "source": [
        "🔹 **What happens if we don’t do this?**\n",
        "Case 1: Load in fp16 (no quantization)\n",
        "\n",
        "VRAM use: ~12–14 GB just for weights.\n",
        "\n",
        "With optimizer + gradients, you need ~20+ GB → T4 crashes (OOM).\n",
        "\n",
        "You couldn’t fine-tune at all.\n",
        "\n",
        "Case 2: Load in int8 (8-bit)\n",
        "\n",
        "VRAM use: ~8–10 GB.\n",
        "\n",
        "Might just fit, but leaves very little for training.\n",
        "\n",
        "Training can still OOM unless you keep batch size = 1 and seq length tiny.\n",
        "\n",
        "Accuracy slightly better than 4-bit, but slower and heavier.\n",
        "\n",
        "Case 3: Load in int4 (our config)\n",
        "\n",
        "VRAM use: ~4–5 GB.\n",
        "\n",
        "Leaves ~10 GB free for training.\n",
        "\n",
        "Stable with nf4 + bfloat16.\n",
        "\n",
        "Perfect for Colab Free."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrAKiyi0G-hV"
      },
      "outputs": [],
      "source": [
        "bnb_cfg = BitsAndBytesConfig(\n",
        "                              load_in_4bit =True,                    # use 4-bit to fit model save GPU since we are in google colab free\n",
        "                              bnb_4bit_use_double_quant=True,        # extra memory saving\n",
        "                              bnb_4bit_quant_type=\"nf4\",             # recommended format\n",
        "                              bnb_4bit_compute_dtype=torch.bfloat16  # safe compute type\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "83562a8f0b0c45e8a5030a23b661fd1e",
            "5d83704761a544d3a367aa3a80ce4678",
            "dd179e67df994df98aea0503d53f2508",
            "3e49c9a0cd674e91a49677e02fe25349",
            "5a5c1a23e8c04c0a8f7ad0e8478d818e",
            "a551c5c09bc24bf49f1d340327324b8d",
            "03ebbbab875149ad9a091aef42514c07",
            "491636ddf03640eb8a57e7366141c0ee",
            "7c67206c88c649c6981d35ae535267d8",
            "c909dc023f0d4df288ad780ae1c87b91",
            "3366ce9468f646978748aefb3b6ae6a7",
            "84387e4b76254c7d98fa92d2aa1c8b31",
            "6d6ba9c9205a4f84b34ca61ff5d2a947",
            "697eff2770244e4d835ca342c5e3b7ef",
            "3cbb91cc71374bebace5dd2708aff5ce",
            "a7fd9e01dc6949caa627deb6b4663048",
            "5c774220370d4ca797eac013ea40e838",
            "50de7a0c476d4b36a739ac2cf38ba4ef",
            "6ad22d9d72b24723902c875db431c654",
            "d0a4141aecd349708f53b26d8650b76f",
            "a381350bde854b00a0d105058725e2c2",
            "f21c64d73d41433b8da1ed079dbc3b7d",
            "039a4ed6861c46d7a547d4241c9a8d3f",
            "984e90893afa40a8beb2a9a6f01bfc1a",
            "1f688e8aa54149c7874a499008fc4d9d",
            "47c08a651a2143e98e61b5619f3c3775",
            "cca5bf2cbf314557a965e26b6d5584c3",
            "4ebe0a371db84a848e6eb654e333cdd7",
            "c4a83b5bf1bc4a11bed287ce11783e87",
            "a90e651e1bb04ac2bf0c512c45da2ac1",
            "02c6af7f7cf3474fb1c08aa491bac0a4",
            "71650d4d2d8e4e68b62d8e3f7b67ffca",
            "b17f9a7e74f64de991b5440a748c5c7e",
            "058af4f27adf498b917932ea18944da2",
            "9e1be4cb09d04aa4a6ee8e7b2769e781",
            "348c7f4a262a456c9177e78df9d53758",
            "eda5b5f2676e42469ba83af87ef18e2b",
            "3966295ca855473e9297ed1b8b9d5e79",
            "16b0f34cb54945cb8c0c47c5388a3937",
            "0b0dc86568c94ea3a357985ebc16a08d",
            "79ca9ce6eb794e448645a60a01a423fb",
            "f5eb630d820d426fa344b273f3432409",
            "c32a78ebd9bc4900b328b792774276ab",
            "166a7fb473a84e7494dfff8b4327d66c"
          ]
        },
        "id": "M94P719EJQKI",
        "outputId": "070aa5c1-287d-4445-ecd4-d198645498fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load tokeninzer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL,use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenizer.padding_side = \"right\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xSvU-NVMu3D"
      },
      "source": [
        "**Self-Attention**\n",
        "\n",
        "Figures out which tokens should pay attention to which others.\n",
        "\n",
        "Uses 4 linear projections:\n",
        "\n",
        "q_proj → turns tokens into queries\n",
        "\n",
        "k_proj → turns tokens into keys\n",
        "\n",
        "v_proj → turns tokens into values\n",
        "\n",
        "o_proj → outputs the weighted result\n",
        "\n",
        "**Feed-Forward (MLP)**\n",
        "\n",
        "After attention, tokens go through a big neural network.\n",
        "\n",
        "In Qwen/LLaMA-style models these are:\n",
        "\n",
        "gate_proj\n",
        "\n",
        "up_proj\n",
        "\n",
        "down_proj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bphMrafRNFSJ"
      },
      "source": [
        "MODELS BUILD ON BILIONS AND MILIONS OF PARAMS BUT WE NEED TO TUNE SPECIFIC TO TUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwHlSzOGLEgv"
      },
      "outputs": [],
      "source": [
        "# LoRA Config\n",
        "\n",
        "lora_cfg= LoraConfig(\n",
        "                      r =16,                        # rank (size of adapter matrices)\n",
        "                      lora_alpha= 32,               # scalling factor\n",
        "                      lora_dropout = 0.05,          # helps to avoud overfitting\n",
        "                      bias = None,                  # saves param\n",
        "                      task_type = \"CAUSAL_LM\",      # this is a causal language model\n",
        "                      target_modules =[\n",
        "                           \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",   # attention layers\n",
        "                           \"gate_proj\",\"up_proj\",\"down_proj\"      # feed-forward layers\n",
        "                      ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjJ7oQApOCKm"
      },
      "source": [
        "###Load & prepare dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kp5R4HdNuyG"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392,
          "referenced_widgets": [
            "143eaa3b5bb544c68bff6428952f5fac",
            "6fb6cffc95c34fa3a646aa8cc36e9809",
            "8cce2bf9299342edb59b90d0ebdfd873",
            "7cb148ac76d040f986608296c98f999d",
            "68e16d9692b34e96bc7b5fd08455ead2",
            "c4eaa0ef2ea741009eed248a5a549bd4",
            "e393c515663e4ec4ab10258c26e0ecb2",
            "c39612a9d76146f9aa927aaa0fc5e3cf",
            "4f7276a827a9497dacace5604f08e7da",
            "356e7cb8647c4ed989a8dfe398b4c3cc",
            "2931a9e628ed4d15add22fcf26fe03ee",
            "228ca63af8bd4e7dbd18f6695d5ac8f0",
            "28a59f79c10440e8a0ce0ed2f86fe47c",
            "34a2f969b7dd4c82ac0ee5d6d090f67d",
            "f0b4b3a5782548e68e6270573eaa1b4e",
            "93e437400e63497b8c69edabeab45728",
            "a6c4186e06f74ea6a5c64cb4fa8f304b",
            "3127376b2f18463fbe2bdb540a88149a",
            "99b3e441bfd04877a2e2988307867c10",
            "3b96c002c71347dd80605d0ab7121c15",
            "dcd29c71ae7c46ab8048c5605697e618",
            "b11dff5c8dcd42e08526f660b19c29e7",
            "c8be5290343f4830a80118d13237340f",
            "711aebf8d7554c2fb25c9d24c773a0f3",
            "f7a2a6a86d074ff5b0aa87b0b0772afe",
            "8e1c859d1e574275bd15923aa3478a2f",
            "d288c424471f49aa9bf0c319bc251681",
            "a5f280c01633477ba3829ae81d663508",
            "048e02645d2340458b2766c2192d44d0",
            "8ae199d0d56b4500909dc0ca755a1473",
            "54e29fbfb6dc46a5a71104187ae409e7",
            "9e05ed686776469ba87c6a528b44e688",
            "88ef7f9710c74cc798429d3e3c4b8f9c",
            "f2ce9569c57e491cad54ea6e127153a1",
            "0a7eb40058b0403bacb970bb06398802",
            "5ecc2b1b631747c99b30ad7b9d9dfbfc",
            "83b49a24ad4a474e9175d82b0f665489",
            "2f11dd25e2e341e1836630651d7868fb",
            "26529803a91d405cb82189d43eabe2dc",
            "19aa6480358143fdb31d0faee1bab8e4",
            "8d118dcd394940bdab30202d6f7a67cb",
            "f357cb5cfd064a8bb0428125ca7dd342",
            "0e5c51a9e0f1438998fc6c4f6debb650",
            "73ccba4d78674cf686274900cbf6818a",
            "7f5849b8fd3241a7b3bab14d71074764",
            "cc76a1daf40a4f2a8940b1fd361268e7",
            "9931fec1455f4ac2bc9074f448fff098",
            "f5146e7f0f124fd4a1a8c69ea37d97d3",
            "fafa7695f5b94f0890c3ba3d2f934f07",
            "cce0eda11d224e18a7129970c05d90a8",
            "b516cbc360434135b87391ffcbf2a411",
            "6a7d3cd8f17c4717b377e8020895107b",
            "82efff3c065b4108acfe1ebdd1cff9bd",
            "a7e9062e91ff446e94160dc9bddab600",
            "f65b271718424075ac757515dd0cd055",
            "2e93e70c31e24a7ba4b7e87879c83bd6",
            "6b561f4e0aff4a82807b1ddc3bb0e9b0",
            "141a9f9f8ecd4119b338510069112f4b",
            "61220715444d4e9ca9752a3559c322fd",
            "ae04926381ef4fb5882b200e06f5d7d8",
            "0950b41ddc8a42e09ee8f0dbc61a7adb",
            "b95d00ce248540ea9fc488205682e6eb",
            "1d7d7602bcb14796b6e2f213ff6f0d3a",
            "ad42e48887434fe88d2c397dff621fe6",
            "cd6d2fb37f714178bb46411f36fa2f09",
            "18e7618f27044cd7a335c14dd81cc64f",
            "d9165422965b42d9a12a8c77460440b9",
            "ec65a346f1db4443b90539f4103586f4",
            "a37a8e9a223444ea9323f33e5728c99d",
            "cea611666a07420e9ca6e336c4e3c0cf",
            "98647f6f565241d59999efb315a1ef12",
            "41868c6a9d92429385f541fad430cc20",
            "d954253bb7fb4550a4a2abc25d1a8e3f",
            "776f76fd8d6c4b898a42ebe57b0edd20",
            "4e660ac81a14486ca9319092265f00a5",
            "03b3225baaa448698f1a2e3fa293c078",
            "1bb6db5538f344c6a9b1ae46992cbac4",
            "dab041a4d2db480f9d34a16b3e265c9d",
            "f650fc6f3fc7469d9640c7d67184c295",
            "903e3d86d8144777a9a420291c36dc6b",
            "29ab898ea6ec4ef09501984a2498888a",
            "a56338e7bd814c3c82933de68ddcbbd5",
            "a4e75293302441f591b1698dbe51ac46",
            "be4ea2ef8a0a418fb629f389f831c638",
            "446c72f5a604421491c7a8e306070773",
            "78d17be7da934e9ba17a7f969cec5429",
            "bf2c17ca0b03408ca22b8a46452bc41f",
            "756e04d251d7452fa65ae3613f7fd1ad",
            "54de288d0a77436a901d46d447978714",
            "0a8a4a320fbf45e28df100997bf8d095",
            "8170117df1f045f6b0ddabb73119af9a",
            "e5fd0aaa467746868d5629bc588b193c",
            "b50d57812eb04c7eb187553cb6eecd97",
            "556492f6380042efbdd4e6625bd764c1",
            "e9f6b2bb4b1f444dad67bdca0899aece",
            "74f7dc3645c94d2d9ea766b67dbf1238",
            "ea0e17d546f24b8499856ff68f3cdfcd",
            "7faed28e92264844a7c6a1a0e8dd5b21",
            "9653c946d59e4369bcfa8a494687fc52",
            "d22148aaa8c643e390844afe63f6501b",
            "4245bada941540ecae5f94909445e2d9",
            "ca48dbe387a1421ab0bf654b994d6032",
            "bd398d6aa0ed48f796671a5bdf8e5912",
            "6ef8c4edd9484fcfb7980b7e067b4f12",
            "a8aadacbdd5749ab8718d239010ea685",
            "a633abc031dd418abd7f52bc045ebd61",
            "509b4bf2f79c4e1188f1b72525918703",
            "1d6d43c537ee4c2ca5332f66b5eb5e70",
            "f02ceb0aa6d1423da5e06de0f47817d7",
            "648ddbd8b6284bf1b1980df64b469484"
          ]
        },
        "id": "qaHf4qAdOErq",
        "outputId": "5c0b86d1-bf20-4cfe-bc47-3471c3afb123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Samples: 200 train / 200 val\n",
            "{'instruction': 'Classify the sentiment of this tweet.', 'input': '\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"', 'output': 'positive'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load tweet sentiment dataset\n",
        "raw_ds = load_dataset(\"tweet_eval\", \"sentiment\")\n",
        "\n",
        "# Map numeric labels -> words\n",
        "label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "def to_instruction(example):\n",
        "    return {\n",
        "        \"instruction\": \"Classify the sentiment of this tweet.\",\n",
        "        \"input\": example[\"text\"],\n",
        "        \"output\": label_map[example[\"label\"]]\n",
        "    }\n",
        "\n",
        "# Apply mapping\n",
        "ds = raw_ds.map(to_instruction, remove_columns=raw_ds[\"train\"].column_names)\n",
        "\n",
        "# Keep a small subset (200 train / 200 validation) so it runs on free Colab\n",
        "ds_small = {\n",
        "    \"train\": ds[\"train\"].select(range(min(200, len(ds[\"train\"])))),\n",
        "    \"validation\": ds[\"validation\"].select(range(min(200, len(ds[\"validation\"]))))\n",
        "}\n",
        "\n",
        "print(\"Samples:\", len(ds_small[\"train\"]), \"train /\", len(ds_small[\"validation\"]), \"val\")\n",
        "print(ds_small[\"train\"][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qny4o9hOjED"
      },
      "source": [
        "###Prompt building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rWTkL-zOcYo"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "a12b0f8f2d5f410684da0df92b9624fa",
            "91b5dedd14a94032b5fd7a89e02c42ac",
            "1d001926cd8d494aaf95743e0280d894",
            "28d9e60e701949c294709de5e241635d",
            "61f59f477387434084113f03934d3225",
            "a91126180b98484593b2e04d942d204c",
            "4f6b99acc9b0477e8cf4172b4a1c94ee",
            "6b3f9aab8e4c4985a1df74b0f96d7bcc",
            "d4877a20acee48f79eb3de51ef7a37cf",
            "695dd11188484ce281c99da34da8d357",
            "5df74a39c4af40649568d0ee65ad1aaf",
            "427f775dfdec4ffc8f9a639da2e79125",
            "a27599d434724a5c8349b1b3698bcbac",
            "818934e1092342d8a938302f5696747c",
            "308f099232ff49db8feff275da3a09a2",
            "7ad5a180a107424eb7ae11cda566af83",
            "039a0d8c49da4fcd9f9bc4f2d13ee254",
            "5a426a22fe6843049ce17fe19c2a168b",
            "16ad3728d5a34a29a80995f1ba7498a3",
            "81484bb9285540dc869a8537dda706a1",
            "e1e79d38f51146568460154b05173a74",
            "782e2319b9f94c4c82af272a7a0417eb"
          ]
        },
        "id": "vn3fNPq5OlgX",
        "outputId": "6bc0a901-5b1b-4f3c-a61c-29755f81ec7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s>[SYSTEM]\n",
            "You are a helpful assistant.\n",
            "[/SYSTEM]\n",
            "[USER]\n",
            "Classify the sentiment of this tweet.\n",
            "\n",
            "Input:\n",
            "\"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin\"\n",
            "[/USER]\n",
            "[ASSISTANT]\n",
            "positive</s>\n"
          ]
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "def build_prompt(example):\n",
        "    system = \"You are a helpful assistant.\"\n",
        "    instr  = (example.get(\"instruction\") or \"\").strip()\n",
        "    inp    = (example.get(\"input\") or \"\").strip()\n",
        "    out    = (example.get(\"output\") or \"\").strip()\n",
        "\n",
        "    # Build the user message\n",
        "    if instr and inp:\n",
        "        user = f\"{instr}\\n\\nInput:\\n{inp}\"\n",
        "    else:\n",
        "        user = instr or inp\n",
        "\n",
        "    # Final combined prompt\n",
        "    return {\n",
        "        \"text\": f\"<s>[SYSTEM]\\n{system}\\n[/SYSTEM]\\n\"\n",
        "                f\"[USER]\\n{user}\\n[/USER]\\n\"\n",
        "                f\"[ASSISTANT]\\n{out}</s>\"\n",
        "    }\n",
        "\n",
        "# Apply formatting to both train and validation\n",
        "ds_text = DatasetDict({\n",
        "    \"train\": ds_small[\"train\"].map(build_prompt),\n",
        "    \"validation\": ds_small[\"validation\"].map(build_prompt)\n",
        "})\n",
        "\n",
        "# Show one example\n",
        "print(ds_text[\"train\"][0][\"text\"][:400])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1usxl6nEP6zu"
      },
      "source": [
        "###Train (QLoRA on T4-safe settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2kmGciGPcDY"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWakJ2YbQcL0"
      },
      "outputs": [],
      "source": [
        "# Load model with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222,
          "referenced_widgets": [
            "13dbfb6d1d044dc0991553ea384fc780",
            "7c65abe6ecd14b6aa7895ce803c2b493",
            "f41691ce96054d25afba10ddece57ad3",
            "daea6094cc1e4f0d8498f900cbbd9473",
            "4f5f4ed521f94a1a850688310accd571",
            "fcc1119944a34894b39a8b8a6d069b23",
            "2907c4bf3f0c4ed7b177d2c720b6334d",
            "e8b43d49d99a4377ad89851f87433145",
            "37703337d5cb47008036ce516796771e",
            "df16ea883a3e44508d21471b87304ffc",
            "2e54e95319d7443582ae03de5223b79f",
            "3d388e0170b845fabfcd9e415a9e5345",
            "6b91cd17d0bc4b9aa440c9eda639a3a2",
            "2f4792f00405417f86456db5b9808c81",
            "1f9bde7c2b3f45dca660ebf4b76794d3",
            "41ad52cc36d84d0da4a6dced21f5436c",
            "bc908438d32147128d66890b73f2a89a",
            "27036f2a918c454fa172bd11d2e6c016",
            "892f6ce287e74a36b8f8c8572f4597a2",
            "ddaf50bcac1248b0bf4851f1dc937b60",
            "1c6caa0941d446d8914674f614e875d2",
            "49065376106b42f08e86b5cb3cb86c40"
          ]
        },
        "id": "EfsdlyYqR6eW",
        "outputId": "1378a1d0-fbb8-4d5e-a415-52dcec3d34b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n",
            "WARNING:trl.trainer.sft_trainer:Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n",
            "WARNING:trl.trainer.sft_trainer:You are using packing, but the attention implementation is not set to 'flash_attention_2' or 'kernels-community/vllm-flash-attn3'. Packing flattens batches into a single sequence, and Flash Attention is the only known attention mechanisms that reliably support this. Using other implementations may lead to cross-contamination between batches. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation='flash_attention_2'` or `attn_implementation='kernels-community/vllm-flash-attn3'` in the model configuration.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ SFTTrainer constructed\n"
          ]
        }
      ],
      "source": [
        "BATCH   = 1\n",
        "ACCUM   = 16\n",
        "\n",
        "cfg = SFTConfig(\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=BATCH,\n",
        "    per_device_eval_batch_size=BATCH,\n",
        "    gradient_accumulation_steps=ACCUM,\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    logging_steps=10,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    warmup_ratio=0.03,\n",
        "    weight_decay=0.0,\n",
        "    gradient_checkpointing=True,\n",
        "    packing=True,\n",
        "    report_to=[],\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "\n",
        "kw = dict(\n",
        "    model=model,\n",
        "    args=cfg,\n",
        "    train_dataset=ds_text[\"train\"],\n",
        "    eval_dataset=ds_text[\"validation\"],\n",
        "    peft_config=lora_cfg,\n",
        ")\n",
        "\n",
        "sig = inspect.signature(SFTTrainer.__init__).parameters\n",
        "if \"dataset_text_field\" in sig:\n",
        "    kw[\"dataset_text_field\"] = \"text\"\n",
        "if \"processing_class\" in sig:\n",
        "    kw[\"processing_class\"] = tokenizer\n",
        "elif \"tokenizer\" in sig:\n",
        "    kw[\"tokenizer\"] = tokenizer\n",
        "\n",
        "trainer = SFTTrainer(**kw)\n",
        "print(\"✅ SFTTrainer constructed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYKTNWfeQsUV"
      },
      "source": [
        "#Quick inference (chat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "YIxUdJXDTUtO",
        "outputId": "253458c3-a4e4-4569-934e-9c9b691ebf8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# === Minimal Gradio chat UI for your fine-tuned LoRA model ===\n",
        "!pip -q install gradio>=4.0.0\n",
        "\n",
        "import os, torch, gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# --- Set your base + possible adapter dirs ---\n",
        "BASE_MODEL = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "ADAPTER_DIRS = [\"/content/qwen3b-qlora_trainer\", \"/content/qwen3b-qlora\"]  # whichever exists\n",
        "TOK_DIRS     = [d for d in ADAPTER_DIRS if os.path.isdir(d)]  # try tokenizer from saved dir first\n",
        "\n",
        "# --- Try to reuse tokenizer/model from the current session if available ---\n",
        "tokenizer = globals().get(\"tokenizer\", None)\n",
        "model     = globals().get(\"model\", None)\n",
        "\n",
        "def ensure_tokenizer():\n",
        "    global tokenizer\n",
        "    if tokenizer is not None:\n",
        "        return tokenizer\n",
        "    # Prefer tokenizer from saved adapter dir (if present), else from base model\n",
        "    tok_src = next((d for d in TOK_DIRS if os.path.isdir(d)), BASE_MODEL)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tok_src, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    return tokenizer\n",
        "\n",
        "def ensure_model():\n",
        "    global model, tokenizer\n",
        "    tok = ensure_tokenizer()\n",
        "    if model is not None:\n",
        "        # Already loaded (possibly LoRA-wrapped)\n",
        "        return model\n",
        "\n",
        "    # Load base in 4-bit (T4 friendly)\n",
        "    bnb_cfg = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16  # T4 prefers fp16\n",
        "    )\n",
        "    base = AutoModelForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        quantization_config=bnb_cfg,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    base.config.use_cache = False\n",
        "\n",
        "    # If we have saved adapters, attach them; else just use base\n",
        "    adapter_src = next((d for d in ADAPTER_DIRS if os.path.isdir(d)), None)\n",
        "    if adapter_src:\n",
        "        model = PeftModel.from_pretrained(base, adapter_src)\n",
        "    else:\n",
        "        model = base\n",
        "    return model\n",
        "\n",
        "def build_prompt(instruction, user_input):\n",
        "    system = \"You are a helpful assistant.\"\n",
        "    user = instruction.strip()\n",
        "    if user_input.strip():\n",
        "        user += f\"\\n\\nInput:\\n{user_input.strip()}\"\n",
        "    return (\n",
        "        f\"<s>[SYSTEM]\\n{system}\\n[/SYSTEM]\\n\"\n",
        "        f\"[USER]\\n{user}\\n[/USER]\\n\"\n",
        "        f\"[ASSISTANT]\\n\"\n",
        "    )\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate(instruction, user_input, max_new_tokens, temperature, top_p):\n",
        "    tok = ensure_tokenizer()\n",
        "    mdl = ensure_model()\n",
        "    prompt = build_prompt(instruction, user_input)\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
        "    out = mdl.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=int(max_new_tokens),\n",
        "        do_sample=True,\n",
        "        temperature=float(temperature),\n",
        "        top_p=float(top_p),\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id\n",
        "    )\n",
        "    text = tok.decode(out[0], skip_special_tokens=False)\n",
        "    # Extract assistant span\n",
        "    return text.split(\"[ASSISTANT]\\n\")[-1].split(\"</s>\")[0].strip()\n",
        "\n",
        "with gr.Blocks(title=\"Qwen2.5-3B LoRA — Test UI\") as demo:\n",
        "    gr.Markdown(\"## 🔧 Test your fine-tuned Qwen2.5-3B (LoRA)\")\n",
        "    with gr.Row():\n",
        "        instruction = gr.Textbox(label=\"Instruction\", value=\"Classify the sentiment of this tweet.\")\n",
        "        user_input  = gr.Textbox(label=\"Input\", value=\"I absolutely adore this product! ❤️\")\n",
        "    with gr.Row():\n",
        "        max_new_tokens = gr.Slider(16, 512, value=128, step=1, label=\"max_new_tokens\")\n",
        "        temperature    = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label=\"temperature\")\n",
        "        top_p          = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label=\"top_p\")\n",
        "    run_btn = gr.Button(\"Generate\")\n",
        "    output  = gr.Textbox(label=\"Model Output\")\n",
        "\n",
        "    run_btn.click(\n",
        "        fn=generate,\n",
        "        inputs=[instruction, user_input, max_new_tokens, temperature, top_p],\n",
        "        outputs=[output]\n",
        "    )\n",
        "\n",
        "demo.launch(share=False)  # set share=True if you want a public link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suGfVSQrTzug"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qjJ7oQApOCKm",
        "1qny4o9hOjED"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
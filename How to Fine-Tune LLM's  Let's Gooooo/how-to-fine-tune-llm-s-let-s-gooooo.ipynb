{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4429121,"sourceType":"datasetVersion","datasetId":2594075,"isSourceIdPinned":false}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How to Fine-Tune LLMs: The No-BS Guide\n\n### *From \"What's fine-tuning?\" to \"I just trained my own AI\" in one notebook*\n\n---\n\n## Wait, What Even IS Fine-Tuning?\n\nOkay so imagine this...\n\nYou just hired the **smartest person alive**. We're talking genius-level IQ. They've literally read:\n- Every book ever written\n- All of Wikipedia (yes, even the weird articles about obscure 18th-century philosophers)\n- Every Stack Overflow answer\n- Billions of random blog posts, tweets, and Reddit comments\n\nThey know **EVERYTHING**. Grammar? Perfect. Facts? Encyclopedia brain. Writing? Shakespeare who?\n\n### But here's the problem...\n\nThis genius has **NO CLUE** about YOUR specific stuff:\n\n| What they know | What they DON'T know |\n|----------------|---------------------|\n| General English | Your company's slang |\n| Random facts | Your product details |\n| How to write | YOUR writing style |\n| Generic advice | Your domain expertise |\n\nAsk them to answer mental health questions like a therapist? They'll give you a generic Wikipedia response. \n\nAsk them to sound like YOUR customer support team? They'll sound like a robot reading a manual.\n\n**That's exactly what a pre-trained LLM is like.**\n\nSmart? Absolutely. Useful for YOUR specific task on day one? Not really.\n\n---\n\n## So What's Fine-Tuning Then?\n\n**Fine-tuning** = Giving this genius a crash course in YOUR specific stuff.\n\nThink of it like onboarding a new employee:\n- Day 1: \"Here's how WE talk to customers\"\n- Day 2: \"Here's what WE know about mental health\"\n- Day 3: \"Here's OUR style, OUR vibe, OUR domain\"\n\nAfter onboarding, that genius now sounds like they've worked at YOUR company for years.\n\n**That's fine-tuning in a nutshell.**\n\n### Before vs After Fine-Tuning:\n\n**BEFORE** (vanilla pre-trained model):\n```\nUser: \"I'm feeling anxious\"\n\nAI: \"Anxiety is a feeling of worry, nervousness, or unease, \n     typically about an imminent event or something with an \n     uncertain outcome. It is considered a normal reaction to \n     stress and can sometimes be beneficial in certain situations.\"\n     \n     ...thanks Wikipedia, very helpful (not)\n```\n\n**AFTER** (our fine-tuned model):\n```\nUser: \"I'm feeling anxious\"  \n\nAI: \"I hear you, and I'm sorry you're going through that. \n     Anxiety can feel really overwhelming sometimes. Would you \n     like to talk about what's on your mind? I'm here to listen \n     and help however I can.\"\n     \n     ...NOW we're talking! Empathetic, supportive, actually useful.\n```\n\nSee the difference? Same base intelligence, but now it knows HOW to respond for our specific use case.\n\n---\n\n## What You'll Actually Learn (Not Just Copy-Paste)\n\nBy the end of this notebook, you'll genuinely understand:\n\n### 1. The \"What\" and \"Why\" Behind Every Line\n\nMost tutorials be like: \"Just run this code, trust me bro\"\n\nAnd then when something breaks, you're Googling for 3 hours.\n\n**Not this notebook.** \n\nWe explain EVERYTHING:\n- What each line does\n- WHY it's there\n- What happens if you change it\n- How to debug when things inevitably go wrong\n\nYou won't just follow along. You'll actually UNDERSTAND.\n\n### 2. The Cheat Codes: LoRA + Quantization\n\nHere's the dirty secret about fine-tuning:\n\n**The Old Way (Full Fine-Tuning):**\n```\nFine-tuning a 7B parameter model:\n\nMemory needed:     ~112 GB of GPU RAM\nHardware required: A100 80GB ($10,000+) or multiple GPUs\nYour budget:       Crying\n```\n\n**The Cheat Code (LoRA + 4-bit Quantization):**\n```\nSame 7B model, same results:\n\nMemory needed:     ~4-8 GB of GPU RAM\nHardware required: Free Colab/Kaggle T4\nYour budget:       Zero dollars\n```\n\nThat's not a typo. Same results. **1/20th the resources.**\n\nLoRA = Only train 1-4% of the model (the rest stays frozen)\nQuantization = Compress the model from 32-bit to 4-bit (8x smaller)\n\nCombined = You can fine-tune models that \"shouldn't\" fit on your GPU.\n\nThis is why fine-tuning went from \"big tech only\" to \"anyone with a laptop\" in like 2 years.\n\n### 3. Train a REAL Model on REAL Data\n\nThis isn't a toy example with 10 fake sentences.\n\nWe're using:\n- **Real Model:** TinyLlama 1.1B (that's 1,100,000,000 parameters)\n- **Real Data:** Mental health conversational dataset from Kaggle\n- **Real Result:** A working chatbot that gives empathetic mental health responses\n\nWhen you finish, you'll have an actual fine-tuned model that you built yourself.\n\n### 4. Not Crash Your GPU (The Memory Management Survival Guide)\n\nIf you've done any ML, you know this error:\n```\nRuntimeError: CUDA out of memory. Tried to allocate 2.00 GiB. \nGPU 0 has a total capacity of 15.78 GiB of which 1.24 GiB is free.\n```\n\nThe training crashes. Your progress is lost. You question your career choices.\n\n**We'll teach you ALL the tricks:**\n- 4-bit quantization (8x smaller model)\n- Gradient checkpointing (trade speed for memory)\n- LoRA (only train 4% of parameters)\n- Gradient accumulation (fake large batches)\n- Proper batch sizing\n\nYour GPU will survive. Your training will complete. No tears.\n\n---\n\n## The Roadmap\n\nHere's what we're doing, step by step:\n\n| Chapter | What We Do | Why It Matters |\n|---------|-----------|----------------|\n| 1-3 | Load data from Kaggle | Good data = good model. Garbage in = garbage out. |\n| 4 | Quick theory on LoRA and Quantization | Know WHY the tricks work, not just that they work |\n| 5-6 | Load model + Apply LoRA | The magic memory-saving setup |\n| 7 | Format the data **(CRITICAL!)** | The #1 cause of \"my model outputs garbage\" |\n| 8-9 | Configure training + Actually train | Where the learning happens |\n| 10 | Test the model | Watch your creation come to life |\n| 11 | Save and load for later | So you don't lose your work |\n\n---\n\n## The One Thing You MUST Remember\n\nBefore we write a single line of code, burn this into your brain:\n\n### THE GOLDEN RULE OF FINE-TUNING\n```\n+------------------------------------------------------------------+\n|                                                                  |\n|   Your INFERENCE format must EXACTLY match your TRAINING format  |\n|                                                                  |\n|   - Miss one </s> token?        --> Garbage output               |\n|   - Wrong special tokens?       --> Garbage output               |\n|   - Extra whitespace?           --> Probably garbage output      |\n|   - Different prompt structure? --> Definitely garbage output    |\n|                                                                  |\n|   FORMAT. MUST. MATCH.                                           |\n|                                                                  |\n+------------------------------------------------------------------+\n```\n\nI'm telling you this now because it will save you HOURS of debugging later. \n\nEvery week on Reddit/Discord/Twitter, someone posts:\n> \"My fine-tuned model outputs random HTML tags and nonsense, what's wrong?\"\n\nAnd the answer is almost always: \"Your inference format doesn't match your training format.\"\n\nDon't be that person. We'll show you the correct format and explain exactly why it matters.\n\n---\n\n## Let's Do This\n\nHere's what you'll have by the end:\n\n- [x] A working fine-tuned mental health chatbot\n- [x] Understanding of every single line of code\n- [x] The ability to adapt this to YOUR own dataset\n- [x] Knowledge of LoRA, quantization, and memory management\n- [x] Confidence to fine-tune other models in the future\n\n**Don't forget to upvote if this helps you!**\n\nAlright, enough talk. Let's build something cool.","metadata":{}},{"cell_type":"markdown","source":"## Chapter 1: The Data\n\nWe're using **Mental Health Conversational Data** from Kaggle - Q&A pairs about mental health.\n\n**Why this dataset?**\n- It's conversational (Q&A format) - perfect for teaching a chatbot\n- It's domain-specific - we can see if the model actually learned something\n- It's small enough to train on free Colab GPUs\n\nDataset: https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data","metadata":{}},{"cell_type":"markdown","source":"## Chapter 2: Setup\n\nInstalling all the tools we need.","metadata":{}},{"cell_type":"code","source":"# INSTALLATION\n# transformers - THE library for LLMs from Hugging Face\n# datasets - Easy data loading\n# peft - Parameter-Efficient Fine-Tuning (LoRA lives here)\n# trl - Has SFTTrainer for easy fine-tuning\n# bitsandbytes - 4-bit quantization magic\n# accelerate - Makes training go brrr\n\n!pip install -q transformers datasets peft trl bitsandbytes accelerate\n!pip install -q scipy kagglehub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:04:51.779334Z","iopub.execute_input":"2025-12-30T16:04:51.780181Z","iopub.status.idle":"2025-12-30T16:05:01.493955Z","shell.execute_reply.started":"2025-12-30T16:04:51.780149Z","shell.execute_reply":"2025-12-30T16:05:01.492460Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# IMPORTS\nimport torch\n# torch = PyTorch\n# \n# Think of PyTorch as the ELECTRICITY that powers everything\n# Every single AI calculation happens through PyTorch:\n#   - Matrix multiplications? PyTorch\n#   - GPU computations? PyTorch  \n#   - Gradients for learning? PyTorch\n#   - Tensors (fancy arrays)? PyTorch\n#\n# Without PyTorch, nothing works. It's the foundation.\n# TensorFlow is the alternative but PyTorch won the war lol\n#\n# WHY WE NEED IT: Everything else (transformers, peft, trl) is built ON TOP of PyTorch\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    # Let's break this name down:\n    #   Auto = \"Hey library, figure out the architecture yourself\"\n    #          You just say \"TinyLlama\" and it knows it's a LlamaForCausalLM\n    #          You say \"GPT2\" and it knows it's a GPT2LMHeadModel\n    #          No need to memorize 100 different class names\n    #\n    #   Model = The actual neural network with billions of numbers (weights)\n    #           This IS the AI. The weights store all the knowledge.\n    #\n    #   ForCausalLM = \"For Causal Language Modeling\"\n    #                 Causal = can only look at PAST tokens, not future\n    #                 Language Modeling = predicting the next word\n    #                 This is how ChatGPT works: predict next token, add it, repeat\n    #\n    # WHY WE NEED IT: To load the pre-trained TinyLlama model\n\n    AutoTokenizer,\n    # Models don't understand \"Hello how are you\"\n    # They understand [15496, 703, 527, 345]\n    #\n    # Tokenizer does TWO things:\n    #   1. Text -> Numbers (encoding): \"Hello\" -> [15496]\n    #   2. Numbers -> Text (decoding): [15496] -> \"Hello\"\n    #\n    # Every model has its OWN tokenizer with its OWN vocabulary\n    # TinyLlama's tokenizer is different from GPT-4's tokenizer\n    # \"Auto\" means it automatically picks the right one for your model\n    #\n    # WHY WE NEED IT: To convert our text data into numbers the model understands\n\n    BitsAndBytesConfig,\n    # This is the MEMORY CHEAT CODE config\n    #\n    # Normal model: each weight = 32 bits (float32)\n    #   7 billion weights * 32 bits = 28 GB just to LOAD the model\n    #   Training needs 4x more = 112 GB (nobody has this)\n    #\n    # With BitsAndBytes: each weight = 4 bits\n    #   7 billion weights * 4 bits = 3.5 GB\n    #   That fits on a free Colab GPU!\n    #\n    # BitsAndBytesConfig = the settings for HOW to compress\n    #   - Which 4-bit format? (nf4 is best)\n    #   - What precision for math? (bfloat16)\n    #   - Double quantization? (yes, saves more memory)\n    #\n    # WHY WE NEED IT: To run big models on small GPUs\n\n    TrainingArguments,\n    # A container for all training settings\n    # Learning rate, batch size, epochs, etc.\n    #\n    # ACTUALLY we don't use this one - SFTConfig replaces it\n    # This import is unnecessary but doesn't hurt\n    #\n    # WHY IT'S HERE: Probably copy-pasted from old code lol\n)\n\nfrom peft import (\n    # PEFT = Parameter-Efficient Fine-Tuning\n    # The library that makes LoRA possible\n    # Made by HuggingFace\n    \n    LoraConfig,\n    # The settings for LoRA adapters:\n    #   - r = rank (size of adapters, 8-64 typically)\n    #   - lora_alpha = scaling factor\n    #   - lora_dropout = regularization\n    #   - target_modules = which layers get adapters\n    #\n    # Think of it as the blueprint for the adapters\n    #\n    # WHY WE NEED IT: To tell LoRA how to set up the adapters\n\n    get_peft_model,\n    # This function does the magic transformation:\n    #   INPUT: normal model (frozen, not trainable)\n    #   OUTPUT: model with LoRA adapters attached (trainable)\n    #\n    # It wraps your model and injects tiny trainable matrices\n    # The original weights stay frozen (unchanged)\n    # Only the new small matrices get trained\n    #\n    # WHY WE NEED IT: To actually ADD LoRA to our model\n\n    prepare_model_for_kbit_training,\n    # Quantized (4-bit) models are weird and need special prep\n    #\n    # This function does behind-the-scenes stuff:\n    #   1. Enables gradient checkpointing (memory trick)\n    #   2. Casts layer norms to float32 (stability)\n    #   3. Prepares embeddings for training\n    #   4. Handles other quantization edge cases\n    #\n    # Without this, training a 4-bit model would crash or give garbage\n    #\n    # WHY WE NEED IT: To make quantized models trainable\n\n    TaskType,\n    # Tells LoRA what kind of task you're doing\n    # Different tasks need slightly different setups\n    #\n    # Options:\n    #   CAUSAL_LM = next token prediction (us, ChatGPT-style)\n    #   SEQ_2_SEQ_LM = translation, summarization\n    #   SEQ_CLS = classification (sentiment, spam detection)\n    #   TOKEN_CLS = NER (finding names, places in text)\n    #   QUESTION_ANS = extractive QA\n    #\n    # WHY WE NEED IT: To tell LoRA we're doing text generation\n)\n\nfrom trl import SFTTrainer, SFTConfig\n# TRL = Transformer Reinforcement Learning library\n# But we're using it for SFT (Supervised Fine-Tuning), not RL\n#\n# SFTTrainer = The coach that runs training\n#   - Loads batches of data\n#   - Runs forward pass (model makes predictions)\n#   - Calculates loss (how wrong was it?)\n#   - Runs backward pass (calculate gradients)\n#   - Updates weights (optimizer step)\n#   - Logs metrics, saves checkpoints\n#   - ALL AUTOMATIC - we just call trainer.train()\n#\n# SFTConfig = All the training hyperparameters\n#   - How many epochs?\n#   - What batch size?\n#   - What learning rate?\n#   - When to save?\n#   - etc.\n#\n# WHY WE NEED IT: So we don't write 200 lines of training loop code\n\nfrom datasets import Dataset\n# HuggingFace's data container class\n#\n# Why not just use a Python list?\n#   - Dataset is optimized for large data (memory-mapped)\n#   - Has .map() for applying functions to all examples\n#   - Has .train_test_split() for splitting data\n#   - Works seamlessly with HuggingFace trainers\n#   - Can shuffle, batch, filter easily\n#\n# WHY WE NEED IT: Trainer expects data in this format\n\nimport os\n# Built-in Python library for operating system stuff\n#   - os.makedirs() = create folders\n#   - os.path.join() = combine paths\n#   - os.listdir() = list files in folder\n#\n# WHY WE NEED IT: To create output directories for saving models\n\nimport random\n# Built-in Python library for randomness\n#   - random.choice() = pick random item\n#   - random.shuffle() = shuffle a list\n#   - random.seed() = reproducibility\n#\n# WHY WE NEED IT: Actually not used in this notebook lol\n#                 But good habit to import for ML work\n\nimport pandas as pd\n# THE data manipulation library\n# Think Excel but in Python\n#\n# We use it for:\n#   - pd.read_json() = load the intents.json file\n#   - Iterating through rows with .iterrows()\n#\n# Could we do this without pandas? Yes\n# Is pandas easier? Way easier\n#\n# WHY WE NEED IT: To load and process the JSON dataset\n\n# ============================================================\n# CHECKING OUR SETUP\n# ============================================================\n\nprint(\"All imports successful!\")\n# If any import failed, we'd get an error before this\n# Seeing this message = everything installed correctly\n\nprint(f\"PyTorch: {torch.__version__}\")\n# Shows PyTorch version like \"2.8.0+cu126\"\n#   - 2.8.0 = PyTorch version\n#   - cu126 = CUDA 12.6 (GPU support)\n# Good for debugging (\"it worked on version X\")\n\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n# CUDA = NVIDIA's GPU computing platform\n#\n# True = You have an NVIDIA GPU AND PyTorch can use it\n#        Training will be fast (minutes)\n#\n# False = No GPU, CPU only\n#         Training will be SLOW (hours)\n#         Go to Runtime > Change runtime type > GPU\n\nif torch.cuda.is_available():\n    # Only run this if we have a GPU\n    \n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    # Shows GPU name like \"Tesla P100-PCIE-16GB\"\n    #   - Tesla P100 = the GPU model\n    #   - 16GB = VRAM amount\n    #\n    # The 0 means \"first GPU\" (index 0)\n    # Most people only have 1 GPU so it's always 0\n    #\n    # Common free GPUs:\n    #   - Tesla T4: 16GB, good\n    #   - Tesla P100: 16GB, good\n    #   - Tesla K80: 12GB, older but works\n    \n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    # Shows VRAM in GB\n    #\n    # .total_memory = bytes of VRAM\n    # / 1e9 = divide by 1 billion = convert to GB\n    # :.2f = format as decimal with 2 places\n    #\n    # More VRAM = can train bigger models, use bigger batches\n    # 16GB is plenty for TinyLlama with 4-bit quantization","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:01.495695Z","iopub.execute_input":"2025-12-30T16:05:01.496470Z","iopub.status.idle":"2025-12-30T16:05:33.327283Z","shell.execute_reply.started":"2025-12-30T16:05:01.496438Z","shell.execute_reply":"2025-12-30T16:05:33.326623Z"}},"outputs":[{"name":"stderr","text":"2025-12-30 16:05:14.924398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767110715.109335      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767110715.164769      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767110715.601098      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767110715.601152      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767110715.601155      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767110715.601157      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"All imports successful!\nPyTorch: 2.8.0+cu126\nCUDA available: True\nGPU: Tesla P100-PCIE-16GB\nMemory: 17.06 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Chapter 3: Loading the Data","metadata":{}},{"cell_type":"code","source":"import kagglehub\n# kagglehub = Kaggle's official library for downloading datasets\n#\n# Kaggle is like GitHub but for datasets and ML competitions\n# They host thousands of free datasets\n# Instead of manually downloading + uploading to Colab, kagglehub does it automatically\n#\n# Alternative methods (old way):\n#   1. Download ZIP from kaggle.com manually\n#   2. Upload to Colab/Kaggle notebook\n#   3. Unzip it\n#   4. Find the path\n#   ... super annoying\n#\n# kagglehub way:\n#   1. One line of code\n#   ... that's it\n#\n# WHY WE NEED IT: To grab the dataset without manual downloading BS\n\npath = kagglehub.dataset_download(\"elvis23/mental-health-conversational-data\")\n# This one line does A LOT:\n#   1. Connects to Kaggle's servers\n#   2. Downloads the dataset ZIP file\n#   3. Extracts it to a local folder\n#   4. Returns the path to that folder\n#\n# The string \"elvis23/mental-health-conversational-data\" is the dataset ID\n#   - \"elvis23\" = the username who uploaded it\n#   - \"mental-health-conversational-data\" = the dataset name\n#   - You can find this in the Kaggle URL:\n#     kaggle.com/datasets/elvis23/mental-health-conversational-data\n#                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#                        this part is the ID\n#\n# path = where the files got saved\n#   - On Kaggle: \"/kaggle/input/mental-health-conversational-data\"\n#   - On Colab: \"/root/.cache/kagglehub/datasets/...\"\n#\n# WHY WE NEED IT: To actually get the data onto our machine\n\nprint(\"Dataset path:\", path)\n# Shows where the dataset was saved\n#\n# Output looks like:\n#   \"Dataset path: /kaggle/input/mental-health-conversational-data\"\n#\n# We need this path to load the files in the next step\n# If something went wrong, we'd see an error here instead\n#\n# WHY WE NEED IT: To confirm download worked + know where files are\n\nprint(\"Files:\", os.listdir(path))\n# os.listdir(path) = list all files in that folder\n#\n# Output looks like:\n#   \"Files: ['intents.json']\"\n#\n# This tells us:\n#   - Download worked (folder exists)\n#   - What files we have to work with\n#   - The filename we need to load (intents.json)\n#\n# Some datasets have multiple files (train.csv, test.csv, etc.)\n# This one just has intents.json\n#\n# WHY WE NEED IT: To see what files we're working with\n#                 So we know what to load next","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:33.328188Z","iopub.execute_input":"2025-12-30T16:05:33.328442Z","iopub.status.idle":"2025-12-30T16:05:38.877717Z","shell.execute_reply.started":"2025-12-30T16:05:33.328419Z","shell.execute_reply":"2025-12-30T16:05:38.876803Z"}},"outputs":[{"name":"stdout","text":"Dataset path: /kaggle/input/mental-health-conversational-data\nFiles: ['intents.json']\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Load the JSON file\ndataset = pd.read_json(os.path.join(path, 'intents.json'))\n# Let's break this down from the INSIDE OUT:\n#\n# STEP 1: os.path.join(path, 'intents.json')\n#   - path = \"/kaggle/input/mental-health-conversational-data\" (from previous cell)\n#   - 'intents.json' = the filename we saw in os.listdir()\n#   - os.path.join() = combines them into a full path\n#   - Result: \"/kaggle/input/mental-health-conversational-data/intents.json\"\n#\n#   Why not just use path + '/intents.json'?\n#   - os.path.join() handles slashes correctly on ANY operating system\n#   - Windows uses backslashes: C:\\folder\\file\n#   - Mac/Linux use forward slashes: /folder/file\n#   - os.path.join() figures it out automatically\n#   - It's a good habit even if you're always on Linux\n#\n# STEP 2: pd.read_json(...)\n#   - pd = pandas (we imported it as pd)\n#   - read_json() = reads a JSON file into a DataFrame\n#   - DataFrame = pandas's table structure (rows and columns, like Excel)\n#\n#   JSON looks like this:\n#   {\n#     \"intents\": [\n#       {\"tag\": \"greeting\", \"patterns\": [\"Hi\", \"Hey\"], \"responses\": [\"Hello!\"]},\n#       {\"tag\": \"goodbye\", \"patterns\": [\"Bye\"], \"responses\": [\"See ya!\"]}\n#     ]\n#   }\n#\n#   pandas converts it to a table:\n#   | intents                                                    |\n#   |------------------------------------------------------------|\n#   | {'tag': 'greeting', 'patterns': ['Hi'...], 'responses':... |\n#   | {'tag': 'goodbye', 'patterns': ['Bye'...], 'responses':... |\n#\n# STEP 3: dataset = ...\n#   - Stores the DataFrame in a variable called 'dataset'\n#   - Now we can work with the data\n#\n# WHY WE NEED IT: To get the raw data into Python so we can process it\n\nprint(f\"Loaded {len(dataset)} intent categories\")\n# len(dataset) = number of rows in the DataFrame\n#\n# Our dataset has 80 rows, each row is an \"intent category\"\n# Intent categories are like topics:\n#   - greeting (Hi, Hey, Hello)\n#   - anxiety (I feel anxious, I'm worried)\n#   - depression (I feel sad, I'm depressed)\n#   - etc.\n#\n# Output: \"Loaded 80 intent categories\"\n#\n# This confirms:\n#   - File loaded successfully (no error)\n#   - We have 80 different conversation topics\n#\n# WHY WE NEED IT: Sanity check - make sure data loaded correctly\n\nprint(dataset.head())\n# .head() = show first 5 rows of the DataFrame\n#\n# Output looks like:\n#                                              intents\n# 0  {'tag': 'greeting', 'patterns': ['Hi', 'Hey'...\n# 1  {'tag': 'morning', 'patterns': ['Good morning...\n# 2  {'tag': 'afternoon', 'patterns': ['Good after...\n# 3  {'tag': 'evening', 'patterns': ['Good evening...\n# 4  {'tag': 'night', 'patterns': ['Good night'],...\n#\n# Each row has ONE column called 'intents'\n# Inside that column is a dictionary with:\n#   - 'tag': the category name (\"greeting\")\n#   - 'patterns': list of user questions [\"Hi\", \"Hey\", \"Hello\"]\n#   - 'responses': list of bot answers [\"Hello there!\", \"Hi!\"]\n#\n# .head() options:\n#   - .head() = first 5 rows (default)\n#   - .head(10) = first 10 rows\n#   - .tail() = last 5 rows\n#   - .sample(5) = random 5 rows\n#\n# WHY WE NEED IT: To peek at the data structure\n#                 So we know how to extract Q&A pairs in the next step","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:38.879558Z","iopub.execute_input":"2025-12-30T16:05:38.879858Z","iopub.status.idle":"2025-12-30T16:05:38.906463Z","shell.execute_reply.started":"2025-12-30T16:05:38.879832Z","shell.execute_reply":"2025-12-30T16:05:38.905933Z"}},"outputs":[{"name":"stdout","text":"Loaded 80 intent categories\n                                             intents\n0  {'tag': 'greeting', 'patterns': ['Hi', 'Hey', ...\n1  {'tag': 'morning', 'patterns': ['Good morning'...\n2  {'tag': 'afternoon', 'patterns': ['Good aftern...\n3  {'tag': 'evening', 'patterns': ['Good evening'...\n4  {'tag': 'night', 'patterns': ['Good night'], '...\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# EXPAND INTO Q&A PAIRS\n# Each pattern (question) gets paired with EACH response (answer)\n# This gives us MORE training data!\n#\n# THE PROBLEM:\n#   Raw data looks like:\n#   {\n#     \"patterns\": [\"Hi\", \"Hey\", \"Hello\"],      <- 3 questions\n#     \"responses\": [\"Hello!\", \"Hi there!\"]     <- 2 answers\n#   }\n#\n#   But for training, we need INDIVIDUAL pairs:\n#   Q: Hi        A: Hello!\n#   Q: Hi        A: Hi there!\n#   Q: Hey       A: Hello!\n#   Q: Hey       A: Hi there!\n#   Q: Hello     A: Hello!\n#   Q: Hello     A: Hi there!\n#\n#   3 questions x 2 answers = 6 training examples from 1 intent!\n#\n# WHY THIS MATTERS:\n#   - More training data = better model\n#   - Original: 80 intents\n#   - After expansion: 661 Q&A pairs\n#   - That's 8x more training examples!\n\nqa_pairs = []\n# Creating an empty list to store our Q&A pairs\n# We'll fill it up with dictionaries like:\n#   {'question': 'Hi', 'answer': 'Hello there!'}\n#   {'question': 'Hey', 'answer': 'Hello there!'}\n#   ... etc\n#\n# WHY A LIST: Easy to append to, easy to convert to Dataset later\n\nfor idx, row in dataset.iterrows():\n    # .iterrows() = loop through DataFrame row by row\n    #\n    # Each iteration gives us:\n    #   idx = row number (0, 1, 2, ... 79)\n    #   row = the actual row data (a pandas Series)\n    #\n    # It's like doing:\n    #   for i in range(len(dataset)):\n    #       row = dataset.iloc[i]\n    # But cleaner\n    #\n    # We don't actually use idx, but iterrows() always returns both\n    # Some people write: for _, row in dataset.iterrows()\n    # The _ means \"I don't care about this value\"\n    #\n    # WHY ITERROWS: To process each intent category one at a time\n\n    intent = row['intents']\n    # row['intents'] = get the 'intents' column from this row\n    #\n    # Remember the DataFrame structure:\n    #   | intents                                           |\n    #   |---------------------------------------------------|\n    #   | {'tag': 'greeting', 'patterns': [...], ...}       |\n    #\n    # So row['intents'] gives us that dictionary:\n    #   {'tag': 'greeting', 'patterns': ['Hi', 'Hey'], 'responses': ['Hello!']}\n    #\n    # Now we can access patterns and responses from it\n    #\n    # WHY: To extract the dictionary containing patterns and responses\n\n    patterns = intent.get('patterns', [])   # Questions\n    # intent.get('patterns', []) = safely get 'patterns' key\n    #\n    # Two ways to get dictionary values:\n    #   intent['patterns']      <- CRASHES if key doesn't exist\n    #   intent.get('patterns')  <- Returns None if key doesn't exist\n    #   intent.get('patterns', [])  <- Returns [] if key doesn't exist\n    #\n    # The [] is the DEFAULT value if 'patterns' is missing\n    # This prevents crashes on malformed data\n    #\n    # patterns = ['Hi', 'Hey', 'Hello', 'Howdy'] (list of user inputs)\n    #\n    # WHY .get(): Defensive coding - don't crash on bad data\n\n    responses = intent.get('responses', []) # Answers\n    # Same thing but for responses\n    #\n    # responses = ['Hello there!', 'Hi! How are you?'] (list of bot replies)\n    #\n    # WHY: To get the list of possible answers\n\n    if patterns and responses:\n        # This checks TWO things:\n        #   1. patterns is not empty (has at least 1 question)\n        #   2. responses is not empty (has at least 1 answer)\n        #\n        # Empty list = False in Python (falsy value)\n        # Non-empty list = True in Python (truthy value)\n        #\n        # So this is shorthand for:\n        #   if len(patterns) > 0 and len(responses) > 0:\n        #\n        # WHY: Skip intents that are missing questions or answers\n        #      Can't make a Q&A pair without both!\n\n        # Pair EACH pattern with EACH response for more data\n        for pattern in patterns:\n            # Loop through each question\n            # pattern = \"Hi\" then \"Hey\" then \"Hello\" etc.\n            \n            for response in responses:\n                # NESTED LOOP: For each question, loop through ALL answers\n                # response = \"Hello there!\" then \"Hi! How are you?\" etc.\n                #\n                # This creates the CARTESIAN PRODUCT:\n                #   pattern=\"Hi\" + response=\"Hello there!\"\n                #   pattern=\"Hi\" + response=\"Hi! How are you?\"\n                #   pattern=\"Hey\" + response=\"Hello there!\"\n                #   pattern=\"Hey\" + response=\"Hi! How are you?\"\n                #   ... and so on\n                #\n                # 3 patterns x 2 responses = 6 combinations\n                #\n                # WHY NESTED LOOPS: To create ALL possible Q&A combinations\n                #                   More training data = better model\n\n                qa_pairs.append({\n                    'question': pattern.strip(),\n                    'answer': response.strip()\n                })\n                # .append() = add item to end of list\n                #\n                # We're adding a dictionary with:\n                #   'question': the user input\n                #   'answer': the bot response\n                #\n                # .strip() = remove whitespace from both ends\n                #   \"  Hi  \" -> \"Hi\"\n                #   \"\\nHello\\n\" -> \"Hello\"\n                #   Cleans up messy data\n                #\n                # After all loops, qa_pairs looks like:\n                # [\n                #   {'question': 'Hi', 'answer': 'Hello there!'},\n                #   {'question': 'Hi', 'answer': 'Hi! How are you?'},\n                #   {'question': 'Hey', 'answer': 'Hello there!'},\n                #   ... 661 total pairs\n                # ]\n                #\n                # WHY THIS FORMAT: Easy to convert to HuggingFace Dataset\n                #                  Easy to access with pair['question']\n\nprint(f\"Created {len(qa_pairs)} Q&A pairs!\")\n# len(qa_pairs) = how many pairs we created\n#\n# Output: \"Created 661 Q&A pairs!\"\n#\n# Started with 80 intents, ended with 661 pairs\n# That's the power of the cartesian product!\n#\n# WHY: Confirm the expansion worked and see how much data we have\n\nprint(f\"\\nExamples:\")\n# \\n = newline, just adds blank line for readability\n\nfor i in range(3):\n    # Loop 3 times: i = 0, 1, 2\n    # Show first 3 examples as a sanity check\n    \n    print(f\"Q: {qa_pairs[i]['question']}\")\n    # qa_pairs[i] = the i-th pair (a dictionary)\n    # qa_pairs[i]['question'] = the question from that pair\n    #\n    # Output: \"Q: Hi\"\n    \n    print(f\"A: {qa_pairs[i]['answer'][:80]}...\\n\")\n    # qa_pairs[i]['answer'][:80] = first 80 characters of answer\n    #\n    # [:80] is string slicing - prevents super long answers from flooding the screen\n    # Some answers are like 200 characters, this keeps output clean\n    #\n    # The \"...\" at the end shows we truncated it\n    # \\n adds blank line between examples\n    #\n    # Output: \"A: Hello there. Tell me how are you feeling today?...\"\n    #\n    # WHY: Visual confirmation that our data looks right\n    #      Always inspect your data before training!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:38.907250Z","iopub.execute_input":"2025-12-30T16:05:38.907489Z","iopub.status.idle":"2025-12-30T16:05:38.916926Z","shell.execute_reply.started":"2025-12-30T16:05:38.907466Z","shell.execute_reply":"2025-12-30T16:05:38.916218Z"}},"outputs":[{"name":"stdout","text":"Created 661 Q&A pairs!\n\nExamples:\nQ: Hi\nA: Hello there. Tell me how are you feeling today?...\n\nQ: Hi\nA: Hi there. What brings you here today?...\n\nQ: Hi\nA: Hi there. How are you feeling today?...\n\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Convert to HuggingFace Dataset format\ntrain_data = Dataset.from_list(qa_pairs)\n# Dataset = HuggingFace's data container class (we imported it earlier)\n# .from_list() = create a Dataset from a list of dictionaries\n#\n# Our qa_pairs looks like:\n# [\n#   {'question': 'Hi', 'answer': 'Hello there!'},\n#   {'question': 'Hey', 'answer': 'Hi! How are you?'},\n#   {'question': 'Hello', 'answer': 'Hello there!'},\n#   ... 661 total\n# ]\n#\n# Dataset.from_list() converts it to a table structure:\n#   | question | answer                    |\n#   |----------|---------------------------|\n#   | Hi       | Hello there!              |\n#   | Hey      | Hi! How are you?          |\n#   | Hello    | Hello there!              |\n#   | ...      | ...                       |\n#\n# Each dictionary KEY becomes a COLUMN\n# Each dictionary becomes a ROW\n#\n# WHY NOT JUST USE THE LIST?\n#   - SFTTrainer EXPECTS a HuggingFace Dataset\n#   - Dataset has useful methods: .map(), .train_test_split(), .shuffle()\n#   - Dataset is memory-efficient for large data (memory-mapped)\n#   - Dataset integrates perfectly with the HuggingFace ecosystem\n#\n# OTHER WAYS TO CREATE A DATASET:\n#   - Dataset.from_dict({'question': [...], 'answer': [...]})\n#   - Dataset.from_pandas(dataframe)\n#   - Dataset.from_csv('file.csv')\n#   - load_dataset('huggingface/dataset_name')  <- from HuggingFace Hub\n#\n# .from_list() is cleanest when you already have list of dicts\n#\n# WHY WE NEED IT: Trainer won't accept a raw Python list\n#                 This is the format HuggingFace tools expect\n\nprint(f\"Dataset: {len(train_data)} examples\")\n# len(train_data) = number of rows in the Dataset\n#\n# Output: \"Dataset: 661 examples\"\n#\n# Same as len(qa_pairs) - just confirming conversion worked\n# If this number was different, something went wrong\n#\n# WHY: Sanity check - make sure no data was lost in conversion\n\nprint(f\"Columns: {train_data.column_names}\")\n# .column_names = list of column names in the Dataset\n#\n# Output: \"Columns: ['question', 'answer']\"\n#\n# These came from the dictionary keys!\n# Every dict in qa_pairs had 'question' and 'answer' keys\n# So Dataset has 'question' and 'answer' columns\n#\n# If you had dicts like {'q': '...', 'a': '...'}\n# You'd get columns ['q', 'a'] instead\n#\n# WHY: Confirm the structure is what we expect\n#      We'll reference these column names later when formatting\n#\n# BONUS - OTHER USEFUL DATASET PROPERTIES:\n#   train_data[0]              <- first row as dict\n#   train_data['question']     <- all questions as list\n#   train_data[0:5]            <- first 5 rows as new Dataset\n#   train_data.shape           <- (num_rows, num_columns)\n#   train_data.features        <- column types (string, int, etc.)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:38.917616Z","iopub.execute_input":"2025-12-30T16:05:38.917812Z","iopub.status.idle":"2025-12-30T16:05:38.942618Z","shell.execute_reply.started":"2025-12-30T16:05:38.917792Z","shell.execute_reply":"2025-12-30T16:05:38.941941Z"}},"outputs":[{"name":"stdout","text":"Dataset: 661 examples\nColumns: ['question', 'answer']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Chapter 4: Quick Theory\n\n**Why LoRA?**\n- Full fine-tuning a 7B model needs ~112GB RAM (for gradients + optimizer states)\n- LoRA freezes the original model and adds tiny \"adapter\" layers\n- We only train ~1% of parameters\n- Same results, fraction of the memory\n\n**Why Quantization?**\n- Normal: 32 bits per weight -> 7B model = 28GB\n- 4-bit: 4 bits per weight -> 7B model = 3.5GB\n- Combine both = fine-tune 7B models on consumer GPUs","metadata":{}},{"cell_type":"markdown","source":"## Chapter 5: Model Setup","metadata":{}},{"cell_type":"code","source":"# 4-BIT QUANTIZATION CONFIG\n# This compresses the model to fit in GPU memory\n#\n# THE PROBLEM:\n#   Model weights are normally stored as float32 (32 bits per number)\n#   TinyLlama has 1.1 billion weights\n#   1.1B * 32 bits = 4.4 GB just to LOAD the model\n#\n#   Bigger models are worse:\n#   7B model * 32 bits = 28 GB  (won't fit on most GPUs)\n#   13B model * 32 bits = 52 GB (definitely won't fit)\n#   70B model * 32 bits = 280 GB (lol good luck)\n#\n#   And that's just to LOAD it!\n#   Training needs 3-4x more memory for gradients + optimizer states\n#\n# THE SOLUTION: Quantization\n#   Store weights in fewer bits\n#   32 bits -> 4 bits = 8x smaller!\n#   7B model: 28 GB -> 3.5 GB (fits on free Colab!)\n#\n# WHY WE NEED IT: To run models that would otherwise need $10,000 GPUs\n\nbnb_config = BitsAndBytesConfig(\n    # BitsAndBytesConfig = settings container for quantization\n    # BitsAndBytes = the library that does the actual compression\n    # Created by Tim Dettmers (legend in ML efficiency)\n    #\n    # We're creating a config object with our preferred settings\n    # This config gets passed to the model loader later\n\n    load_in_4bit=True,\n    # load_in_4bit = compress weights to 4-bit when loading\n    #\n    # Bit depth comparison:\n    #   float32 = 32 bits = full precision (default)\n    #   float16 = 16 bits = half precision (2x smaller)\n    #   int8    = 8 bits  = 4x smaller\n    #   int4    = 4 bits  = 8x smaller  <- WE'RE USING THIS\n    #\n    # True = yes, compress to 4-bit\n    # False = don't compress (you'd need way more VRAM)\n    #\n    # The compression happens WHEN the model loads\n    # Original model on disk: float16 or float32\n    # Model in memory: 4-bit (thanks to this setting)\n    #\n    # WHY: 8x memory savings. The main reason we can run big models.\n\n    bnb_4bit_quant_type=\"nf4\",\n    # bnb_4bit_quant_type = WHICH 4-bit format to use\n    #\n    # Options:\n    #   \"fp4\" = Float Point 4-bit\n    #           Standard 4-bit, works okay\n    #\n    #   \"nf4\" = NormalFloat 4-bit  <- THE GOOD ONE\n    #           Specifically designed for neural network weights\n    #           Based on the observation that neural net weights\n    #           follow a NORMAL DISTRIBUTION (bell curve)\n    #           So nf4 spaces out its 16 values (2^4=16) to match\n    #           that bell curve, giving better precision where it matters\n    #\n    # Research showed nf4 beats fp4 on basically every benchmark\n    # There's no reason to use fp4 unless you're experimenting\n    #\n    # WHY \"nf4\": Best quality for neural networks, basically free upgrade\n\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    # bnb_4bit_compute_dtype = precision for CALCULATIONS\n    #\n    # IMPORTANT DISTINCTION:\n    #   - Weights are STORED in 4-bit (saves memory)\n    #   - Calculations are DONE in higher precision (better accuracy)\n    #\n    # When the model does math:\n    #   1. Decompress 4-bit weights to bfloat16 (on the fly)\n    #   2. Do the matrix multiplication in bfloat16\n    #   3. Keep result in bfloat16\n    #\n    # Why not compute in 4-bit?\n    #   - 4-bit math is imprecise and causes errors to accumulate\n    #   - Decompressing to 16-bit for math is fast (GPU is good at this)\n    #   - Best of both worlds: small storage, accurate compute\n    #\n    # Options:\n    #   torch.float32  = most precise, but slower and more memory\n    #   torch.float16  = half precision, good but can overflow\n    #   torch.bfloat16 = \"brain float 16\", same range as float32 but less precise\n    #                    designed by Google specifically for ML\n    #                    handles big/small numbers without overflow\n    #                    THE BEST CHOICE for training\n    #\n    # WHY bfloat16: Stable training, good speed, designed for exactly this use case\n\n    bnb_4bit_use_double_quant=True,\n    # bnb_4bit_use_double_quant = quantize the quantization constants too\n    #\n    # NERDY DETAIL (skip if you want):\n    #   When you quantize weights, you need to store \"scaling factors\"\n    #   These tell you how to convert 4-bit back to real numbers\n    #   \n    #   Example: weights [0.12, 0.15, 0.11, 0.14]\n    #   Quantized: [2, 3, 1, 2] with scale=0.05\n    #   To get back: 2*0.05=0.10, 3*0.05=0.15, etc.\n    #\n    #   The scales themselves take up memory (usually float32)\n    #   Double quant = quantize the scales too (to 8-bit)\n    #   Saves another ~0.4 bits per parameter\n    #\n    # In practice:\n    #   double_quant=False: ~4.5 bits per weight\n    #   double_quant=True:  ~4.0 bits per weight\n    #   That's about 10% extra memory savings!\n    #\n    # True = yes, do the extra compression\n    # False = don't bother (slightly faster loading, slightly more memory)\n    #\n    # WHY True: Free memory savings with no quality loss. Why not?\n)\n\nprint(\"Quantization config ready!\")\n# Just confirms we created the config without errors\n#\n# This config doesn't DO anything yet\n# It's just settings saved in a variable\n# The actual quantization happens when we load the model\n# We'll pass this config to AutoModelForCausalLM.from_pretrained()\n#\n# WHY: Confirmation message, good practice to verify steps completed\n\n# ============================================================\n# MEMORY SAVINGS SUMMARY\n# ============================================================\n#\n# TinyLlama 1.1B without quantization:\n#   1.1B params * 32 bits = 4.4 GB\n#   Training memory: ~17 GB (gradients, optimizer, activations)\n#\n# TinyLlama 1.1B WITH this config:\n#   1.1B params * 4 bits = 0.55 GB\n#   Training memory: ~3-4 GB (fits easily on free GPUs!)\n#\n# For a 7B model:\n#   Without: 28 GB (needs A100 or better)\n#   With: 3.5 GB (runs on T4/P100!)\n#\n# THE TRADEOFF:\n#   - Quality loss: ~1-3% worse on benchmarks (barely noticeable)\n#   - Speed: slightly slower (decompression overhead)\n#   - Memory: 8x smaller (HUGE win)\n#\n# For fine-tuning on consumer hardware, this is basically mandatory","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:38.943355Z","iopub.execute_input":"2025-12-30T16:05:38.944355Z","iopub.status.idle":"2025-12-30T16:05:38.949293Z","shell.execute_reply.started":"2025-12-30T16:05:38.944322Z","shell.execute_reply":"2025-12-30T16:05:38.948688Z"}},"outputs":[{"name":"stdout","text":"Quantization config ready!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# MODEL SELECTION\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n# This is the Hugging Face model ID\n# Format: \"organization/model-name\"\n#\n# Let's break down the name:\n#   TinyLlama/         = the organization that made it (TinyLlama team)\n#   TinyLlama          = model family name\n#   1.1B               = 1.1 billion parameters (the \"size\" of the brain)\n#   Chat               = fine-tuned for conversation (not just raw text completion)\n#   v1.0               = version 1.0\n#\n# WHERE THIS COMES FROM:\n#   Hugging Face Hub = like GitHub but for AI models\n#   URL: huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n#   \n#   Anyone can upload models there\n#   You just use the ID string to download them\n#   No manual downloading - the library handles everything\n#\n# WHY TINYLLAMA?\n#   1. It's SMALL: 1.1B parameters\n#      - Loads in ~0.7 GB with 4-bit quantization\n#      - Trains fast (minutes, not hours)\n#      - Perfect for learning/experimenting\n#\n#   2. It's CAPABLE: Despite being small\n#      - Trained on 3 trillion tokens (massive dataset)\n#      - Punches above its weight class\n#      - Actually produces coherent responses\n#\n#   3. It's CHAT-TUNED: The \"-Chat\" part matters\n#      - Base models just complete text randomly\n#      - Chat models understand conversation format\n#      - Already knows <|system|>, <|user|>, <|assistant|> format\n#      - We're building on top of existing chat ability\n#\n#   4. It's FREE: No API keys, no payments\n#      - Open source, open weights\n#      - Use it however you want\n#\n# Small enough for free Colab, but still capable\n# ^ This is the key tradeoff\n#   Bigger model = smarter but needs more VRAM\n#   Smaller model = fits anywhere but less capable\n#   TinyLlama hits a sweet spot for learning\n\n# Other options: \"microsoft/phi-2\", \"mistralai/Mistral-7B-v0.1\"\n#\n# IF YOU HAVE MORE GPU MEMORY, TRY THESE:\n#\n# \"microsoft/phi-2\" (2.7B parameters)\n#   - 2.5x bigger than TinyLlama\n#   - Microsoft's \"small but mighty\" model\n#   - Surprisingly good at reasoning\n#   - Needs ~2 GB with 4-bit quant\n#   - Good middle ground\n#\n# \"mistralai/Mistral-7B-v0.1\" (7B parameters)\n#   - 6x bigger than TinyLlama\n#   - One of the best open source models\n#   - Beats many 13B models in benchmarks\n#   - Needs ~4 GB with 4-bit quant\n#   - Use if you have T4/P100 with room to spare\n#\n# \"meta-llama/Llama-2-7b-chat-hf\" (7B parameters)\n#   - Meta's official Llama 2\n#   - Requires accepting license on HuggingFace\n#   - Very capable, widely used\n#   - Similar requirements to Mistral-7B\n#\n# \"mistralai/Mistral-7B-Instruct-v0.2\" (7B parameters)\n#   - Instruction-tuned version of Mistral\n#   - Better at following commands\n#   - Great for chat applications\n#\n# PARAMETER COUNT REFERENCE:\n#   1B params   = small, fast, limited capability\n#   3B params   = decent balance\n#   7B params   = good quality, needs ~16GB GPU\n#   13B params  = high quality, needs ~24GB GPU\n#   70B params  = state of the art, needs multiple GPUs\n#\n# FOR THIS TUTORIAL:\n#   Stick with TinyLlama - guaranteed to work on free GPUs\n#   Once you understand the process, scale up to bigger models\n\nprint(f\"Using model: {model_name}\")\n# Output: \"Using model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n#\n# Just confirms which model we're using\n# Useful when you copy-paste code and forget what you set\n#\n# This doesn't LOAD the model yet\n# We're just storing the ID string in a variable\n# Actual loading happens in the next cell with AutoModelForCausalLM.from_pretrained()\n#\n# WHY A VARIABLE?\n#   We reference model_name multiple times:\n#   - Loading the model\n#   - Loading the tokenizer\n#   - Saving metadata\n#   \n#   If it's a variable, we change it in ONE place\n#   If we hardcoded it everywhere, we'd have to change multiple lines\n#   \n#   model_name = \"different/model\"  <- change once, works everywhere\n#\n# WHY: Good practice, keeps code DRY (Don't Repeat Yourself)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:38.950468Z","iopub.execute_input":"2025-12-30T16:05:38.950943Z","iopub.status.idle":"2025-12-30T16:05:38.962617Z","shell.execute_reply.started":"2025-12-30T16:05:38.950919Z","shell.execute_reply":"2025-12-30T16:05:38.962071Z"}},"outputs":[{"name":"stdout","text":"Using model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# LOAD TOKENIZER\n# Converts text <-> token IDs\n#\n# WHY WE NEED A TOKENIZER:\n#   Models don't understand text. At all.\n#   They only understand numbers (tensors of integers)\n#\n#   Human: \"Hello, how are you?\"\n#   Model: \"wtf is this gibberish?\"\n#\n#   Human: [15496, 11, 703, 527, 345, 30]\n#   Model: \"Ah yes, I know exactly what to do!\"\n#\n#   Tokenizer is the translator between human world and model world\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# Let's break this down:\n#\n# AutoTokenizer\n#   - \"Auto\" = automatically detect the right tokenizer class\n#   - TinyLlama uses LlamaTokenizer under the hood\n#   - GPT-2 uses GPT2Tokenizer\n#   - BERT uses BertTokenizer\n#   - You don't need to know which - Auto figures it out\n#\n# .from_pretrained(model_name, ...)\n#   - Downloads the tokenizer files from Hugging Face Hub\n#   - model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n#   - It fetches:\n#       tokenizer.json     (the actual vocabulary)\n#       tokenizer_config.json  (settings)\n#       special_tokens_map.json  (like <s>, </s>, etc.)\n#\n#   IMPORTANT: Tokenizer MUST match the model!\n#   - TinyLlama's tokenizer knows 32,000 tokens\n#   - Those exact 32,000 tokens map to the model's embedding layer\n#   - If you use wrong tokenizer, token ID 5000 means different things\n#   - Result: complete garbage output\n#   - Always load tokenizer from SAME model ID as the model\n#\n# trust_remote_code=True\n#   - Some models have custom tokenizer code\n#   - This allows running that custom code\n#   - Security note: only use for trusted models (like TinyLlama)\n#   - For sketchy random models, set False and hope it works\n#\n# WHAT'S IN A TOKENIZER:\n#   1. Vocabulary: mapping of text pieces to IDs\n#      \"hello\" -> 15496\n#      \"the\" -> 278\n#      \"ing\" -> 292  (yes, word PIECES, not just words)\n#\n#   2. Encoding rules: how to break text into pieces\n#      \"unhappiness\" -> [\"un\", \"happiness\"] or [\"unhapp\", \"iness\"]?\n#      Different tokenizers do it differently\n#\n#   3. Special tokens: control tokens\n#      <s> = start of sequence\n#      </s> = end of sequence\n#      <pad> = padding\n#      <unk> = unknown token\n#\n# WHY WE NEED IT: Can't feed text to model without converting to numbers first\n\n# Set padding token (many models don't have one by default)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(f\"Set pad_token to: '{tokenizer.eos_token}'\")\n#\n# THE PADDING PROBLEM:\n#   When training, we process BATCHES of examples\n#   Batch = multiple examples at once (faster than one-by-one)\n#\n#   But examples have different lengths:\n#     Example 1: \"Hi\"                    -> [15496]           (1 token)\n#     Example 2: \"How are you doing?\"    -> [5765, 526, 345, 892]  (4 tokens)\n#\n#   GPUs need rectangular tensors (same length for all)\n#   Can't have jagged arrays!\n#\n#   Solution: PAD shorter examples to match longest:\n#     Example 1: [15496, PAD, PAD, PAD]  (padded to 4 tokens)\n#     Example 2: [5765, 526, 345, 892]   (already 4 tokens)\n#\n#   Now both are length 4 - can stack into a batch!\n#\n# THE MISSING PAD TOKEN:\n#   Many models (like Llama family) weren't trained with padding\n#   They only have: <s> (start), </s> (end), <unk> (unknown)\n#   No <pad> token exists in their vocabulary!\n#\n#   tokenizer.pad_token is None = no padding token defined\n#\n# THE FIX:\n#   tokenizer.pad_token = tokenizer.eos_token\n#   \"Just use the end-of-sequence token for padding\"\n#   \n#   </s> = token ID 2 (for TinyLlama)\n#   Now padding uses ID 2\n#\n#   Is this a hack? Kind of.\n#   Does it work? Yes, perfectly fine for fine-tuning.\n#   \n#   The model learns to ignore padding tokens anyway\n#   Using </s> for padding doesn't confuse it\n#\n# WHY WE CHECK FIRST (if ... is None):\n#   Some models DO have a pad token already\n#   Don't want to override it if it exists\n#   Only set it if it's missing\n\ntokenizer.padding_side = \"right\"\n# padding_side = WHERE to add the padding tokens\n#\n# Two options:\n#   \"right\" = pad at the END      [Hello, <pad>, <pad>]\n#   \"left\"  = pad at the START    [<pad>, <pad>, Hello]\n#\n# FOR DECODER-ONLY MODELS (GPT, Llama, TinyLlama):\n#   Use \"right\" padding\n#\n#   Why? These models generate LEFT to RIGHT\n#   They predict: given everything before, what comes next?\n#   \n#   With right padding:\n#     [Hello, how, are, you, <pad>, <pad>]\n#     Model sees real tokens first, padding after\n#     Makes sense - real content, then filler\n#\n#   With left padding:\n#     [<pad>, <pad>, Hello, how, are, you]\n#     Model sees padding first, then real tokens\n#     Can confuse the model during training\n#     (Though left padding IS used for inference sometimes)\n#\n# FOR ENCODER MODELS (BERT):\n#   Either side works, usually use \"right\"\n#\n# FOR ENCODER-DECODER (T5):\n#   Usually \"right\" for both\n#\n# TinyLlama is decoder-only, so \"right\" is correct\n#\n# WHY IT MATTERS:\n#   Wrong padding side can hurt training performance\n#   Model might learn weird patterns from padding position\n#   \"right\" is the safe default for causal LMs\n\nprint(f\"Vocab size: {tokenizer.vocab_size:,}\")\n# tokenizer.vocab_size = how many tokens the tokenizer knows\n#\n# Output: \"Vocab size: 32,000\"\n#\n# This means:\n#   - 32,000 unique tokens in the vocabulary\n#   - Token IDs range from 0 to 31,999\n#   - Any text gets broken into these 32,000 building blocks\n#\n# The :, in the f-string adds commas for readability\n#   32000 -> 32,000\n#   1000000 -> 1,000,000\n#\n# VOCAB SIZE COMPARISON:\n#   TinyLlama: 32,000 tokens\n#   GPT-2: 50,257 tokens\n#   Llama 2: 32,000 tokens\n#   GPT-4: ~100,000 tokens (estimated)\n#\n# Bigger vocab = more tokens to represent concepts\n#              = more efficient (fewer tokens per word)\n#              = but bigger embedding matrix\n#\n# WHY WE PRINT IT: Sanity check, confirms tokenizer loaded correctly\n\nprint(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n# EOS = End Of Sequence\n#\n# Output: \"EOS token: '</s>' (ID: 2)\"\n#\n# tokenizer.eos_token = the string representation \"</s>\"\n# tokenizer.eos_token_id = the numeric ID (2)\n#\n# WHY EOS MATTERS:\n#   1. Tells model when to STOP generating\n#      Without EOS, model rambles forever\n#      \"Hello how are you I am fine the weather is nice today and...\"\n#      With EOS: \"Hello how are you?</s>\" -> model stops\n#\n#   2. We use it in training data\n#      Every example ends with </s>\n#      Model learns: \"when I generate </s>, I'm done\"\n#\n#   3. We used it as pad_token (see above)\n#\n# OTHER SPECIAL TOKENS:\n#   tokenizer.bos_token = \"<s>\" (Beginning Of Sequence)\n#   tokenizer.unk_token = \"<unk>\" (Unknown - rare/unseen words)\n#   tokenizer.pad_token = \"</s>\" (we just set this)\n#\n# WHY WE PRINT IT: \n#   - Confirms EOS is what we expect\n#   - We'll reference this token in our prompt format\n#   - Good to know the actual string and ID","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:38.963466Z","iopub.execute_input":"2025-12-30T16:05:38.963701Z","iopub.status.idle":"2025-12-30T16:05:41.709039Z","shell.execute_reply.started":"2025-12-30T16:05:38.963680Z","shell.execute_reply":"2025-12-30T16:05:41.708269Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee59ceba532456ab34d4099faf73d7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34c370751d4f44e6a7408b184a40b358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cf14b6ac86f4e4f80f166c907313e91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f85180ead64e3f9204611481d81736"}},"metadata":{}},{"name":"stdout","text":"Vocab size: 32,000\nEOS token: '</s>' (ID: 2)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# LOAD MODEL\nprint(\"Loading model (might take a minute)...\")\n# Just a heads up message because loading takes time\n#\n# What happens during loading:\n#   1. Download model files from Hugging Face (if not cached)\n#      - First time: downloads ~2.2 GB of files\n#      - After that: loads from local cache (fast)\n#   2. Read the weight files into memory\n#   3. Apply quantization (compress to 4-bit)\n#   4. Move to GPU\n#\n# This can take 30 seconds to a few minutes depending on:\n#   - Internet speed (if downloading)\n#   - Disk speed (if loading from cache)\n#   - GPU speed (for quantization)\n#\n# WHY PRINT THIS: So you don't think it crashed when nothing happens for a while\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    # AutoModelForCausalLM = Auto-detect the right model class\n    #\n    # \"Auto\" does the magic:\n    #   - Reads config.json from the model repo\n    #   - Sees architecture: \"LlamaForCausalLM\"\n    #   - Automatically uses the LlamaForCausalLM class\n    #\n    # Without Auto, you'd need to know the exact class:\n    #   from transformers import LlamaForCausalLM\n    #   model = LlamaForCausalLM.from_pretrained(...)\n    #\n    # Auto saves you from memorizing 50+ model class names\n    #\n    # \"ForCausalLM\" part:\n    #   - Causal = can only see PAST tokens (not future)\n    #   - LM = Language Model (predicts next token)\n    #   - This is how GPT/ChatGPT style models work\n    #   - Input: \"The cat sat on the\"\n    #   - Output: \"mat\" (predicts next word)\n    #\n    # .from_pretrained() = load a pre-trained model\n    #   - \"pre-trained\" = someone already trained it on trillions of tokens\n    #   - We're not starting from random weights\n    #   - We're building on top of existing knowledge\n    #\n    # WHY WE NEED IT: To get the actual neural network with all its weights\n\n    model_name,\n    # model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    # We defined this earlier\n    #\n    # This tells from_pretrained() WHERE to find the model:\n    #   1. First checks local cache (~/.cache/huggingface/)\n    #   2. If not there, downloads from huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n    #\n    # The model repo contains:\n    #   - config.json (architecture settings)\n    #   - model.safetensors (the actual weights, ~2.2 GB)\n    #   - generation_config.json (default generation settings)\n    #\n    # WHY: Tells the function which model to load\n\n    quantization_config=bnb_config,\n    # bnb_config = the BitsAndBytesConfig we created earlier\n    #\n    # Remember what's in it:\n    #   load_in_4bit=True\n    #   bnb_4bit_quant_type=\"nf4\"\n    #   bnb_4bit_compute_dtype=torch.bfloat16\n    #   bnb_4bit_use_double_quant=True\n    #\n    # This tells the loader:\n    #   \"When you load those weights, compress them to 4-bit\"\n    #\n    # WITHOUT this config:\n    #   Model loads in float16/float32\n    #   TinyLlama would use ~2.2 GB\n    #   Bigger models wouldn't fit at all\n    #\n    # WITH this config:\n    #   Model loads in 4-bit\n    #   TinyLlama uses ~0.7 GB\n    #   7B models fit on consumer GPUs\n    #\n    # The quantization happens ON THE FLY during loading:\n    #   1. Read original float16 weights from disk\n    #   2. Compress each layer to 4-bit as it's loaded\n    #   3. Store compressed version in GPU memory\n    #\n    # Original weights on disk are unchanged\n    # Only the in-memory version is quantized\n    #\n    # WHY: This is what makes the memory magic happen\n\n    device_map=\"auto\",\n    # device_map = WHERE to put the model layers\n    #\n    # \"auto\" = let the library figure it out automatically\n    #\n    # What \"auto\" does:\n    #   1. Check available devices (GPU? Multiple GPUs? CPU?)\n    #   2. Check available memory on each device\n    #   3. Spread model layers across devices optimally\n    #\n    # For most people (1 GPU):\n    #   - Puts entire model on GPU 0\n    #   - Simple and fast\n    #\n    # For multiple GPUs:\n    #   - Splits model across GPUs automatically\n    #   - Layer 0-11 on GPU 0, Layer 12-22 on GPU 1, etc.\n    #   - Called \"model parallelism\"\n    #\n    # For huge models that don't fit on GPU:\n    #   - Puts what fits on GPU\n    #   - Puts overflow on CPU RAM\n    #   - Slow but works\n    #\n    # OTHER OPTIONS:\n    #   device_map=\"cuda:0\"  = force everything on GPU 0\n    #   device_map=\"cpu\"     = force everything on CPU (slow!)\n    #   device_map={\"\": 0}   = another way to say GPU 0\n    #   device_map={\"model.layers.0\": 0, \"model.layers.1\": 1, ...}  = manual assignment\n    #\n    # \"auto\" is almost always what you want\n    #\n    # WHY: So we don't manually manage which GPU gets which layer\n\n    trust_remote_code=True,\n    # trust_remote_code = allow running custom code from the model repo\n    #\n    # Some models have custom Python files:\n    #   - Custom attention implementations\n    #   - Custom tokenization logic\n    #   - Custom architecture tweaks\n    #\n    # True = download and execute that custom code\n    # False = only use standard HuggingFace classes\n    #\n    # SECURITY WARNING:\n    #   Custom code could theoretically be malicious\n    #   Random person uploads model with evil code\n    #   You run it and get hacked\n    #\n    #   In practice:\n    #   - Popular models (TinyLlama, Llama, Mistral) are safe\n    #   - They're audited by thousands of users\n    #   - Don't use trust_remote_code=True on sketchy random models\n    #\n    # TinyLlama is trustworthy, so True is fine\n    #\n    # WHY: Some model features require custom code to work\n\n    torch_dtype=torch.bfloat16,\n    # torch_dtype = precision for model weights (before quantization)\n    #\n    # Wait, didn't we already set 4-bit quantization?\n    # Yes, but this is for the COMPUTATION dtype\n    #\n    # The flow:\n    #   1. Load weights (originally float16 or float32 on disk)\n    #   2. Convert to torch_dtype (bfloat16)\n    #   3. Then quantize to 4-bit for storage\n    #   4. During forward pass: decompress to bfloat16 for math\n    #\n    # bfloat16 vs float16:\n    #   float16 = 16 bits, good precision, but LIMITED RANGE\n    #             Can overflow on big numbers (gives infinity)\n    #             \n    #   bfloat16 = 16 bits, SAME RANGE as float32, less precision\n    #              Never overflows, slightly less accurate\n    #              Google invented it specifically for ML training\n    #\n    # For training, bfloat16 is safer:\n    #   - No overflow issues\n    #   - Gradients stay stable\n    #   - Basically same speed as float16\n    #\n    # NOTE: You might see a warning about this being \"deprecated\"\n    #       They want you to use `dtype` instead of `torch_dtype`\n    #       Both work fine, don't worry about it\n    #\n    # WHY: Stable computation without overflow during training\n)\n\nprint(\"Model loaded!\")\n# Success message - model is now in GPU memory\n#\n# At this point:\n#   - 1.1 billion parameters are loaded\n#   - Compressed to 4-bit\n#   - Sitting in GPU memory\n#   - Ready to process text (but not ready for TRAINING yet)\n#\n# If you got here without errors, you're in good shape!\n# Common errors that would stop you here:\n#   - CUDA out of memory (model too big, try smaller one)\n#   - Model not found (typo in model_name)\n#   - trust_remote_code required (set it to True)\n#\n# WHY: Confirmation that loading succeeded\n\nif torch.cuda.is_available():\n    print(f\"GPU memory used: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n# Let's see how much GPU memory the model is using\n#\n# torch.cuda.is_available() = check if we have a GPU\n#   True = GPU exists and CUDA works\n#   False = no GPU or CUDA broken\n#\n# torch.cuda.memory_allocated() = bytes currently used by PyTorch tensors\n#   This is the actual memory YOUR stuff is using\n#   Not total GPU memory, not memory used by other programs\n#   Just PyTorch tensors (mainly our model weights)\n#\n# / 1e9 = divide by 1,000,000,000 = convert bytes to gigabytes\n# :.2f = format with 2 decimal places\n#\n# OUTPUT: \"GPU memory used: 0.77 GB\"\n#\n# 0.77 GB for a 1.1 billion parameter model!\n# Without quantization it would be ~2.2 GB (float16) or ~4.4 GB (float32)\n# That's 3-6x memory savings from 4-bit quantization!\n#\n# This 0.77 GB is just the MODEL\n# Training will use more:\n#   - Gradients (for trainable LoRA params only)\n#   - Optimizer states\n#   - Activations (intermediate values)\n#   - Input batches\n#\n# But LoRA keeps this small since we only train ~4% of params\n#\n# WHY: Verify quantization worked and see actual memory usage\n\n# ============================================================\n# WHAT WE HAVE NOW\n# ============================================================\n#\n# model = a neural network with:\n#   - 22 transformer layers\n#   - 1.1 billion weights (quantized to 4-bit)\n#   - Pre-trained knowledge from 3 trillion tokens\n#   - Chat abilities from instruction tuning\n#\n# But it's FROZEN - all weights are fixed\n# We can use it for inference (generating text)\n# We CANNOT train it yet (no gradients computed)\n#\n# Next step: prepare_model_for_kbit_training()\n# That will set up the model to be trainable with LoRA","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:41.711328Z","iopub.execute_input":"2025-12-30T16:05:41.711577Z","iopub.status.idle":"2025-12-30T16:05:55.655822Z","shell.execute_reply.started":"2025-12-30T16:05:41.711552Z","shell.execute_reply":"2025-12-30T16:05:55.655262Z"}},"outputs":[{"name":"stdout","text":"Loading model (might take a minute)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b284ab988f9c46259df3a04f1c97d0af"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c5f50178379489e8113870eec3082ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34de2e34a07f4c97bc3e14b805d77206"}},"metadata":{}},{"name":"stdout","text":"Model loaded!\nGPU memory used: 0.77 GB\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# PREPARE FOR TRAINING\nmodel = prepare_model_for_kbit_training(\n    # prepare_model_for_kbit_training = \"get this quantized model ready for training\"\n    #\n    # THE PROBLEM:\n    #   We loaded a 4-bit quantized model\n    #   Quantized models are WEIRD and need special handling\n    #   If you just try to train them directly, things break:\n    #     - Gradients don't flow properly\n    #     - Numerical instability (NaN losses)\n    #     - Some layers don't update correctly\n    #     - Memory usage is suboptimal\n    #\n    # THE SOLUTION:\n    #   This function does ALL the behind-the-scenes fixes\n    #   One function call, all problems solved\n    #\n    # It comes from the PEFT library (same people who made LoRA)\n    # They figured out all the edge cases so we don't have to\n    #\n    # WHY WE NEED IT: Quantized models need special prep before training works\n\n    model,\n    # The model we just loaded\n    # This function modifies it IN PLACE but also returns it\n    # We reassign: model = prepare_model_for_kbit_training(model)\n    # Now 'model' refers to the prepared version\n    #\n    # WHAT THIS FUNCTION DOES TO THE MODEL:\n    #\n    # 1. FREEZES ALL PARAMETERS\n    #    for param in model.parameters():\n    #        param.requires_grad = False\n    #\n    #    requires_grad = \"should we calculate gradients for this?\"\n    #    False = frozen, won't be trained\n    #    True = trainable, will be updated\n    #\n    #    We freeze EVERYTHING because:\n    #    - 4-bit weights can't be trained directly anyway\n    #    - LoRA will add NEW trainable params on top later\n    #    - We only want to train those new params\n    #\n    # 2. CASTS LAYER NORMS TO FLOAT32\n    #    for name, module in model.named_modules():\n    #        if \"LayerNorm\" in type(module).__name__:\n    #            module.to(torch.float32)\n    #\n    #    LayerNorm = normalization layers (stabilize training)\n    #    These are sensitive to precision\n    #    4-bit or even float16 can make them unstable\n    #    float32 keeps them accurate and stable\n    #\n    #    It's a tiny amount of extra memory (LayerNorms are small)\n    #    But huge impact on training stability\n    #\n    # 3. ENABLES INPUT GRADIENTS FOR EMBEDDING LAYER\n    #    model.enable_input_require_grads()\n    #\n    #    Embedding layer = converts token IDs to vectors\n    #    Gradients need to flow BACK through this layer\n    #    Without this, training signal doesn't reach LoRA\n    #\n    # 4. SETS UP GRADIENT CHECKPOINTING (if enabled)\n    #    This is a memory optimization technique (see below)\n    #\n    # WHY ONE FUNCTION:\n    #   - These steps are fiddly and easy to mess up\n    #   - Order matters for some of them\n    #   - Different model architectures need slightly different handling\n    #   - The function handles all edge cases automatically\n\n    use_gradient_checkpointing=True,\n    # gradient_checkpointing = a MEMORY SAVING trick\n    #\n    # THE MEMORY PROBLEM:\n    #   During training, we need to store \"activations\"\n    #   Activations = intermediate values from forward pass\n    #\n    #   Forward pass: Input -> Layer 1 -> Layer 2 -> ... -> Layer 22 -> Output\n    #   \n    #   Each layer produces activations:\n    #   Input -> [Act1] -> [Act2] -> [Act3] -> ... -> [Act22] -> Output\n    #\n    #   WHY KEEP THEM?\n    #   For backward pass (calculating gradients), we need these values\n    #   Gradient at Layer 5 depends on activations FROM Layer 5\n    #   \n    #   Normally: store ALL activations (lots of memory!)\n    #   22 layers x batch_size x sequence_length x hidden_dim = HUGE\n    #\n    # THE GRADIENT CHECKPOINTING TRICK:\n    #   Instead of storing all activations:\n    #   1. Only store activations at certain \"checkpoint\" layers\n    #   2. During backward pass, RECOMPUTE the others on the fly\n    #\n    #   Example (simplified):\n    #   Normal:      Store [Act1, Act2, Act3, Act4, Act5, Act6] = 6 units memory\n    #   Checkpointed: Store [Act1, Act3, Act5] = 3 units memory\n    #                 When we need Act2: recompute from Act1\n    #                 When we need Act4: recompute from Act3\n    #\n    # THE TRADEOFF:\n    #   Memory: ~30-50% LESS (big win!)\n    #   Speed: ~20-30% SLOWER (recomputing takes time)\n    #\n    #   For us: Memory is the bottleneck, not speed\n    #   We're on a free GPU with limited VRAM\n    #   Trading speed for memory is a GREAT deal\n    #\n    # True = YES, use gradient checkpointing (save memory)\n    # False = NO, keep all activations (faster but more memory)\n    #\n    # WHEN TO USE:\n    #   True: When memory is tight (free Colab, small GPU)\n    #   False: When you have tons of VRAM (A100 80GB etc.)\n    #\n    # For free GPUs, ALWAYS use True\n    #\n    # WHY: We'd run out of memory during training without this\n)\n\nprint(\"Model prepared for training!\")\n# Success message - model is now ready for LoRA\n#\n# WHAT'S DIFFERENT NOW:\n#   Before prepare_model_for_kbit_training():\n#   - Model loaded and quantized\n#   - All params had requires_grad=True (default)\n#   - Would crash or be unstable during training\n#   - No gradient checkpointing\n#\n#   After prepare_model_for_kbit_training():\n#   - All params have requires_grad=False (frozen)\n#   - LayerNorms are float32 (stable)\n#   - Gradient checkpointing enabled (memory efficient)\n#   - Ready for LoRA adapters to be added\n#\n# NEXT STEP:\n#   Add LoRA adapters with get_peft_model()\n#   Those adapters will be the ONLY trainable parts\n#   ~4% of parameters, but that's all we need!\n#\n# WHY: Confirms preparation succeeded\n\n# ============================================================\n# MEMORY SAVINGS SUMMARY SO FAR\n# ============================================================\n#\n# 1. Quantization (4-bit):\n#    - Model weights: 4.4 GB -> 0.77 GB\n#    - 5.7x smaller!\n#\n# 2. Gradient Checkpointing:\n#    - Activations: ~2-4 GB -> ~1-2 GB (during training)\n#    - ~2x smaller!\n#\n# 3. LoRA (coming next):\n#    - Only train 4% of params\n#    - Gradients + optimizer states for 4% instead of 100%\n#    - ~25x smaller!\n#\n# Combined: A model that would need 20+ GB\n#           Now fits in 4-5 GB during training\n#           That's why we can train on free Colab!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:55.656781Z","iopub.execute_input":"2025-12-30T16:05:55.657391Z","iopub.status.idle":"2025-12-30T16:05:55.681862Z","shell.execute_reply.started":"2025-12-30T16:05:55.657365Z","shell.execute_reply":"2025-12-30T16:05:55.681296Z"}},"outputs":[{"name":"stdout","text":"Model prepared for training!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Chapter 6: LoRA Config","metadata":{}},{"cell_type":"code","source":"# LORA CONFIGURATION\n# These settings control the adapter layers\n#\n# QUICK LORA RECAP:\n#   Instead of training ALL 1.1 billion parameters, we:\n#   1. Freeze the original model (don't touch it)\n#   2. Add tiny \"adapter\" matrices to certain layers\n#   3. Only train those adapters (~50 million params)\n#\n#   Same results, fraction of the memory and time\n#\n# HOW LORA WORKS (the actual math):\n#   Original layer: y = Wx (W is a huge matrix, like 2048x2048)\n#   \n#   Instead of modifying W directly, LoRA adds a BYPASS:\n#   y = Wx + BAx\n#       ^^   ^^^\n#       |    |__ LoRA adapter (two small matrices multiplied)\n#       |_______ Original weights (frozen, unchanged)\n#\n#   B and A are MUCH smaller:\n#   - A is (r x 2048) - compresses input to small \"r\" dimensions\n#   - B is (2048 x r) - expands back to original size\n#   - r = \"rank\" = how small? (we're using 64)\n#\n#   Original W: 2048 x 2048 = 4,194,304 parameters\n#   LoRA A+B: (64 x 2048) + (2048 x 64) = 262,144 parameters\n#   That's 16x fewer parameters for ONE layer!\n#\n# WHY WE NEED CONFIG: To tell LoRA exactly how to set up these adapters\n\nlora_config = LoraConfig(\n    # LoraConfig = container for all LoRA settings\n    # We define the config here, apply it to model in next cell\n\n    r=64,\n    # r = RANK of the adapter matrices\n    #\n    # This is THE most important LoRA parameter\n    #\n    # WHAT IT MEANS:\n    #   The \"bottleneck\" dimension in the A and B matrices\n    #   A compresses to r dimensions, B expands back out\n    #\n    #   Think of it like:\n    #   - Original: 2048-dimensional information\n    #   - LoRA: squeeze through 64-dimensional bottleneck\n    #   - Then expand back to 2048\n    #\n    #   Smaller r = tighter squeeze = less capacity = faster/smaller\n    #   Larger r = looser squeeze = more capacity = slower/bigger\n    #\n    # HOW TO CHOOSE:\n    #   r=4 or r=8:   Very small, simple tasks only\n    #                 Like: changing response style, minor tweaks\n    #                 \n    #   r=16:         Good default for simple fine-tuning\n    #                 Most tutorials use this\n    #\n    #   r=32:         Better for moderate complexity\n    #                 Good balance of quality and efficiency\n    #\n    #   r=64:         What we're using - good for complex tasks\n    #                 Learning new domain knowledge (mental health)\n    #                 More capacity to learn new patterns\n    #\n    #   r=128+:       Heavy lifting, approaching full fine-tuning quality\n    #                 Diminishing returns past 64-128 usually\n    #\n    # MEMORY IMPACT:\n    #   r=16:  ~12 million trainable params\n    #   r=32:  ~25 million trainable params\n    #   r=64:  ~50 million trainable params (us)\n    #   r=128: ~100 million trainable params\n    #\n    #   Still WAY less than 1.1 billion!\n    #\n    # WHY 64:\n    #   We're teaching the model a whole new domain (mental health)\n    #   Need enough capacity to learn the vocabulary and response patterns\n    #   64 is generous but still very efficient\n    #\n    # r = rank of the adapter matrices\n    # Higher = more capacity to learn, but more memory\n    # 64 is good for complex tasks\n\n    lora_alpha=128,\n    # lora_alpha = SCALING FACTOR for LoRA outputs\n    #\n    # WHAT IT DOES:\n    #   Remember: y = Wx + BAx\n    #   Actually it's: y = Wx + (alpha/r) * BAx\n    #                        ^^^^^^^^^^^^^\n    #                        scaling factor\n    #\n    #   With r=64 and alpha=128:\n    #   Scaling = 128/64 = 2.0\n    #   LoRA output gets multiplied by 2.0\n    #\n    # WHY SCALE?\n    #   LoRA matrices are initialized near zero\n    #   Without scaling, their contribution is tiny\n    #   Scaling amplifies their effect on the output\n    #\n    #   Higher alpha = LoRA has STRONGER influence\n    #   Lower alpha = LoRA has WEAKER influence\n    #\n    # COMMON PATTERNS:\n    #   alpha = r:       Scaling = 1.0 (neutral)\n    #   alpha = 2*r:     Scaling = 2.0 (stronger, what we use)\n    #   alpha = r/2:     Scaling = 0.5 (weaker)\n    #\n    # HOW TO CHOOSE:\n    #   Start with alpha = r (scaling = 1.0)\n    #   If model doesn't learn enough: increase alpha\n    #   If model overfits/goes crazy: decrease alpha\n    #\n    #   alpha = 2*r is a popular choice for stronger learning\n    #   That's what we're doing: 128 = 2 * 64\n    #\n    # RELATIONSHIP WITH LEARNING RATE:\n    #   alpha affects how much LoRA contributes\n    #   learning_rate affects how fast weights update\n    #   They interact! If you change one, might need to adjust other\n    #\n    #   Rule of thumb: If you double alpha, might halve learning rate\n    #\n    # WHY 128:\n    #   With r=64, alpha=128 gives 2x scaling\n    #   Strong enough to learn new patterns\n    #   Not so strong that it destabilizes training\n    #\n    # Scaling factor. Output scaled by alpha/r\n    # 128/64 = 2x scaling\n\n    lora_dropout=0.1,\n    # lora_dropout = REGULARIZATION for LoRA layers\n    #\n    # WHAT IS DROPOUT:\n    #   During training, randomly \"drop\" some connections\n    #   Set them to zero temporarily\n    #   Different connections dropped each batch\n    #\n    #   0.1 = drop 10% of connections randomly each time\n    #\n    # WHY IT HELPS:\n    #   Prevents OVERFITTING\n    #   \n    #   Overfitting = model memorizes training data\n    #                 instead of learning general patterns\n    #   \n    #   \"Q: Hi\" -> \"A: Hello there!\" (memorized exact response)\n    #   vs\n    #   \"Q: Hi\" -> Understanding how to greet (learned the pattern)\n    #\n    #   Dropout forces the model to not rely on any single connection\n    #   Has to learn robust patterns that work even with missing links\n    #   Like studying with random pages torn out of your textbook\n    #   Forces you to understand concepts, not memorize pages\n    #\n    # HOW TO CHOOSE:\n    #   0.0:  No dropout (risk of overfitting)\n    #   0.05: Light dropout (small datasets, simple tasks)\n    #   0.1:  Standard dropout (what we use - good default)\n    #   0.2:  Heavy dropout (very small data, high overfit risk)\n    #   0.3+: Rarely used, too aggressive\n    #\n    # OUR SITUATION:\n    #   661 training examples = pretty small dataset\n    #   Risk of overfitting is real\n    #   0.1 dropout helps prevent that\n    #\n    # NOTE: Dropout only happens during TRAINING\n    #       During inference, all connections are used\n    #       (scaled appropriately)\n    #\n    # Dropout for regularization (prevents overfitting)\n\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n        \"gate_proj\", \"up_proj\", \"down_proj\",      # MLP layers\n    ],\n    # target_modules = WHICH layers get LoRA adapters\n    #\n    # TRANSFORMER ARCHITECTURE RECAP:\n    #   Each transformer layer has two main parts:\n    #\n    #   1. ATTENTION (the \"thinking\" part)\n    #      - Decides which tokens to focus on\n    #      - \"When generating next word, look at these previous words\"\n    #      - Has 4 projection matrices:\n    #        q_proj = Query projection  (\"what am I looking for?\")\n    #        k_proj = Key projection    (\"what do I contain?\")\n    #        v_proj = Value projection  (\"what info do I have?\")\n    #        o_proj = Output projection (\"combine attention results\")\n    #\n    #   2. MLP/FFN (the \"processing\" part)\n    #      - Transforms information after attention\n    #      - Where factual knowledge is often stored\n    #      - Has 3 projection matrices (in Llama architecture):\n    #        gate_proj = Gating mechanism (\"how much to let through\")\n    #        up_proj   = Expand to larger dimension\n    #        down_proj = Compress back down\n    #\n    # WHICH TO TARGET?\n    #   Minimal: [\"q_proj\", \"v_proj\"]\n    #     - Just attention, most common in early LoRA papers\n    #     - 2 modules per layer, fewer params\n    #\n    #   Standard: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n    #     - All attention layers\n    #     - 4 modules per layer\n    #     - Good for learning new patterns\n    #\n    #   Full (what we use): attention + MLP\n    #     - 7 modules per layer\n    #     - Maximum learning capacity\n    #     - Better for learning new knowledge/domain\n    #\n    # WHY WE TARGET ALL 7:\n    #   We're teaching mental health domain knowledge\n    #   That knowledge needs to be stored somewhere\n    #   MLP layers are believed to store factual knowledge\n    #   Attention layers learn patterns and relationships\n    #   We want BOTH, so we target everything\n    #\n    # MORE MODULES = MORE TRAINABLE PARAMS:\n    #   2 modules (q,v only):    ~18 million params\n    #   4 modules (attention):   ~35 million params\n    #   7 modules (all):         ~50 million params\n    #\n    #   Still just 4% of total model!\n    #\n    # HOW TO FIND MODULE NAMES:\n    #   Different models have different names!\n    #   Llama/TinyLlama: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n    #   GPT-2: c_attn, c_proj, c_fc\n    #   BERT: query, key, value, dense\n    #\n    #   To find them: print([n for n, _ in model.named_modules()])\n    #   Or check the model's documentation\n    #\n    # Which layers get LoRA adapters\n    # More modules = more learning capacity\n\n    bias=\"none\",\n    # bias = whether to train bias terms in LoRA layers\n    #\n    # WHAT ARE BIASES:\n    #   Linear layer: y = Wx + b\n    #                     ^^   ^\n    #                     |    |__ bias (added to output)\n    #                     |_______ weights (multiplied with input)\n    #\n    #   Weights are the big matrices\n    #   Biases are small vectors (one number per output dimension)\n    #\n    # OPTIONS:\n    #   \"none\" = don't train any biases (most efficient)\n    #   \"all\"  = train all biases in target modules\n    #   \"lora_only\" = only train biases in LoRA layers (not original)\n    #\n    # WHY \"none\":\n    #   Biases are tiny compared to weights\n    #   Training them adds complexity but minimal benefit\n    #   Original LoRA paper found \"none\" works great\n    #   Keeps things simple and efficient\n    #\n    # WHEN TO USE OTHER OPTIONS:\n    #   \"all\": If you're really squeezing for every bit of quality\n    #          Marginal improvement, rarely worth it\n    #   \"lora_only\": Middle ground, almost never used\n    #\n    # WHY: Maximum efficiency, following best practices\n\n    task_type=TaskType.CAUSAL_LM,\n    # task_type = what kind of task are we doing?\n    #\n    # OPTIONS:\n    #   TaskType.CAUSAL_LM     = next token prediction (us!)\n    #                            GPT-style text generation\n    #\n    #   TaskType.SEQ_2_SEQ_LM  = encoder-decoder models\n    #                            Like T5, for translation/summarization\n    #\n    #   TaskType.SEQ_CLS       = sequence classification\n    #                            \"Is this email spam?\" -> yes/no\n    #\n    #   TaskType.TOKEN_CLS     = token classification\n    #                            NER: \"John went to Paris\" -> [PERSON, O, O, LOCATION]\n    #\n    #   TaskType.QUESTION_ANS  = extractive QA\n    #                            Find answer span in context\n    #\n    #   TaskType.FEATURE_EXTRACTION = get embeddings\n    #                                  Not really training, just using model\n    #\n    # WHY CAUSAL_LM:\n    #   TinyLlama is a causal (autoregressive) language model\n    #   It generates text left-to-right, predicting next token\n    #   That's exactly what we want for chat responses\n    #\n    # WHAT THIS AFFECTS:\n    #   How LoRA layers are set up internally\n    #   Mostly just tells PEFT which config defaults to use\n    #\n    # WHY: Tells LoRA we're doing text generation, not classification etc.\n)\n\nprint(\"LoRA config:\")\nprint(f\"  Rank: {lora_config.r}\")\n# Output: \"Rank: 64\"\n# Confirms our bottleneck dimension\n\nprint(f\"  Alpha: {lora_config.lora_alpha}\")\n# Output: \"Alpha: 128\"\n# Confirms our scaling factor\n\nprint(f\"  Scaling: {lora_config.lora_alpha / lora_config.r}\")\n# Output: \"Scaling: 2.0\"\n# The actual multiplier applied to LoRA outputs\n# alpha/r = 128/64 = 2.0\n# LoRA contributions are doubled\n\nprint(f\"  Modules: {lora_config.target_modules}\")\n# Output: \"Modules: {'down_proj', 'q_proj', 'gate_proj', 'up_proj', 'v_proj', 'o_proj', 'k_proj'}\"\n# Shows all 7 modules we're targeting\n# Note: it's a set so order is random, that's fine\n#\n# WHY PRINT ALL THIS: Verify config is what we expect before applying\n\n# ============================================================\n# WHAT WE'VE CONFIGURED\n# ============================================================\n#\n# LoRA Adapters will be:\n#   - Rank 64 (good capacity for learning new domain)\n#   - Scaled 2x (alpha=128, strong influence)\n#   - 10% dropout (prevent overfitting)\n#   - Applied to 7 module types (attention + MLP)\n#   - Across all 22 transformer layers\n#\n# This will create:\n#   22 layers × 7 modules × 2 matrices (A and B) = 308 LoRA matrices\n#   Total: ~50 million trainable parameters\n#   That's 4.4% of the 1.1 billion total\n#\n# NEXT STEP:\n#   Apply this config to the model with get_peft_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:55.682698Z","iopub.execute_input":"2025-12-30T16:05:55.682984Z","iopub.status.idle":"2025-12-30T16:05:55.995039Z","shell.execute_reply.started":"2025-12-30T16:05:55.682940Z","shell.execute_reply":"2025-12-30T16:05:55.994265Z"}},"outputs":[{"name":"stdout","text":"LoRA config:\n  Rank: 64\n  Alpha: 128\n  Scaling: 2.0\n  Modules: {'down_proj', 'q_proj', 'gate_proj', 'up_proj', 'v_proj', 'o_proj', 'k_proj'}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# APPLY LORA TO MODEL\nmodel = get_peft_model(model, lora_config)\n# get_peft_model = \"take this model and add LoRA adapters to it\"\n#\n# WHAT THIS FUNCTION DOES:\n#   1. Takes your frozen base model\n#   2. Reads the lora_config we just created\n#   3. Finds all the target_modules in the model\n#   4. Injects LoRA adapter matrices (A and B) into each one\n#   5. Returns a wrapped model that looks the same but has adapters\n#\n# BEFORE get_peft_model():\n#   model = frozen TinyLlama\n#   All 1.1B params have requires_grad=False\n#   Nothing is trainable\n#   Just a static inference model\n#\n# AFTER get_peft_model():\n#   model = TinyLlama + LoRA adapters\n#   Original 1.1B params still frozen (requires_grad=False)\n#   NEW ~50M LoRA params are trainable (requires_grad=True)\n#   Ready for fine-tuning!\n#\n# WHAT'S HAPPENING UNDER THE HOOD:\n#   For each target module (q_proj, k_proj, etc.):\n#\n#   BEFORE:\n#   input -> [q_proj weights] -> output\n#            (frozen 2048x2048)\n#\n#   AFTER:\n#   input -> [q_proj weights] -----> (+) -> output\n#            (frozen 2048x2048)       ^\n#                                     |\n#            [LoRA A] -> [LoRA B] ----+\n#            (64x2048)   (2048x64)\n#            (trainable) (trainable)\n#\n#   The original path is unchanged (frozen)\n#   LoRA adds a parallel bypass path (trainable)\n#   Outputs are summed together\n#\n# THE WRAPPER:\n#   get_peft_model() returns a PeftModel object\n#   It wraps the original model\n#   From the outside, it behaves the same:\n#     - Same input format\n#     - Same output format\n#     - Same forward() method\n#   But internally, LoRA adapters are doing their thing\n#\n# WHY REASSIGN model = ...:\n#   The function returns a NEW wrapped model\n#   We want to use the wrapped version going forward\n#   So we reassign the variable 'model' to point to it\n#\n# WHY WE NEED IT: This is what actually adds the trainable LoRA layers\n\n# See how many parameters we're training\nmodel.print_trainable_parameters()\n# .print_trainable_parameters() = show parameter breakdown\n#\n# OUTPUT:\n#   \"trainable params: 50,462,720 || all params: 1,150,511,104 || trainable%: 4.3861\"\n#\n# LET'S BREAK THAT DOWN:\n#\n# trainable params: 50,462,720\n#   These are the LoRA adapter parameters\n#   The ONLY things that will be updated during training\n#   ~50 million numbers that start random and get optimized\n#\n#   Where do 50M params come from?\n#   - 22 transformer layers\n#   - 7 target modules per layer (q,k,v,o,gate,up,down)\n#   - Each module gets 2 matrices: A and B\n#   - Matrix sizes depend on layer dimensions and rank\n#\n#   Rough math for one q_proj:\n#     A matrix: 64 × 2048 = 131,072 params\n#     B matrix: 2048 × 64 = 131,072 params\n#     Total: 262,144 params per module\n#\n#   262,144 × 7 modules × 22 layers ≈ 40 million\n#   (Actual is 50M because some layers have different sizes)\n#\n# all params: 1,150,511,104\n#   Total parameters in the ENTIRE model\n#   Original TinyLlama (1.1B) + LoRA adapters (50M)\n#   1,100,048,384 + 50,462,720 = 1,150,511,104\n#\n#   The base model params are still there, just frozen\n#   They do the heavy lifting during forward pass\n#   But they don't change during training\n#\n# trainable%: 4.3861\n#   What percentage of total params are trainable\n#   50,462,720 / 1,150,511,104 × 100 = 4.39%\n#\n#   WE'RE ONLY TRAINING 4.4% OF THE MODEL!\n#\n#   This is the magic of LoRA:\n#   - 96% of model is frozen (no gradients, no optimizer states)\n#   - Only 4% needs gradients and optimizer\n#   - Memory for training is ~25x smaller than full fine-tuning\n#\n# Should be around 1-2% of total!\n# ^ Actually we got 4.4% because:\n#   - We used r=64 (higher rank = more params)\n#   - We targeted 7 modules (more modules = more params)\n#   - Still very efficient! Full fine-tuning would be 100%\n\n# ============================================================\n# WHAT JUST HAPPENED - VISUAL\n# ============================================================\n#\n# BEFORE (frozen model):\n#   Layer 1: [frozen weights] -> output\n#   Layer 2: [frozen weights] -> output\n#   ...\n#   Layer 22: [frozen weights] -> output\n#\n#   Trainable: 0 params (0%)\n#\n# AFTER (model with LoRA):\n#   Layer 1: [frozen weights] + [LoRA adapters] -> output\n#   Layer 2: [frozen weights] + [LoRA adapters] -> output\n#   ...\n#   Layer 22: [frozen weights] + [LoRA adapters] -> output\n#\n#   Trainable: 50M params (4.4%)\n#   Frozen: 1.1B params (95.6%)\n#\n# ============================================================\n# WHY THIS IS AMAZING\n# ============================================================\n#\n# FULL FINE-TUNING (the old way):\n#   - Train all 1.1B parameters\n#   - Need to store gradients for 1.1B params\n#   - Need optimizer states for 1.1B params (2x for Adam)\n#   - Memory: model(4GB) + gradients(4GB) + optimizer(8GB) = 16GB+\n#   - For 7B model: 28GB + 28GB + 56GB = 112GB (impossible!)\n#\n# LORA FINE-TUNING (what we're doing):\n#   - Train only 50M parameters (4.4%)\n#   - Gradients for 50M params only\n#   - Optimizer states for 50M params only\n#   - Memory: model(0.7GB) + gradients(0.2GB) + optimizer(0.4GB) ≈ 1.3GB\n#   - Rest is activations and batch data (~2-3GB)\n#   - Total: ~4GB (fits on free GPU!)\n#\n# AND THE RESULTS ARE ALMOST AS GOOD:\n#   Research shows LoRA achieves 90-99% of full fine-tuning quality\n#   For our task (mental health chat), it's plenty\n#   The base model already knows language\n#   We just need to nudge it toward our specific domain\n#\n# ============================================================\n# BONUS: OTHER USEFUL PEFT MODEL METHODS\n# ============================================================\n#\n# model.print_trainable_parameters()  <- what we just used\n# model.get_nb_trainable_parameters() <- returns the number\n# model.save_pretrained(\"path/\")      <- save ONLY the LoRA weights\n# model.merge_and_unload()            <- merge LoRA into base model\n# model.disable_adapter()             <- temporarily disable LoRA\n# model.enable_adapter()              <- re-enable LoRA\n#\n# The save_pretrained() is KEY:\n#   Full model: ~2.2 GB on disk\n#   LoRA adapters only: ~200 MB on disk\n#   You save the small adapters, load base model + adapters later\n#   Much easier to share and store!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:55.996098Z","iopub.execute_input":"2025-12-30T16:05:55.996467Z","iopub.status.idle":"2025-12-30T16:05:56.644969Z","shell.execute_reply.started":"2025-12-30T16:05:55.996443Z","shell.execute_reply":"2025-12-30T16:05:56.644235Z"}},"outputs":[{"name":"stdout","text":"trainable params: 50,462,720 || all params: 1,150,511,104 || trainable%: 4.3861\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Chapter 7: Data Formatting (CRITICAL!)\n\n**THIS IS WHERE MOST PEOPLE MESS UP!**\n\nTinyLlama was trained with a specific chat format:\n```\n<|system|>\n{system message}</s>\n<|user|>\n{user message}</s>\n<|assistant|>\n{assistant response}</s>\n```\n\n**The `</s>` after EACH section is CRITICAL!** Without it, the model outputs garbage.","metadata":{}},{"cell_type":"code","source":"# FORMAT FUNCTION - MUST MATCH TINYLLAMA'S EXACT FORMAT!\n#\n# THIS IS THE MOST IMPORTANT CELL IN THE ENTIRE NOTEBOOK\n# Getting this wrong = garbage output (those <<< < < < << you saw earlier)\n# Getting this right = model actually works\n#\n# WHY FORMAT MATTERS:\n#   TinyLlama was TRAINED on a specific chat format\n#   During its original training, it saw millions of examples like:\n#     <|system|>\n#     You are helpful...</s>\n#     <|user|>\n#     Hello</s>\n#     <|assistant|>\n#     Hi there!</s>\n#\n#   The model learned: \"When I see this pattern, I know how to respond\"\n#   \n#   If you use a DIFFERENT format during fine-tuning:\n#     [INST] Hello [/INST]    <- Llama 2 format\n#     or\n#     User: Hello\\nAssistant: <- Generic format\n#\n#   The model goes: \"WTF is this? I've never seen this pattern\"\n#   Result: Random garbage, HTML tags, nonsense\n#\n#   SAME FORMAT = model understands = good outputs\n#   DIFFERENT FORMAT = model confused = garbage outputs\n\ndef format_prompt(example):\n    # This function takes ONE training example and formats it\n    #\n    # INPUT: example = {'question': 'Hi', 'answer': 'Hello there!'}\n    # OUTPUT: {'text': '<|system|>\\nYou are...\\n<|user|>\\nHi</s>\\n...'}\n    #\n    # We'll apply this to every example in the dataset using .map()\n    \n    \"\"\"\n    Format Q&A into TinyLlama's chat format.\n    \n    CRITICAL: </s> must come after EACH section!\n    - After system message\n    - After user message  \n    - After assistant message\n    \"\"\"\n    # Docstring explaining what this function does\n    # The CRITICAL note is there because this is where most people mess up\n    \n    question = example['question']\n    # Extract the question from the example dict\n    # example['question'] = \"Hi\" or \"How do I manage anxiety?\" etc.\n    \n    answer = example['answer']\n    # Extract the answer from the example dict\n    # example['answer'] = \"Hello there!\" or \"Here are some tips...\" etc.\n    \n    # CORRECT TinyLlama format with </s> after each part\n    prompt = f\"\"\"<|system|>\nYou are a helpful mental health assistant. Provide supportive, empathetic, and informative responses.</s>\n<|user|>\n{question}</s>\n<|assistant|>\n{answer}</s>\"\"\"\n    #\n    # LET'S BREAK THIS DOWN LINE BY LINE:\n    #\n    # <|system|>\n    #   The system message marker\n    #   Tells model: \"What follows is instructions about WHO you are\"\n    #   <| and |> are special delimiters TinyLlama recognizes\n    #\n    # You are a helpful mental health assistant. Provide supportive, empathetic, and informative responses.</s>\n    #   The actual system prompt - defines the AI's personality\n    #   </s> at the end = \"end of system section\"\n    #   THIS </s> WAS MISSING IN THE BROKEN VERSION!\n    #\n    # <|user|>\n    #   The user message marker\n    #   Tells model: \"What follows is from the human\"\n    #\n    # {question}</s>\n    #   The actual user question (inserted from our data)\n    #   </s> at the end = \"end of user section\"\n    #   THIS </s> WAS ALSO MISSING IN THE BROKEN VERSION!\n    #\n    # <|assistant|>\n    #   The assistant message marker\n    #   Tells model: \"What follows is what I should say\"\n    #\n    # {answer}</s>\n    #   The actual answer (what we want model to learn)\n    #   </s> at the end = \"end of assistant section\"\n    #   This one was present in the broken version (only one that was!)\n    #\n    # THE BROKEN FORMAT (what was causing garbage):\n    #   <|system|>\n    #   You are helpful...      <- NO </s> HERE!\n    #   <|user|>\n    #   {question}              <- NO </s> HERE!\n    #   <|assistant|>\n    #   {answer}</s>\n    #\n    # THE FIXED FORMAT (what we're using now):\n    #   <|system|>\n    #   You are helpful...</s>  <- </s> ADDED!\n    #   <|user|>\n    #   {question}</s>          <- </s> ADDED!\n    #   <|assistant|>\n    #   {answer}</s>\n    #\n    # WHY </s> AFTER EACH SECTION:\n    #   </s> = End Of Sequence token (token ID 2)\n    #   TinyLlama was trained with </s> marking section boundaries\n    #   It's like punctuation for the model\n    #   \n    #   Without </s>:\n    #     Model sees: \"<|system|>\\nYou are helpful\\n<|user|>\"\n    #     Thinks: \"Is system message still going? Where does it end?\"\n    #     Gets confused about boundaries between sections\n    #\n    #   With </s>:\n    #     Model sees: \"<|system|>\\nYou are helpful</s>\\n<|user|>\"\n    #     Thinks: \"System message ended, now user section starts\"\n    #     Clear boundaries = understands the structure\n    #\n    # THE f\"\"\" SYNTAX:\n    #   f = f-string (allows {variable} insertions)\n    #   \"\"\" = multi-line string (can span multiple lines)\n    #   Combined: multi-line string with variable insertion\n    #\n    # WHY MULTI-LINE:\n    #   - Easier to read and verify the format\n    #   - Newlines are preserved exactly as written\n    #   - No need for \\n escape characters everywhere\n    #\n    # WHITESPACE MATTERS:\n    #   The prompt must be EXACTLY like this\n    #   No extra spaces, no missing newlines\n    #   Even invisible whitespace differences cause problems!\n    #\n    #   That's why we put the string at column 0 (no indentation)\n    #   If we indented it, we'd get extra spaces in the prompt\n\n    return {\"text\": prompt}\n    # Return a dictionary with key \"text\"\n    #\n    # WHY A DICT WITH \"text\":\n    #   HuggingFace datasets work with dictionaries\n    #   .map() expects function to return dict\n    #   \"text\" is the column name where formatted prompt goes\n    #   SFTTrainer will look for \"text\" column during training\n    #\n    # The full prompt is now one string in the \"text\" field\n    # Trainer will tokenize this and use it for training\n\n# Test it\nsample = train_data[0]\n# Get the first example from our dataset\n# sample = {'question': 'Hi', 'answer': 'Hello there. Tell me how are you feeling today?'}\n\nformatted = format_prompt(sample)\n# Apply our format function to this one example\n# formatted = {'text': '<|system|>\\nYou are...\\n...'}\n\nprint(\"Formatted prompt (notice </s> after each section!):\")\nprint(formatted['text'])\n# Print so we can visually verify the format is correct\n#\n# OUTPUT:\n#   <|system|>\n#   You are a helpful mental health assistant. Provide supportive, empathetic, and informative responses.</s>\n#   <|user|>\n#   Hi</s>\n#   <|assistant|>\n#   Hello there. Tell me how are you feeling today?</s>\n#\n# WHAT TO CHECK:\n#   1. </s> after system message? YES\n#   2. </s> after user message (Hi)? YES\n#   3. </s> after assistant message? YES\n#   4. All special tokens present? YES (<|system|>, <|user|>, <|assistant|>)\n#   5. Newlines in right places? YES\n#\n# If any of these are wrong, training might \"work\" but model outputs garbage\n#\n# WHY WE TEST ON ONE EXAMPLE:\n#   Always inspect your data before training!\n#   Easy to make typos in the format string\n#   5 seconds of checking saves hours of debugging later\n\n# ============================================================\n# THE FULL PICTURE: WHAT MODEL LEARNS\n# ============================================================\n#\n# During training, model sees hundreds of examples like:\n#\n#   <|system|>\n#   You are a helpful mental health assistant...</s>\n#   <|user|>\n#   Hi</s>\n#   <|assistant|>\n#   Hello there! How are you feeling today?</s>\n#\n#   <|system|>\n#   You are a helpful mental health assistant...</s>\n#   <|user|>\n#   I'm feeling anxious</s>\n#   <|assistant|>\n#   I'm sorry to hear that. Anxiety can be really challenging...</s>\n#\n# Model learns the PATTERN:\n#   \"When I see <|system|>...<|user|>...question</s><|assistant|>\"\n#   \"I should generate a helpful mental health response\"\n#\n# The loss function teaches it:\n#   - Given everything before <|assistant|>\n#   - Predict the tokens that come after\n#   - Get better at matching the training answers\n#\n# ============================================================\n# DURING INFERENCE (using the trained model)\n# ============================================================\n#\n# We give the model:\n#   <|system|>\n#   You are a helpful mental health assistant...</s>\n#   <|user|>\n#   How do I manage stress?</s>\n#   <|assistant|>\n#   <- MODEL GENERATES FROM HERE\n#\n# Model recognizes the pattern and generates an appropriate response\n# Because it learned: \"after <|assistant|>\\n, I produce mental health advice\"\n#\n# IF THE FORMAT DOESN'T MATCH:\n#   Training: <|system|>...\\n<|user|>...\\n<|assistant|>...\n#   Inference: <|system|>...NO NEWLINE<|user|>...\n#   Model: \"This looks different, I don't recognize this pattern\"\n#   Result: Garbage output\n#\n# That's why we're so careful about EXACT format matching!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:56.646054Z","iopub.execute_input":"2025-12-30T16:05:56.646431Z","iopub.status.idle":"2025-12-30T16:05:56.656073Z","shell.execute_reply.started":"2025-12-30T16:05:56.646405Z","shell.execute_reply":"2025-12-30T16:05:56.655331Z"}},"outputs":[{"name":"stdout","text":"Formatted prompt (notice </s> after each section!):\n<|system|>\nYou are a helpful mental health assistant. Provide supportive, empathetic, and informative responses.</s>\n<|user|>\nHi</s>\n<|assistant|>\nHello there. Tell me how are you feeling today?</s>\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# FORMAT ALL DATA\nprint(\"Formatting dataset...\")\n# Just a status message so you know something's happening\n# With 661 examples it's fast, but good habit for larger datasets\n\nformatted_dataset = train_data.map(\n    # .map() = apply a function to EVERY example in the dataset\n    #\n    # Think of it like a for loop but optimized:\n    #\n    #   WHAT .map() DOES (conceptually):\n    #   results = []\n    #   for example in train_data:\n    #       result = format_prompt(example)\n    #       results.append(result)\n    #   formatted_dataset = Dataset.from_list(results)\n    #\n    # But .map() is BETTER than a for loop because:\n    #   - Optimized C code under the hood (faster)\n    #   - Can parallelize across CPU cores (num_proc parameter)\n    #   - Memory efficient (doesn't load everything at once)\n    #   - Integrates with HuggingFace caching system\n    #   - Shows a nice progress bar\n    #\n    # HOW IT WORKS:\n    #   1. Takes each example: {'question': 'Hi', 'answer': 'Hello!'}\n    #   2. Passes it to format_prompt()\n    #   3. Gets back: {'text': '<|system|>\\n...'}\n    #   4. Collects all results into new dataset\n    #\n    # WHY NOT A FOR LOOP:\n    #   For 661 examples, a loop would be fine\n    #   But for 100,000+ examples, .map() is way faster\n    #   Good habit to use .map() even for small datasets\n    #   Plus you get the progress bar for free!\n\n    format_prompt,\n    # The function to apply to each example\n    #\n    # This is our format_prompt function we defined above\n    # Note: we pass the function itself, not format_prompt()\n    #   format_prompt  = \"here's the function, you call it\"\n    #   format_prompt() = \"I'm calling it now\" (wrong!)\n    #\n    # .map() will call format_prompt(example) for each example\n    #\n    # FUNCTION REQUIREMENTS:\n    #   - Input: one example (a dictionary)\n    #   - Output: a dictionary (new/modified columns)\n    #   \n    # Our function:\n    #   - Input: {'question': '...', 'answer': '...'}\n    #   - Output: {'text': '...'}\n\n    remove_columns=train_data.column_names,\n    # remove_columns = delete these columns after mapping\n    #\n    # train_data.column_names = ['question', 'answer']\n    #\n    # BEFORE .map():\n    #   | question          | answer                    |\n    #   |-------------------|---------------------------|\n    #   | Hi                | Hello there!              |\n    #   | How are you?      | I'm doing well!           |\n    #\n    # AFTER .map() WITHOUT remove_columns:\n    #   | question          | answer          | text                      |\n    #   |-------------------|-----------------|---------------------------|\n    #   | Hi                | Hello there!    | <|system|>\\n...           |\n    #   | How are you?      | I'm doing well! | <|system|>\\n...           |\n    #   \n    #   We'd have the old columns PLUS the new 'text' column\n    #   Wastes memory keeping data we don't need anymore\n    #\n    # AFTER .map() WITH remove_columns:\n    #   | text                      |\n    #   |---------------------------|\n    #   | <|system|>\\n...           |\n    #   | <|system|>\\n...           |\n    #\n    #   Only the 'text' column remains\n    #   Clean and memory efficient!\n    #\n    # WHY REMOVE OLD COLUMNS:\n    #   - We don't need 'question' and 'answer' separately anymore\n    #   - They're now embedded in the 'text' column (formatted prompt)\n    #   - Saves memory\n    #   - Cleaner dataset structure\n    #   - SFTTrainer only needs the 'text' column anyway\n    #\n    # HOW train_data.column_names WORKS:\n    #   Returns list of all column names: ['question', 'answer']\n    #   Equivalent to: remove_columns=['question', 'answer']\n    #   But dynamic - if columns change, this still works\n)\n\n# WHAT JUST HAPPENED:\n#\n#   661 examples transformed from:\n#     {'question': 'Hi', 'answer': 'Hello there!'}\n#\n#   To:\n#     {'text': '<|system|>\\nYou are a helpful mental health assistant. Provide supportive, empathetic, and informative responses.</s>\\n<|user|>\\nHi</s>\\n<|assistant|>\\nHello there!</s>'}\n#\n#   All 661 examples now have the correct TinyLlama chat format\n#   Ready for training!\n\n# OTHER USEFUL .map() PARAMETERS:\n#\n#   num_proc=4\n#     Use 4 CPU cores in parallel (faster for big datasets)\n#     Default is 1 (sequential)\n#     Don't set higher than your CPU cores\n#\n#   batched=True\n#     Pass batches of examples instead of one at a time\n#     Function receives: {'question': ['Hi', 'Hey', ...], 'answer': [...]}\n#     Faster for some operations, but we don't need it here\n#\n#   batch_size=1000\n#     If batched=True, how many examples per batch\n#\n#   load_from_cache_file=False\n#     Don't use cached results, always recompute\n#     Useful when debugging and changing the function\n#\n#   desc=\"Processing\"\n#     Custom description for the progress bar\n\nprint(f\"Formatted {len(formatted_dataset)} examples\")\n# len(formatted_dataset) = number of examples in the new dataset\n#\n# OUTPUT: \"Formatted 661 examples\"\n#\n# Should be same as original train_data (661)\n# We didn't add or remove examples, just transformed them\n#\n# If this number was different:\n#   - Less: some examples got filtered out (bad data?)\n#   - More: something weird happened (shouldn't happen with our function)\n#\n# WHY CHECK: Sanity check that .map() worked correctly\n\n# ============================================================\n# QUICK PEEK AT THE RESULT\n# ============================================================\n#\n# You can inspect the formatted data:\n#\n#   print(formatted_dataset[0])\n#   # {'text': '<|system|>\\nYou are...'}\n#\n#   print(formatted_dataset['text'][0])\n#   # '<|system|>\\nYou are...'\n#\n#   print(formatted_dataset[0:3])\n#   # First 3 examples\n#\n# ============================================================\n# WHAT'S NEXT\n# ============================================================\n#\n# formatted_dataset now has 661 examples like:\n#   {'text': '<|system|>\\n...\\n<|user|>\\nHi</s>\\n<|assistant|>\\nHello!</s>'}\n#\n# Next step: Split into train and validation sets\n#   - Training set: what model learns from\n#   - Validation set: what we test on (to check for overfitting)\n#\n# Then: Create the trainer and start training!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:56.657581Z","iopub.execute_input":"2025-12-30T16:05:56.657805Z","iopub.status.idle":"2025-12-30T16:05:56.717780Z","shell.execute_reply.started":"2025-12-30T16:05:56.657784Z","shell.execute_reply":"2025-12-30T16:05:56.717180Z"}},"outputs":[{"name":"stdout","text":"Formatting dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/661 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc8ea61623a54f03a34b612dc44dc4c0"}},"metadata":{}},{"name":"stdout","text":"Formatted 661 examples\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# TRAIN/VALIDATION SPLIT\nsplit_dataset = formatted_dataset.train_test_split(\n    test_size=0.1,\n    seed=42,\n)\nprint(f\"Training: {len(split_dataset['train'])} examples\")\nprint(f\"Validation: {len(split_dataset['test'])} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:05:56.718631Z","iopub.execute_input":"2025-12-30T16:05:56.718849Z","iopub.status.idle":"2025-12-30T16:05:56.728103Z","shell.execute_reply.started":"2025-12-30T16:05:56.718829Z","shell.execute_reply":"2025-12-30T16:05:56.727504Z"}},"outputs":[{"name":"stdout","text":"Training: 594 examples\nValidation: 67 examples\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"## Chapter 8: Training Config","metadata":{}},{"cell_type":"code","source":"# TRAINING CONFIGURATION\n#\n# This is the CONTROL PANEL for training\n# Every setting here affects how the model learns\n# Wrong settings = wasted hours, bad results\n# Right settings = efficient training, good model\n#\n# Think of it like a recipe:\n#   - Ingredients (data) - already prepared\n#   - Oven temperature (learning rate) - too hot burns it, too cold undercooks\n#   - Cooking time (epochs) - too long = overdone, too short = raw\n#   - etc.\n\noutput_dir = \"./fine_tuned_model\"\n# Where to save checkpoints and final model\n#\n# During training, the trainer will save:\n#   ./fine_tuned_model/checkpoint-100/  (after 100 steps)\n#   ./fine_tuned_model/checkpoint-200/  (after 200 steps)\n#   etc.\n#\n# \"./\" means current directory\n# So it creates a folder called \"fine_tuned_model\" right where you are\n#\n# WHY SAVE CHECKPOINTS:\n#   - If training crashes, you can resume from last checkpoint\n#   - Can compare different points in training\n#   - load_best_model_at_end uses these to find the best one\n\nos.makedirs(output_dir, exist_ok=True)\n# Create the output directory if it doesn't exist\n#\n# os.makedirs() = create folder (and parent folders if needed)\n# exist_ok=True = don't crash if folder already exists\n#\n# Without exist_ok=True:\n#   First run: creates folder, works fine\n#   Second run: folder exists, CRASHES with FileExistsError\n#\n# With exist_ok=True:\n#   First run: creates folder\n#   Second run: folder exists, that's fine, continue\n#\n# WHY WE NEED IT: Training will try to save files here, folder must exist\n\ntraining_args = SFTConfig(\n    # SFTConfig = Supervised Fine-Tuning Configuration\n    # All training hyperparameters in one object\n    #\n    # This is from the TRL library (trl.SFTConfig)\n    # Alternative: TrainingArguments from transformers (similar but less SFT-specific)\n    #\n    # We pass this to SFTTrainer later\n    # Trainer reads all settings from this config\n\n    output_dir=output_dir,\n    # Where to save stuff (we defined this above)\n    # Checkpoints, logs, final model all go here\n\n    # ============================================================\n    # TRAINING DURATION\n    # ============================================================\n    \n    num_train_epochs=5,\n    # num_train_epochs = how many times to go through ALL the data\n    #\n    # 1 epoch = model sees every training example exactly once\n    # 5 epochs = model sees every example 5 times\n    #\n    # EXAMPLE WITH OUR DATA:\n    #   We have ~594 training examples (after 90/10 split)\n    #   1 epoch = 594 examples processed\n    #   5 epochs = 594 × 5 = 2,970 total examples processed\n    #\n    # WHY MULTIPLE EPOCHS:\n    #   Model doesn't learn everything in one pass\n    #   Like studying for an exam - you read notes multiple times\n    #   Each pass, model picks up patterns it missed before\n    #\n    # HOW MANY EPOCHS TO USE:\n    #   1-2 epochs:  Large datasets (millions of examples)\n    #   3-5 epochs:  Medium datasets (thousands of examples) <- US\n    #   5-10 epochs: Small datasets (hundreds of examples)\n    #   10+ epochs:  Tiny datasets (risk of overfitting!)\n    #\n    # SIGNS OF TOO MANY EPOCHS (overfitting):\n    #   Training loss keeps going down\n    #   Validation loss starts going UP\n    #   Model memorizes training data instead of learning patterns\n    #\n    # SIGNS OF TOO FEW EPOCHS:\n    #   Both losses still decreasing when training ends\n    #   Model outputs are generic/bad\n    #   Didn't learn enough\n    #\n    # WHY 5: Good starting point for ~600 examples\n\n    # ============================================================\n    # BATCH SIZE\n    # ============================================================\n    \n    per_device_train_batch_size=4,\n    # How many examples to process at once PER GPU\n    #\n    # WHAT IS A BATCH:\n    #   Instead of processing 1 example at a time:\n    #     forward(example1), backward(), update weights\n    #     forward(example2), backward(), update weights\n    #     ... (very slow!)\n    #\n    #   We process multiple examples together:\n    #     forward([example1, example2, example3, example4]), backward(), update weights\n    #     (much faster! GPUs love parallel work)\n    #\n    # BATCH SIZE TRADEOFFS:\n    #   Larger batch (8, 16, 32):\n    #     + Faster (more GPU parallelism)\n    #     + More stable gradients (averaged over more examples)\n    #     - Uses more GPU memory\n    #     - Might generalize worse (debated)\n    #\n    #   Smaller batch (1, 2, 4):\n    #     + Uses less memory\n    #     + Can fit on smaller GPUs\n    #     - Noisier gradients (more variance)\n    #     - Slower (less parallelism)\n    #\n    # HOW TO CHOOSE:\n    #   Start with largest batch that fits in memory\n    #   If you get \"CUDA out of memory\", reduce batch size\n    #   4 is conservative and works on most free GPUs\n    #\n    # \"per_device\" because if you have multiple GPUs:\n    #   2 GPUs × batch_size 4 = 8 examples per step total\n    #   We have 1 GPU, so it's just 4\n\n    per_device_eval_batch_size=4,\n    # Same but for validation/evaluation\n    #\n    # Can usually be LARGER than train batch because:\n    #   - No gradients stored during eval (forward pass only)\n    #   - Uses less memory than training\n    #\n    # We keep it same as train for simplicity\n    # Could set to 8 or 16 to speed up evaluation\n\n    gradient_accumulation_steps=4,\n    # Accumulate gradients over N steps before updating weights\n    #\n    # THE PROBLEM:\n    #   We want large effective batch size (stable training)\n    #   But large batches don't fit in GPU memory\n    #\n    # THE SOLUTION:\n    #   Accumulate gradients from multiple small batches\n    #   Then do one big weight update\n    #\n    # HOW IT WORKS:\n    #   Step 1: forward(batch1), compute gradients, DON'T update yet, SAVE gradients\n    #   Step 2: forward(batch2), compute gradients, ADD to saved gradients\n    #   Step 3: forward(batch3), compute gradients, ADD to saved gradients\n    #   Step 4: forward(batch4), compute gradients, ADD to saved gradients\n    #   NOW: average all gradients, update weights\n    #\n    # EFFECTIVE BATCH SIZE:\n    #   per_device_batch × gradient_accumulation × num_gpus\n    #   4 × 4 × 1 = 16\n    #\n    #   It's LIKE training with batch size 16\n    #   But only batch size 4 in memory at once!\n    #\n    # WHY 4:\n    #   Gives us effective batch of 16 (good for stability)\n    #   Without needing 4× the GPU memory\n    #\n    # TRADEOFF:\n    #   More accumulation = slower (more forward passes per update)\n    #   But same final result as larger batch\n\n    # ============================================================\n    # LEARNING RATE\n    # ============================================================\n    \n    learning_rate=5e-5,\n    # How big of a step to take when updating weights\n    #\n    # 5e-5 = 0.00005 = 0.005%\n    #\n    # THE MOST IMPORTANT HYPERPARAMETER\n    # This controls how fast the model learns\n    #\n    # WHAT LEARNING RATE DOES:\n    #   new_weight = old_weight - learning_rate × gradient\n    #\n    #   Gradient says \"move this direction to reduce loss\"\n    #   Learning rate says \"move THIS MUCH in that direction\"\n    #\n    # TOO HIGH (1e-3, 1e-2):\n    #   - Takes huge steps\n    #   - Overshoots the optimal values\n    #   - Loss goes crazy (spikes, NaN)\n    #   - Model learns garbage\n    #\n    # TOO LOW (1e-7, 1e-8):\n    #   - Takes tiny steps\n    #   - Barely moves from starting point\n    #   - Would need millions of steps to learn anything\n    #   - Wasted time\n    #\n    # JUST RIGHT (1e-5 to 1e-4 for fine-tuning):\n    #   - Steady progress\n    #   - Loss decreases smoothly\n    #   - Model learns without going crazy\n    #\n    # COMMON LEARNING RATES:\n    #   Pre-training from scratch: 1e-4 to 1e-3\n    #   Full fine-tuning: 1e-5 to 5e-5\n    #   LoRA fine-tuning: 1e-5 to 2e-4\n    #\n    # WHY 5e-5:\n    #   Conservative choice for LoRA\n    #   Less likely to destabilize training\n    #   Works well with our alpha=128 (2× scaling)\n    #   If too slow, could try 1e-4\n\n    lr_scheduler_type=\"cosine\",\n    # How learning rate changes during training\n    #\n    # CONSTANT (no scheduler):\n    #   Learning rate stays 5e-5 the whole time\n    #   Simple but not optimal\n    #\n    # LINEAR:\n    #   Starts at 5e-5, decreases linearly to 0\n    #   Step 0: 5e-5\n    #   Step 500: 2.5e-5\n    #   Step 1000: 0\n    #\n    # COSINE (what we use):\n    #   Follows a cosine curve from max to min\n    #   Starts at 5e-5\n    #   Slowly decreases (slower than linear at start)\n    #   Accelerates decrease toward end\n    #   Smoothly reaches near-zero\n    #\n    #   It looks like this:\n    #   LR |‾‾‾‾\\\n    #      |     \\\n    #      |      \\_____\n    #      +------------- Steps\n    #\n    # WHY COSINE:\n    #   - Allows faster learning at start (when there's lots to learn)\n    #   - Slows down at end (fine-tuning, don't overshoot)\n    #   - Empirically works well for many tasks\n    #   - Industry standard for most fine-tuning\n    #\n    # OTHER OPTIONS:\n    #   \"linear\": straight line decrease\n    #   \"polynomial\": customizable curve\n    #   \"constant\": no change\n    #   \"constant_with_warmup\": constant after warmup\n\n    warmup_ratio=0.1,\n    # What fraction of training to \"warm up\" the learning rate\n    #\n    # WARMUP = start with tiny LR and gradually increase to full LR\n    #\n    # WHY WARMUP:\n    #   At the very start, model weights are \"cold\"\n    #   Gradients can be wild and unstable\n    #   Big learning rate + wild gradients = disaster\n    #\n    #   Warmup says: \"Start gentle, then ramp up\"\n    #   Lets the model stabilize before going full speed\n    #\n    # 0.1 = 10% of training steps are warmup\n    #\n    # EXAMPLE:\n    #   Total steps: 500\n    #   Warmup: 10% × 500 = 50 steps\n    #\n    #   Step 0-50: LR increases from 0 to 5e-5 (warmup)\n    #   Step 50-500: LR follows cosine schedule from 5e-5 down\n    #\n    # HOW TO CHOOSE:\n    #   0.03-0.1: typical range\n    #   Small dataset: 0.1 (more warmup relative to short training)\n    #   Large dataset: 0.03 (don't waste steps on warmup)\n    #\n    # WHY 0.1:\n    #   Our training is relatively short\n    #   10% warmup adds stability without losing much training time\n\n    # ============================================================\n    # PRECISION\n    # ============================================================\n    \n    bf16=True,\n    # Use bfloat16 precision for training computations\n    #\n    # PRECISION OPTIONS:\n    #   fp32 (32-bit float): highest precision, slowest, most memory\n    #   fp16 (16-bit float): half precision, fast, can overflow\n    #   bf16 (bfloat16): half precision, fast, STABLE (no overflow)\n    #\n    # BFLOAT16 ADVANTAGES:\n    #   - 2× faster than fp32 on modern GPUs\n    #   - 2× less memory than fp32\n    #   - Same numerical RANGE as fp32 (no overflow)\n    #   - Slightly less PRECISION than fp32 (fine for training)\n    #\n    # WHY NOT FP16:\n    #   fp16 has smaller range of values\n    #   Very large or small numbers become infinity or zero\n    #   Causes NaN losses during training\n    #   Needs \"loss scaling\" tricks to work (extra complexity)\n    #\n    # WHY BF16:\n    #   Best of both worlds\n    #   Fast like fp16\n    #   Stable like fp32\n    #   No special tricks needed\n    #\n    # REQUIREMENT:\n    #   bf16 needs Ampere GPU or newer (RTX 30xx, A100, etc.)\n    #   T4 and P100 don't have native bf16 (but it still works via emulation)\n    #   If you get errors, try fp16=True instead\n    #\n    # WHY True: Faster training, lower memory, stable\n\n    # ============================================================\n    # REGULARIZATION\n    # ============================================================\n    \n    weight_decay=0.05,\n    # L2 regularization - penalize large weights\n    #\n    # WHAT IT DOES:\n    #   Adds a penalty to the loss based on weight magnitudes\n    #   loss = original_loss + weight_decay × sum(weights²)\n    #\n    #   Model is punished for having large weights\n    #   Encourages smaller, more distributed weights\n    #\n    # WHY IT HELPS:\n    #   Large weights = model is \"memorizing\" specific examples\n    #   Small weights = model learning general patterns\n    #   Prevents overfitting\n    #\n    # EXAMPLE:\n    #   Without weight decay: model might learn\n    #     \"When input is EXACTLY 'Hi', output EXACTLY 'Hello there!'\"\n    #   With weight decay: model learns\n    #     \"Greetings should be responded to with greetings\"\n    #\n    # HOW TO CHOOSE:\n    #   0.0: no regularization\n    #   0.01: light regularization\n    #   0.05: moderate regularization (us)\n    #   0.1: strong regularization\n    #\n    # WHY 0.05:\n    #   We have small dataset (661 examples)\n    #   Risk of overfitting is real\n    #   0.05 is moderate - not too aggressive\n\n    max_grad_norm=0.5,\n    # Gradient clipping - cap gradient magnitudes\n    #\n    # WHAT IT DOES:\n    #   If gradient magnitude > 0.5, scale it down to 0.5\n    #   Prevents \"exploding gradients\"\n    #\n    # THE PROBLEM:\n    #   Sometimes gradients get HUGE (gradient explosion)\n    #   Huge gradient × learning rate = massive weight update\n    #   Massive update = model goes crazy\n    #   Common with small batches and certain architectures\n    #\n    # THE SOLUTION:\n    #   \"Clip\" gradients to max magnitude\n    #   Direction preserved, magnitude limited\n    #   Wild gradients become manageable gradients\n    #\n    # EXAMPLE:\n    #   Gradient = [100, -200, 50]\n    #   Magnitude = sqrt(100² + 200² + 50²) = 226\n    #   max_grad_norm = 0.5\n    #   Since 226 > 0.5, scale down: [0.22, -0.44, 0.11]\n    #\n    # HOW TO CHOOSE:\n    #   1.0: standard, light clipping\n    #   0.5: moderate clipping (us)\n    #   0.1: aggressive clipping\n    #\n    # WHY 0.5:\n    #   More conservative than default 1.0\n    #   Extra stability for our small dataset\n    #   Prevents any single bad batch from ruining training\n\n    # ============================================================\n    # EVALUATION\n    # ============================================================\n    \n    eval_strategy=\"steps\",\n    # When to run evaluation on validation set\n    #\n    # OPTIONS:\n    #   \"no\": never evaluate (fastest, but flying blind)\n    #   \"epoch\": evaluate after each epoch\n    #   \"steps\": evaluate every N steps (most control)\n    #\n    # WHY \"steps\":\n    #   We want to see progress during training\n    #   Not just at end of each epoch\n    #   Can catch overfitting earlier\n    #\n    # Combined with eval_steps below\n\n    eval_steps=50,\n    # Evaluate every 50 training steps\n    #\n    # A \"step\" = one weight update\n    # With gradient_accumulation_steps=4 and batch_size=4:\n    #   1 step = 16 examples processed\n    #   50 steps = 800 examples = ~1.3 epochs\n    #\n    # WHAT HAPPENS DURING EVAL:\n    #   1. Pause training\n    #   2. Run model on ALL validation examples\n    #   3. Calculate validation loss\n    #   4. Log the metrics\n    #   5. Resume training\n    #\n    # WHY 50:\n    #   Often enough to see trends\n    #   Not so often that it slows down training\n    #   With ~185 steps per epoch, we evaluate ~4 times per epoch\n\n    # ============================================================\n    # LOGGING\n    # ============================================================\n    \n    logging_steps=25,\n    # Log training metrics every 25 steps\n    #\n    # WHAT GETS LOGGED:\n    #   - Training loss (how wrong model is on training data)\n    #   - Learning rate (current LR after scheduler)\n    #   - Gradient norm (are gradients exploding?)\n    #   - Speed (samples per second, steps per second)\n    #\n    # More frequent than eval (eval is expensive, logging is cheap)\n    # Lets you see training progress in real-time\n    #\n    # WHY 25:\n    #   See progress without flooding the screen\n    #   Every 25 steps ≈ every few minutes of training\n\n    logging_first_step=True,\n    # Log metrics after the very first step\n    #\n    # WHY:\n    #   See initial loss immediately\n    #   Useful for sanity checking\n    #   If first loss is NaN or 1000+, something is wrong\n    #\n    #   Normal initial loss: 2-4 (for language models)\n    #   Suspiciously high: 10+ (maybe wrong format)\n    #   NaN: definitely broken (precision issues, bad data)\n\n    # ============================================================\n    # CHECKPOINTING\n    # ============================================================\n    \n    save_strategy=\"steps\",\n    # When to save model checkpoints\n    #\n    # OPTIONS:\n    #   \"no\": never save (risky!)\n    #   \"epoch\": save after each epoch\n    #   \"steps\": save every N steps\n    #\n    # WHY \"steps\":\n    #   More frequent saves = less lost work if crash\n    #   Can analyze different training stages\n    #   Combined with save_steps below\n\n    save_steps=100,\n    # Save checkpoint every 100 steps\n    #\n    # WHAT GETS SAVED:\n    #   - Model weights (the LoRA adapters)\n    #   - Optimizer state (momentum, etc.)\n    #   - Scheduler state (where in LR schedule)\n    #   - Training progress (which step/epoch)\n    #\n    # This creates folders like:\n    #   ./fine_tuned_model/checkpoint-100/\n    #   ./fine_tuned_model/checkpoint-200/\n    #\n    # WHY 100:\n    #   Often enough to not lose too much progress\n    #   Not so often that disk fills up with checkpoints\n    #   Balance between safety and disk space\n\n    save_total_limit=3,\n    # Only keep the 3 most recent checkpoints\n    #\n    # WITHOUT THIS:\n    #   checkpoint-100, checkpoint-200, checkpoint-300, ...\n    #   Could end up with dozens of checkpoints\n    #   Each is ~200MB, adds up fast!\n    #\n    # WITH save_total_limit=3:\n    #   Keeps: checkpoint-700, checkpoint-800, checkpoint-900\n    #   Deletes: checkpoint-100 through checkpoint-600\n    #   Auto-cleanup saves disk space\n    #\n    # EXCEPTION:\n    #   The \"best\" checkpoint (if tracking) is always kept\n    #   Won't delete the best one even if it's old\n    #\n    # WHY 3: Enough to recover from recent issues, not too much space\n\n    load_best_model_at_end=True,\n    # After training finishes, load the best checkpoint (not the last one)\n    #\n    # THE PROBLEM:\n    #   Training might overshoot\n    #   Step 500: loss 0.4 (good!)\n    #   Step 600: loss 0.45 (overfitting...)\n    #   Step 700: loss 0.5 (worse!)\n    #\n    #   Without this: you get step 700 model (the worst one!)\n    #   With this: you get step 500 model (the best one!)\n    #\n    # HOW IT WORKS:\n    #   Trainer tracks eval loss at each evaluation\n    #   Remembers which checkpoint had lowest eval loss\n    #   At the very end, loads that best checkpoint\n    #\n    # REQUIRES:\n    #   save_strategy must be set (need checkpoints to load)\n    #   eval_strategy must be set (need eval metrics to compare)\n    #\n    # WHY True: Automatically get the best model, not just the last one\n\n    metric_for_best_model=\"eval_loss\",\n    # Which metric to use when determining \"best\" checkpoint\n    #\n    # OPTIONS:\n    #   \"eval_loss\": validation loss (lower is better) <- MOST COMMON\n    #   \"accuracy\": if you compute accuracy (higher is better)\n    #   \"f1\": if you compute F1 score (higher is better)\n    #   Any custom metric you log\n    #\n    # For language models, eval_loss is standard\n    # It measures: \"how well does model predict validation data?\"\n    #\n    # WHY \"eval_loss\": Direct measure of model quality on unseen data\n\n    greater_is_better=False,\n    # Is a HIGHER metric better, or LOWER?\n    #\n    # False = lower is better (loss, error rate)\n    # True = higher is better (accuracy, F1 score)\n    #\n    # eval_loss: lower is better, so False\n    #\n    # EXAMPLE:\n    #   Checkpoint A: eval_loss = 0.5\n    #   Checkpoint B: eval_loss = 0.4\n    #   \n    #   greater_is_better=False\n    #   B is better (0.4 < 0.5)\n    #\n    # WHY False: We're using loss, lower loss = better model\n\n    seed=42,\n    # Random seed for reproducibility\n    #\n    # WHAT IT AFFECTS:\n    #   - Data shuffling order\n    #   - Dropout randomness\n    #   - Weight initialization (for new layers)\n    #\n    # WHY SET IT:\n    #   Same seed = same random choices = same results\n    #   Can reproduce experiments\n    #   Can compare changes fairly (only your change differs)\n    #\n    # WHY 42:\n    #   It's a meme (Hitchhiker's Guide to the Galaxy)\n    #   \"The answer to life, the universe, and everything\"\n    #   Any number works, 42 is tradition in ML\n\n    report_to=\"none\",\n    # Where to send training metrics\n    #\n    # OPTIONS:\n    #   \"none\": just local logging\n    #   \"tensorboard\": send to TensorBoard (visualization tool)\n    #   \"wandb\": send to Weights & Biases (popular tracking service)\n    #   \"mlflow\": send to MLflow\n    #\n    # WHY \"none\":\n    #   Keep it simple for this tutorial\n    #   No external accounts needed\n    #   Metrics still print to console\n    #\n    # IN PRODUCTION:\n    #   Would use \"wandb\" or \"tensorboard\" for better tracking\n    #   Nice graphs, comparison tools, experiment management\n)\n\nprint(\"Training config ready!\")\n# Confirms all settings were accepted without errors\n#\n# If any parameter name was wrong, we'd get an error here\n# Success means config is valid and ready for trainer\n#\n# NEXT: Create SFTTrainer with this config and start training!\n\n# ============================================================\n# SUMMARY OF OUR SETTINGS\n# ============================================================\n#\n# Training Duration:\n#   5 epochs, ~185 steps per epoch, ~925 total steps\n#\n# Batch Size:\n#   Actual: 4\n#   Effective: 16 (with 4× accumulation)\n#\n# Learning Rate:\n#   Start: 5e-5\n#   Schedule: cosine decay\n#   Warmup: 10% of training\n#\n# Regularization:\n#   Weight decay: 0.05\n#   Gradient clipping: 0.5\n#   (Plus LoRA dropout 0.1 from lora_config)\n#\n# Monitoring:\n#   Log every 25 steps\n#   Eval every 50 steps\n#   Save every 100 steps\n#\n# Safety:\n#   Keep best model (by eval_loss)\n#   Keep last 3 checkpoints\n#   bf16 precision for stability","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:07:26.409595Z","iopub.execute_input":"2025-12-30T16:07:26.409936Z","iopub.status.idle":"2025-12-30T16:07:26.438827Z","shell.execute_reply.started":"2025-12-30T16:07:26.409900Z","shell.execute_reply":"2025-12-30T16:07:26.438242Z"}},"outputs":[{"name":"stdout","text":"Training config ready!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## Chapter 9: Training!","metadata":{}},{"cell_type":"code","source":"# CREATE TRAINER\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=split_dataset[\"train\"],\n    eval_dataset=split_dataset[\"test\"],\n    processing_class=tokenizer,\n)\n\nprint(f\"Trainer ready!\")\nprint(f\"  Train: {len(trainer.train_dataset)} examples\")\nprint(f\"  Eval: {len(trainer.eval_dataset)} examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:07:30.618434Z","iopub.execute_input":"2025-12-30T16:07:30.619152Z","iopub.status.idle":"2025-12-30T16:07:30.992276Z","shell.execute_reply.started":"2025-12-30T16:07:30.619121Z","shell.execute_reply":"2025-12-30T16:07:30.991530Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e1dd0be5df48d0ad3ddf65c49c2403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acc77a471d774ca38bf72c220da32d0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/594 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4af69bf76cb45fabb810eb037b38631"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to eval dataset:   0%|          | 0/67 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6067c68dc208401c9c52d2fe3fdf51d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/67 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d450b30330349539a816e63fd288e50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/67 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f631e156ba3440d19581b17e1f0ac431"}},"metadata":{}},{"name":"stdout","text":"Trainer ready!\n  Train: 594 examples\n  Eval: 67 examples\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# TRAIN!\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"Starting training...\")\nprint(\"Watch the loss - should decrease over time!\")\nprint()\n\ntrain_result = trainer.train()\n\nprint(\"\\nTraining complete!\")\nprint(f\"Final loss: {train_result.training_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:07:33.664993Z","iopub.execute_input":"2025-12-30T16:07:33.665753Z","iopub.status.idle":"2025-12-30T16:19:29.647935Z","shell.execute_reply.started":"2025-12-30T16:07:33.665723Z","shell.execute_reply":"2025-12-30T16:19:29.647288Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\nWatch the loss - should decrease over time!\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='190' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [190/190 11:51, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Entropy</th>\n      <th>Num Tokens</th>\n      <th>Mean Token Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.565000</td>\n      <td>0.523949</td>\n      <td>0.534874</td>\n      <td>58328.000000</td>\n      <td>0.865922</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.341600</td>\n      <td>0.374702</td>\n      <td>0.365874</td>\n      <td>117453.000000</td>\n      <td>0.904407</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.226500</td>\n      <td>0.344615</td>\n      <td>0.316968</td>\n      <td>175305.000000</td>\n      <td>0.911803</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nTraining complete!\nFinal loss: 0.4685\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# SAVE MODEL\n#\n# Training is done! Now we need to save our work\n# Otherwise when the notebook closes, everything is GONE\n#\n# WHAT WE'RE SAVING:\n#   Not the entire 1.1B parameter model (that would be huge)\n#   Just the LoRA adapters (~50M parameters)\n#   The tiny \"diff\" that makes base model -> our fine-tuned model\n#\n# IT'S LIKE:\n#   You don't save a whole new copy of Photoshop when you edit a photo\n#   You save the edits (layers, adjustments)\n#   LoRA adapters = the edits\n#   Base model = Photoshop (stays unchanged, download again later)\n\nlora_output_dir = \"./fine_tuned_lora\"\n# Where to save our LoRA adapters\n#\n# Different from output_dir (\"./fine_tuned_model\") which has checkpoints\n# This is the FINAL clean save location\n#\n# WHY SEPARATE FOLDER:\n#   output_dir has checkpoints, optimizer states, training logs\n#   Messy, lots of files, some are huge\n#   \n#   lora_output_dir has just the final model\n#   Clean, minimal, ready to share or deploy\n#\n# \"./\" means current directory\n# Creates folder called \"fine_tuned_lora\" right here\n\nos.makedirs(lora_output_dir, exist_ok=True)\n# Create the directory if it doesn't exist\n#\n# os.makedirs() = create folder (and any parent folders needed)\n# exist_ok=True = don't crash if folder already exists\n#\n# Same pattern we used for output_dir earlier\n# Good habit: always create dir before saving to it\n\ntrainer.model.save_pretrained(lora_output_dir)\n# Save the LoRA adapter weights\n#\n# LET'S BREAK THIS DOWN:\n#\n# trainer.model\n#   The trained model inside our SFTTrainer\n#   This is the PeftModel (base TinyLlama + LoRA adapters)\n#   After training, LoRA weights have been optimized\n#\n# .save_pretrained(path)\n#   Save model in HuggingFace format\n#   Creates files that can be loaded with .from_pretrained() later\n#\n# WHAT IT SAVES (for LoRA/PEFT models):\n#   adapter_config.json     - LoRA configuration (r, alpha, target_modules, etc.)\n#   adapter_model.safetensors - The actual LoRA weights (~200MB)\n#\n# WHAT IT DOES NOT SAVE:\n#   The base TinyLlama weights (1.1B params)\n#   Those stay on Hugging Face Hub\n#   We just save our tiny adapters\n#\n# THIS IS THE MAGIC OF LORA:\n#   Full model save: ~2.2 GB (or 4.4GB for fp32)\n#   LoRA adapter save: ~200 MB\n#   10x smaller!\n#\n# To use the model later:\n#   1. Load base TinyLlama (from Hub or cache)\n#   2. Load LoRA adapters (from this folder)\n#   3. Combine them\n#   Done!\n#\n# WHY .safetensors FORMAT:\n#   Newer, safer format than .bin\n#   Can't contain malicious code (unlike pickle-based .bin)\n#   Faster to load\n#   Industry standard now\n\ntokenizer.save_pretrained(lora_output_dir)\n# Save the tokenizer too\n#\n# \"Wait, we didn't change the tokenizer, why save it?\"\n#\n# GOOD PRACTICE REASONS:\n#   1. Everything needed in one folder\n#      Don't need to remember \"use TinyLlama tokenizer\"\n#      Just load from this folder, everything's there\n#\n#   2. Future compatibility\n#      What if TinyLlama tokenizer changes?\n#      You have YOUR version saved\n#\n#   3. Easier deployment\n#      Copy one folder, everything works\n#      No hunting for matching tokenizer\n#\n# WHAT IT SAVES:\n#   tokenizer.json          - The actual vocabulary and rules\n#   tokenizer_config.json   - Tokenizer settings\n#   special_tokens_map.json - Special tokens (<s>, </s>, etc.)\n#\n# These files are small (a few MB total)\n# Worth saving for convenience\n\nprint(f\"Saved to: {lora_output_dir}\")\n# Confirm where files were saved\n#\n# Output: \"Saved to: ./fine_tuned_lora\"\n#\n# If something went wrong, we'd see an error before this\n# Seeing this message = save successful!\n\nfor f in os.listdir(lora_output_dir):\n    size = os.path.getsize(os.path.join(lora_output_dir, f)) / 1e6\n    print(f\"  {f}: {size:.2f} MB\")\n# List all saved files with their sizes\n#\n# LET'S BREAK IT DOWN:\n#\n# os.listdir(lora_output_dir)\n#   Get list of all files in the folder\n#   Returns: ['adapter_config.json', 'adapter_model.safetensors', 'tokenizer.json', ...]\n#\n# for f in ...:\n#   Loop through each filename\n#\n# os.path.getsize(os.path.join(lora_output_dir, f))\n#   Get file size in bytes\n#   os.path.join() combines folder + filename into full path\n#   \"./fine_tuned_lora\" + \"adapter_model.safetensors\" \n#   = \"./fine_tuned_lora/adapter_model.safetensors\"\n#\n# / 1e6\n#   Convert bytes to megabytes\n#   1e6 = 1,000,000 bytes = 1 MB\n#\n# :.2f\n#   Format as decimal with 2 places\n#   192.847264 -> 192.85\n#\n# TYPICAL OUTPUT:\n#   adapter_config.json: 0.00 MB\n#   adapter_model.safetensors: 192.85 MB\n#   tokenizer.json: 1.84 MB\n#   tokenizer_config.json: 0.00 MB\n#   special_tokens_map.json: 0.00 MB\n#\n# KEY OBSERVATION:\n#   adapter_model.safetensors is ~200 MB\n#   This contains ALL 50 million LoRA parameters\n#   50M params × 4 bytes (float32) = 200 MB (checks out!)\n#\n#   Compare to full model: 1.1B × 4 bytes = 4.4 GB\n#   We're saving 22x less data!\n#\n# WHY PRINT FILE SIZES:\n#   Verify save worked correctly\n#   Confirm files aren't empty (0 bytes = something wrong)\n#   See how much space LoRA actually takes\n#   Satisfying to see how small the adapters are!\n\n# ============================================================\n# WHAT WE SAVED\n# ============================================================\n#\n# ./fine_tuned_lora/\n# ├── adapter_config.json      (~0.5 KB)\n# │   └── LoRA settings: r=64, alpha=128, target_modules, etc.\n# │\n# ├── adapter_model.safetensors (~200 MB)\n# │   └── The actual trained LoRA weights\n# │       22 layers × 7 modules × 2 matrices = 308 LoRA matrices\n# │       This is what we trained!\n# │\n# ├── tokenizer.json           (~1.8 MB)\n# │   └── Full vocabulary (32,000 tokens)\n# │\n# ├── tokenizer_config.json    (~1 KB)\n# │   └── Tokenizer settings\n# │\n# └── special_tokens_map.json  (~0.5 KB)\n#     └── Maps special tokens like <s>, </s>, <pad>\n#\n# Total: ~200 MB (vs 2.2 GB for full model)\n#\n# ============================================================\n# HOW TO USE THESE FILES LATER\n# ============================================================\n#\n# OPTION 1: Load on same machine (files still here)\n#\n#   from peft import PeftModel\n#   \n#   # Load base model\n#   base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n#   \n#   # Load LoRA adapters\n#   model = PeftModel.from_pretrained(base_model, \"./fine_tuned_lora\")\n#   \n#   # Load tokenizer\n#   tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_lora\")\n#\n#\n# OPTION 2: Download the folder and use elsewhere\n#\n#   1. Download the ./fine_tuned_lora folder (zip it or whatever)\n#   2. On new machine, same code as above but with new path\n#\n#\n# OPTION 3: Upload to Hugging Face Hub (share with the world!)\n#\n#   from huggingface_hub import HfApi\n#   api = HfApi()\n#   api.upload_folder(\n#       folder_path=\"./fine_tuned_lora\",\n#       repo_id=\"your-username/mental-health-tinyllama-lora\",\n#       repo_type=\"model\",\n#   )\n#\n#   Then anyone can use:\n#   model = PeftModel.from_pretrained(base_model, \"your-username/mental-health-tinyllama-lora\")\n#\n# ============================================================\n# COMPARE: LORA VS FULL MODEL SAVING\n# ============================================================\n#\n# FULL FINE-TUNING SAVE:\n#   - Save ALL 1.1B parameters\n#   - File size: 2.2 GB (fp16) or 4.4 GB (fp32)\n#   - Self-contained (no need for base model)\n#   - Can't easily switch between fine-tuned versions\n#\n# LORA SAVE (what we did):\n#   - Save only 50M adapter parameters\n#   - File size: ~200 MB\n#   - Need base model to use (download once, reuse)\n#   - Can swap adapters easily!\n#       - Load TinyLlama base\n#       - Add mental-health adapter -> mental health bot\n#       - Remove adapter, add coding adapter -> coding bot\n#       - Same base model, different \"personalities\"\n#\n# LoRA adapters are like costume changes for your model\n# Quick to save, quick to load, easy to swap!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:19:29.649141Z","iopub.execute_input":"2025-12-30T16:19:29.649369Z","iopub.status.idle":"2025-12-30T16:19:30.207281Z","shell.execute_reply.started":"2025-12-30T16:19:29.649344Z","shell.execute_reply":"2025-12-30T16:19:30.206537Z"}},"outputs":[{"name":"stdout","text":"Saved to: ./fine_tuned_lora\n  special_tokens_map.json: 0.00 MB\n  tokenizer.json: 3.62 MB\n  README.md: 0.01 MB\n  tokenizer.model: 0.50 MB\n  adapter_config.json: 0.00 MB\n  tokenizer_config.json: 0.00 MB\n  chat_template.jinja: 0.00 MB\n  adapter_model.safetensors: 201.89 MB\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Chapter 10: Testing!","metadata":{}},{"cell_type":"code","source":"# INFERENCE FUNCTION\n# MUST use same format as training!\n#\n# THIS IS WHERE THE MAGIC BECOMES REAL\n# We trained the model, now we actually USE it\n#\n# THE GOLDEN RULE:\n#   Inference format MUST MATCH training format EXACTLY\n#   Same special tokens, same structure, same everything\n#   Any mismatch = garbage output\n#\n# DURING TRAINING:\n#   Model learned: \"When I see <|system|>...</s><|user|>...</s><|assistant|>\"\n#                  \"I should generate a helpful mental health response\"\n#\n# DURING INFERENCE:\n#   We give it: \"<|system|>...</s><|user|>...</s><|assistant|>\"\n#   Model goes: \"I recognize this pattern! I know what to do!\"\n#   And generates a response\n#\n# If we used different format at inference:\n#   Model: \"Wtf is this? Never seen this pattern before\"\n#   Output: garbage, random text, HTML tags, nonsense\n\ndef generate_response(model, tokenizer, question, max_new_tokens=150):\n    # Function that takes a question and returns model's answer\n    #\n    # PARAMETERS:\n    #   model: our fine-tuned TinyLlama (with LoRA adapters)\n    #   tokenizer: converts text <-> tokens\n    #   question: what the user is asking (string)\n    #   max_new_tokens: maximum length of response (default 150)\n    #\n    # RETURNS:\n    #   The model's response as a string\n    \n    \"\"\"\n    Generate a response. Uses EXACT same format as training.\n    \"\"\"\n    # Docstring - describes what function does\n    # \"EXACT same format\" is emphasized because it's THAT important\n    \n    # Same format as training - with </s> after system and user!\n    prompt = f\"\"\"<|system|>\nYou are a helpful mental health assistant. Provide supportive, empathetic, and informative responses.</s>\n<|user|>\n{question}</s>\n<|assistant|>\n\"\"\"\n    # BUILD THE PROMPT - must match training format!\n    #\n    # LET'S COMPARE:\n    #\n    # TRAINING FORMAT:\n    #   <|system|>\n    #   You are a helpful mental health assistant...</s>\n    #   <|user|>\n    #   {question}</s>\n    #   <|assistant|>\n    #   {answer}</s>              <- answer included\n    #\n    # INFERENCE FORMAT:\n    #   <|system|>\n    #   You are a helpful mental health assistant...</s>\n    #   <|user|>\n    #   {question}</s>\n    #   <|assistant|>\n    #                            <- NO answer, model generates it!\n    #\n    # SAME:\n    #   ✓ <|system|> with message and </s>\n    #   ✓ <|user|> with question and </s>\n    #   ✓ <|assistant|> marker\n    #\n    # DIFFERENT:\n    #   Training: answer is provided (model learns to predict it)\n    #   Inference: answer is missing (model generates it)\n    #\n    # Note: NO </s> after assistant - model generates that\n    #\n    # WHY NO </s> AFTER <|assistant|>:\n    #   If we put </s> there, we're saying \"assistant turn is done\"\n    #   But assistant hasn't said anything yet!\n    #   Model would be confused: \"Turn ended but I didn't speak?\"\n    #\n    #   We leave it open: \"<|assistant|>\\n\"\n    #   Model knows: \"My turn to speak, I should generate until </s>\"\n    #   Model generates: \"Hello! How can I help?</s>\"\n    #   The MODEL produces the </s> when it's done talking\n    #\n    # THIS IS A COMMON MISTAKE:\n    #   People add </s> after <|assistant|> in inference\n    #   Model sees complete conversation, nothing to generate\n    #   Outputs nothing or garbage\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(model.device)\n    # TOKENIZE THE PROMPT\n    #\n    # tokenizer(prompt, ...)\n    #   Converts our text prompt into token IDs\n    #   \"Hello\" -> [15496]\n    #   Our full prompt -> [bunch of token IDs]\n    #\n    # return_tensors=\"pt\"\n    #   Return PyTorch tensors (not lists, not numpy)\n    #   \"pt\" = PyTorch\n    #   \"tf\" = TensorFlow\n    #   \"np\" = NumPy\n    #   Model expects PyTorch tensors\n    #\n    # truncation=True\n    #   If prompt is too long, cut it off\n    #   Without this: error if prompt > max_length\n    #   With this: silently truncate (from the right/end)\n    #\n    # max_length=256\n    #   Maximum prompt length in tokens\n    #   Same as what we used in training\n    #   Longer prompts get truncated\n    #\n    # .to(model.device)\n    #   Move tensors to same device as model (GPU or CPU)\n    #   \n    #   model.device = where the model lives (cuda:0 or cpu)\n    #   Tensors and model MUST be on same device\n    #   GPU model + CPU tensor = error!\n    #   \n    #   .to(model.device) ensures they match\n    #\n    # WHAT inputs CONTAINS:\n    #   {\n    #     'input_ids': tensor([[1, 529, 29989, ...]]),  # token IDs\n    #     'attention_mask': tensor([[1, 1, 1, ...]])    # which tokens to attend to\n    #   }\n\n    with torch.no_grad():\n        # DISABLE GRADIENT COMPUTATION\n        #\n        # During training:\n        #   PyTorch tracks all operations for backpropagation\n        #   Stores intermediate values (activations) for gradient calculation\n        #   Uses lots of memory!\n        #\n        # During inference:\n        #   We're NOT training, just generating\n        #   Don't need gradients\n        #   Don't need to store activations\n        #\n        # torch.no_grad() tells PyTorch:\n        #   \"Don't track operations, don't store gradients\"\n        #   Uses ~3-4x LESS memory\n        #   Also slightly faster\n        #\n        # ALWAYS use torch.no_grad() for inference\n        # Common mistake: forgetting this, running out of memory\n        \n        outputs = model.generate(\n            # .generate() = the text generation function\n            #\n            # This is where the magic happens\n            # Model takes our prompt and produces new tokens\n            #\n            # HOW IT WORKS (simplified):\n            #   1. Encode prompt: \"Hello\" -> [15496]\n            #   2. Feed through model, get probability distribution over vocab\n            #   3. Sample next token from distribution: \"there\" -> [15496, 727]\n            #   4. Feed [15496, 727] through model, get next distribution\n            #   5. Sample next token: [15496, 727, 11]\n            #   6. Repeat until </s> token or max_new_tokens reached\n            #\n            # Each step, model predicts ONE token based on ALL previous tokens\n            \n            **inputs,\n            # **inputs = unpack the dictionary\n            # Same as: input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask']\n            # Just cleaner syntax\n            \n            max_new_tokens=max_new_tokens,\n            # Maximum number of NEW tokens to generate\n            # New = tokens after the prompt\n            #\n            # We pass this in as parameter (default 150)\n            # Prompt might be 50 tokens, total output up to 50+150=200 tokens\n            #\n            # WHY LIMIT:\n            #   Model could ramble forever\n            #   Need a stopping condition\n            #   Also prevents hanging if model doesn't produce </s>\n            #\n            # 150 tokens ≈ 100-120 words ≈ a decent paragraph\n            \n            do_sample=True,\n            # SAMPLING vs GREEDY\n            #\n            # do_sample=False (GREEDY):\n            #   Always pick the highest probability token\n            #   \"What's most likely next? Pick that.\"\n            #   Deterministic: same input = same output every time\n            #   Problem: boring, repetitive, gets stuck in loops\n            #\n            # do_sample=True (SAMPLING):\n            #   Randomly sample from probability distribution\n            #   Higher prob tokens more likely, but not guaranteed\n            #   Stochastic: same input = different outputs\n            #   More creative, more varied, more natural\n            #\n            # For chat: ALWAYS use sampling (True)\n            # For code/facts: might use greedy (False)\n            \n            temperature=0.7,\n            # TEMPERATURE - controls randomness of sampling\n            #\n            # HOW IT WORKS:\n            #   Probabilities are scaled by 1/temperature before sampling\n            #   Mathematically: softmax(logits / temperature)\n            #\n            # temperature=1.0 (neutral):\n            #   Use raw probabilities as-is\n            #   \n            # temperature < 1.0 (e.g., 0.7):\n            #   \"Sharpen\" the distribution\n            #   High-prob tokens become even higher prob\n            #   Low-prob tokens become even lower prob\n            #   Result: more focused, predictable outputs\n            #\n            # temperature > 1.0 (e.g., 1.5):\n            #   \"Flatten\" the distribution\n            #   All tokens become more equally likely\n            #   Result: more random, creative, chaotic outputs\n            #\n            # VISUAL:\n            #   Low temp (0.3):  [0.9, 0.05, 0.03, 0.02] - very confident\n            #   Neutral (1.0):   [0.5, 0.25, 0.15, 0.10] - normal\n            #   High temp (1.5): [0.35, 0.25, 0.22, 0.18] - uncertain\n            #\n            # WHY 0.7:\n            #   Slightly focused - good balance\n            #   Creative but not crazy\n            #   Industry standard for chat applications\n            #   ChatGPT uses around 0.7-0.9\n            \n            top_p=0.9,\n            # TOP-P (NUCLEUS) SAMPLING\n            #\n            # After temperature, ANOTHER filter on which tokens to consider\n            #\n            # HOW IT WORKS:\n            #   1. Sort tokens by probability (highest first)\n            #   2. Add probabilities until you reach top_p (0.9 = 90%)\n            #   3. Only sample from those tokens\n            #   4. Ignore the bottom 10% (long tail of unlikely tokens)\n            #\n            # EXAMPLE:\n            #   Tokens sorted: [0.3, 0.25, 0.2, 0.15, 0.05, 0.03, 0.02]\n            #   top_p=0.9\n            #   Sum: 0.3+0.25+0.2+0.15=0.9 (stop here!)\n            #   Only sample from first 4 tokens\n            #   Bottom 3 (0.05, 0.03, 0.02) are ignored\n            #\n            # WHY IT HELPS:\n            #   Removes garbage tokens (very low probability)\n            #   Even with sampling, won't pick nonsense\n            #   Adaptive: uses more tokens when uncertain, fewer when confident\n            #\n            # WHY 0.9:\n            #   Keep 90% of probability mass\n            #   Removes only the weirdest options\n            #   Standard value, works well\n            \n            top_k=50,\n            # TOP-K SAMPLING\n            #\n            # ANOTHER filter (can use with or without top_p)\n            #\n            # HOW IT WORKS:\n            #   Only consider the top K most likely tokens\n            #   Ignore everything else\n            #\n            # EXAMPLE:\n            #   top_k=50\n            #   Only sample from the 50 most likely next tokens\n            #   Ignore the other 31,950 tokens in vocabulary\n            #\n            # WHY IT HELPS:\n            #   Hard cutoff on number of choices\n            #   Guarantees we never pick something super weird\n            #\n            # TOP_K vs TOP_P:\n            #   top_k: fixed number of tokens (always exactly K)\n            #   top_p: variable number (however many to reach P probability)\n            #   \n            #   Using both: tokens must pass BOTH filters\n            #   Top 50 AND contribute to top 90% probability\n            #\n            # WHY 50:\n            #   Reasonable variety without too much chaos\n            #   Some use 40, some use 100\n            #   50 is a safe middle ground\n            \n            pad_token_id=tokenizer.pad_token_id,\n            # Tell model which token ID is padding\n            #\n            # Remember: we set pad_token = eos_token earlier\n            # So pad_token_id = 2 (same as </s>)\n            #\n            # WHY NEEDED:\n            #   Generation function needs to know about special tokens\n            #   If not provided, might get warning or weird behavior\n            \n            eos_token_id=tokenizer.eos_token_id,\n            # Tell model which token ID means \"stop generating\"\n            #\n            # eos_token_id = 2 (the </s> token)\n            #\n            # HOW IT'S USED:\n            #   Model generates token by token\n            #   If it generates token 2 (</s>), STOP\n            #   \"End of sequence - I'm done talking\"\n            #\n            # Without this:\n            #   Model might not know when to stop\n            #   Could ramble until max_new_tokens\n            \n            repetition_penalty=1.2,\n            # PENALIZE REPEATING TOKENS\n            #\n            # HOW IT WORKS:\n            #   If a token already appeared, reduce its probability\n            #   penalty=1.0: no penalty\n            #   penalty=1.2: tokens that appeared before are 1.2x less likely\n            #   penalty=2.0: much stronger penalty\n            #\n            # WHY IT HELPS:\n            #   Without penalty, model might loop:\n            #   \"I think I think I think I think I think...\"\n            #   \"The the the the the the the...\"\n            #\n            #   With penalty, repeating tokens become less likely\n            #   Forces model to use varied vocabulary\n            #\n            # WHY 1.2:\n            #   Mild penalty - reduces repetition without killing natural patterns\n            #   Some repetition is normal (\"I am happy to help you\")\n            #   We don't want to ban all repetition, just excessive loops\n            #\n            # Common values: 1.0 (off), 1.1, 1.2, 1.5\n            \n            no_repeat_ngram_size=3,\n            # HARD BAN ON REPEATING N-GRAMS\n            #\n            # n-gram = sequence of n tokens\n            # 3-gram = sequence of 3 tokens\n            #\n            # HOW IT WORKS:\n            #   Track all 3-token sequences that appeared\n            #   If a 3-gram would repeat, set its probability to 0\n            #   CANNOT repeat ANY 3-word phrase\n            #\n            # EXAMPLE:\n            #   Generated so far: \"I am happy to help you\"\n            #   3-grams seen: [\"I am happy\", \"am happy to\", \"happy to help\", \"to help you\"]\n            #   \n            #   Next token prediction:\n            #   If model wants to generate \"help you\" after \"to\"...\n            #   That would create \"to help you\" again\n            #   But that 3-gram already exists!\n            #   So \"you\" is BLOCKED after \"to help\"\n            #\n            # WHY 3:\n            #   2-gram: too strict (many 2-word phrases naturally repeat)\n            #   3-gram: good balance (blocks \"I think I think\" patterns)\n            #   4-gram: might allow some repetitive loops\n            #\n            # COMBINED WITH repetition_penalty:\n            #   repetition_penalty: soft penalty (reduces probability)\n            #   no_repeat_ngram_size: hard ban (zero probability)\n            #   Together = very effective at preventing repetition\n        )\n\n    text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    # DECODE TOKENS BACK TO TEXT\n    #\n    # outputs = tensor of token IDs\n    #   Shape: [1, sequence_length]\n    #   The 1 is batch dimension (we only generated 1 sequence)\n    #\n    # outputs[0] = get the first (only) sequence\n    #   Shape: [sequence_length]\n    #   Just the token IDs\n    #\n    # tokenizer.decode(...) = convert token IDs to string\n    #   [1, 529, 29989, ...] -> \"<|system|>\\nYou are a helpful...\"\n    #\n    # skip_special_tokens=False\n    #   Keep special tokens like </s>, <|system|>, etc.\n    #   We want to see them so we can parse the response\n    #\n    #   If skip_special_tokens=True:\n    #     Special tokens removed from output\n    #     Harder to find where assistant response starts\n    #\n    # text now contains the FULL generated text:\n    #   \"<|system|>\\n...\\n<|user|>\\n{question}</s>\\n<|assistant|>\\n{response}</s>\"\n    #   Prompt + generated response all together\n\n    # Extract response after <|assistant|>\n    if \"<|assistant|>\" in text:\n        response = text.split(\"<|assistant|>\")[-1]\n        response = response.replace(\"</s>\", \"\").strip()\n    else:\n        response = text\n    # EXTRACT JUST THE RESPONSE\n    #\n    # We don't want to return the whole thing:\n    #   \"<|system|>\\n...\\n<|user|>\\nHi</s>\\n<|assistant|>\\nHello!</s>\"\n    #\n    # We just want:\n    #   \"Hello!\"\n    #\n    # HOW WE EXTRACT IT:\n    #\n    # if \"<|assistant|>\" in text:\n    #   Check if the marker exists (it should!)\n    #   Safety check in case something weird happened\n    #\n    # text.split(\"<|assistant|>\")\n    #   Split string at \"<|assistant|>\" marker\n    #   Before: \"...<|user|>\\nHi</s>\\n<|assistant|>\\nHello!</s>\"\n    #   After: [\"...<|user|>\\nHi</s>\\n\", \"\\nHello!</s>\"]\n    #   Returns list of parts\n    #\n    # [-1]\n    #   Get the LAST element (index -1 in Python)\n    #   That's everything AFTER the marker\n    #   \"\\nHello!</s>\"\n    #\n    # .replace(\"</s>\", \"\")\n    #   Remove the end-of-sequence token\n    #   \"\\nHello!</s>\" -> \"\\nHello!\"\n    #\n    # .strip()\n    #   Remove whitespace from both ends\n    #   \"\\nHello!\" -> \"Hello!\"\n    #\n    # else: response = text\n    #   If no <|assistant|> found (shouldn't happen), return everything\n    #   Fallback for weird edge cases\n    #\n    # RESULT:\n    #   Clean response string: \"Hello!\"\n    #   No special tokens, no prompt, just the answer\n\n    return response\n    # Return the clean response string\n    # Caller can print it, store it, whatever they want\n\nprint(\"Ready to test!\")\n# Just confirms function was defined without errors\n#\n# The function doesn't DO anything until we call it\n# Next cell will call it with test questions\n#\n# OUTPUT: \"Ready to test!\"\n#\n# If there was a syntax error in the function, we'd see it here\n# Success = function is ready to use\n\n# ============================================================\n# GENERATION PARAMETERS SUMMARY\n# ============================================================\n#\n# WHAT WE'RE USING:\n#   max_new_tokens=150     (limit response length)\n#   do_sample=True         (use sampling, not greedy)\n#   temperature=0.7        (slightly focused randomness)\n#   top_p=0.9              (consider top 90% probability mass)\n#   top_k=50               (consider top 50 tokens max)\n#   repetition_penalty=1.2 (soft penalty on repeats)\n#   no_repeat_ngram_size=3 (hard ban on 3-gram repeats)\n#\n# THIS GIVES US:\n#   - Creative but coherent responses\n#   - No repetitive loops\n#   - Natural-sounding text\n#   - Reasonable length\n#\n# ============================================================\n# ALTERNATIVE SETTINGS FOR DIFFERENT NEEDS\n# ============================================================\n#\n# MORE CREATIVE (brainstorming, stories):\n#   temperature=1.0\n#   top_p=0.95\n#   top_k=100\n#\n# MORE FOCUSED (factual, consistent):\n#   temperature=0.3\n#   top_p=0.8\n#   top_k=30\n#\n# DETERMINISTIC (exactly reproducible):\n#   do_sample=False\n#   (ignores temperature, top_p, top_k)\n#\n# ============================================================\n# COMMON ISSUES AND FIXES\n# ============================================================\n#\n# ISSUE: Repetitive output (\"I think I think I think\")\n# FIX: Increase repetition_penalty to 1.3-1.5\n#      Or increase no_repeat_ngram_size to 4\n#\n# ISSUE: Too random/incoherent\n# FIX: Lower temperature (0.5)\n#      Lower top_p (0.7)\n#\n# ISSUE: Too boring/repetitive content\n# FIX: Higher temperature (0.9)\n#      Higher top_p (0.95)\n#\n# ISSUE: Cuts off mid-sentence\n# FIX: Increase max_new_tokens\n#\n# ISSUE: Doesn't stop, rambles forever\n# FIX: Check eos_token_id is set correctly\n#      Model might not have learned to produce </s>","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:19:30.208175Z","iopub.execute_input":"2025-12-30T16:19:30.208375Z","iopub.status.idle":"2025-12-30T16:19:30.215088Z","shell.execute_reply.started":"2025-12-30T16:19:30.208354Z","shell.execute_reply":"2025-12-30T16:19:30.214326Z"}},"outputs":[{"name":"stdout","text":"Ready to test!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# TEST THE MODEL!\n\ntest_questions = [\n    \"Hi\",\n    \"What are some ways to manage anxiety?\",\n    \"I've been feeling stressed at work. Any suggestions?\",\n    \"How can I improve my sleep?\",\n    \"I'm feeling sad today\",\n]\n\nprint(\"Testing fine-tuned model:\")\nprint(\"=\" * 60)\n\nfor i, q in enumerate(test_questions, 1):\n    print(f\"\\nQ{i}: {q}\")\n    print(\"-\" * 40)\n    response = generate_response(model, tokenizer, q)\n    print(f\"A: {response}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:19:30.216527Z","iopub.execute_input":"2025-12-30T16:19:30.216736Z","iopub.status.idle":"2025-12-30T16:19:58.186962Z","shell.execute_reply.started":"2025-12-30T16:19:30.216715Z","shell.execute_reply":"2025-12-30T16:19:58.186365Z"}},"outputs":[{"name":"stdout","text":"Testing fine-tuned model:\n============================================================\n\nQ1: Hi\n----------------------------------------\nA: Hello there. Tell me how are you feeling today?\n\nQ2: What are some ways to manage anxiety?\n----------------------------------------\nA: 1. Exercise: Regular physical activity can help reduce feelings of anxiety by increasing endorphins in the brain.\n2. Stay hydrated: Drinking water helps regulate moods.\n3. Get enough sleep: Lack of sleep can cause anxiety.\n4. Eat well: A balanced diet with plenty of fiber, vitamins, minerals, and healthy fats can improve digestion and promote good nutrient absorption which can aid in reducing symptoms of anxietiy.\n5. Practice mindfulness: Pay attention to what you're doing without judgment. Focus on your breath or other sensory experiences. This practice can\n\nQ3: I've been feeling stressed at work. Any suggestions?\n----------------------------------------\nA: How long have you been feeling this way? What else is on your mind?\n\nQ4: How can I improve my sleep?\n----------------------------------------\nA: Maintaining regular sleep patterns is crucial for good physical and emotional well-being. It's important to establish a consistent sleep schedule so that you go to bed at the same time each night and wake up at the appropriate hour every day. Keeping a routine in your life will help regulate the rhythm of your body's natural sleep/wake cycle. Avoid caffeine or alcohol close to bedtime as they may interfere with the production of melatonin. Also, avoid using electronics an hour before going to bed. Increasing consumption of sleepy foods like chocolate, cheese, nuts, etc. Is also not recommended because it can prolong the\n\nQ5: I'm feeling sad today\n----------------------------------------\nA: What do you think is causing this?\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Chapter 11: Load Model Later","metadata":{}},{"cell_type":"code","source":"# HOW TO LOAD YOUR MODEL LATER\n#\n# You trained the model, saved the LoRA adapters, now the session ends\n# Tomorrow you come back and want to USE the model\n# Or you want to use it on a different machine\n# Or share it with a friend\n#\n# THIS IS HOW YOU DO IT\n#\n# THE PROCESS:\n#   1. Load the base model (TinyLlama) - download from HuggingFace or cache\n#   2. Load YOUR LoRA adapters - from the folder you saved\n#   3. Combine them - PeftModel does this automatically\n#   4. Done! Use generate_response() like before\n#\n# WHY TWO STEPS:\n#   We only saved the adapters (~200 MB), not full model (2.2 GB)\n#   Base model lives on HuggingFace Hub\n#   We download base once, then apply our custom adapters\n#   Like downloading Photoshop once, then loading different edit files\n\nfrom peft import PeftModel\n# PeftModel = the wrapper class that combines base model + adapters\n#\n# We imported other peft stuff earlier (LoraConfig, get_peft_model)\n# PeftModel is specifically for LOADING saved adapters\n#\n# get_peft_model: add NEW adapters to a model (training time)\n# PeftModel.from_pretrained: load SAVED adapters (inference time)\n\ndef load_model(base_model_name, lora_path):\n    # A reusable function to load your fine-tuned model\n    #\n    # PARAMETERS:\n    #   base_model_name: HuggingFace model ID\n    #                    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    #\n    #   lora_path: where you saved the LoRA adapters\n    #              \"./fine_tuned_lora\" or any path\n    #\n    # RETURNS:\n    #   model: ready-to-use model with LoRA adapters applied\n    #   tokenizer: matching tokenizer\n    #\n    # USAGE:\n    #   model, tokenizer = load_model(\"TinyLlama/...\", \"./fine_tuned_lora\")\n    #   response = generate_response(model, tokenizer, \"Hi!\")\n    \n    \"\"\"Load base model with LoRA adapters.\"\"\"\n    # Docstring - short description of what function does\n    \n    tokenizer = AutoTokenizer.from_pretrained(lora_path)\n    # LOAD TOKENIZER FROM YOUR SAVED FOLDER\n    #\n    # Remember: we saved tokenizer alongside LoRA adapters\n    # lora_path contains:\n    #   - adapter_config.json\n    #   - adapter_model.safetensors\n    #   - tokenizer.json         <- loading this!\n    #   - tokenizer_config.json  <- and this!\n    #   - special_tokens_map.json\n    #\n    # WHY FROM lora_path (not base_model_name)?\n    #   We saved our exact tokenizer configuration\n    #   Guaranteed to match what we trained with\n    #   No risk of version mismatch\n    #\n    #   Could also do: AutoTokenizer.from_pretrained(base_model_name)\n    #   Would work, but using saved copy is safer\n    \n    base_model = AutoModelForCausalLM.from_pretrained(\n        # LOAD THE BASE MODEL (same as during training)\n        #\n        # This is the ORIGINAL TinyLlama, no fine-tuning\n        # Same model everyone downloads\n        # Our customization comes from LoRA adapters (loaded separately)\n        \n        base_model_name,\n        # \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n        #\n        # First time: downloads from HuggingFace Hub (~2.2 GB)\n        # After that: loads from local cache (fast!)\n        #\n        # Cache location: ~/.cache/huggingface/hub/\n        # The heavy download only happens once per machine\n        \n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.bfloat16,\n            bnb_4bit_use_double_quant=True,\n        ),\n        # SAME QUANTIZATION CONFIG AS TRAINING\n        #\n        # We trained with 4-bit quantization\n        # We should load with 4-bit quantization\n        # Keeps memory usage low (~0.7 GB)\n        #\n        # COULD YOU LOAD WITHOUT QUANTIZATION?\n        #   Yes! Load in fp16 or fp32\n        #   Would use more memory but might be slightly better quality\n        #   For inference, 4-bit is usually fine\n        #\n        # THE SETTINGS:\n        #   load_in_4bit=True         - use 4-bit weights\n        #   bnb_4bit_quant_type=\"nf4\" - NormalFloat4 format\n        #   bnb_4bit_compute_dtype=torch.bfloat16 - math in bf16\n        #   bnb_4bit_use_double_quant=True - extra compression\n        #\n        # Same as training - consistency is key\n        \n        device_map=\"auto\",\n        # Automatically put model on GPU (or CPU if no GPU)\n        # Same as training\n        \n        torch_dtype=torch.bfloat16,\n        # Computation precision\n        # Same as training\n    )\n    # NOW WE HAVE: base TinyLlama model, quantized, on GPU\n    # But it's NOT fine-tuned yet - just vanilla TinyLlama\n    # The magic happens in the next step...\n    \n    model = PeftModel.from_pretrained(base_model, lora_path)\n    # COMBINE BASE MODEL + LORA ADAPTERS\n    #\n    # THIS IS THE KEY STEP!\n    #\n    # PeftModel.from_pretrained():\n    #   Takes: base model + path to saved adapters\n    #   Does: loads adapters from disk, attaches to base model\n    #   Returns: combined model ready for inference\n    #\n    # WHAT IT LOADS FROM lora_path:\n    #   adapter_config.json - LoRA settings (r=64, alpha=128, etc.)\n    #   adapter_model.safetensors - the actual trained weights (~200MB)\n    #\n    # WHAT HAPPENS INTERNALLY:\n    #   1. Read adapter_config.json to know LoRA structure\n    #   2. Create empty LoRA matrices matching that config\n    #   3. Load trained weights from adapter_model.safetensors\n    #   4. Attach LoRA layers to base model's target modules\n    #   5. Return wrapped PeftModel\n    #\n    # RESULT:\n    #   base_model: vanilla TinyLlama (frozen, unchanged)\n    #   model: TinyLlama + YOUR trained adapters\n    #\n    # model = base_model + your mental health fine-tuning\n    # When you generate text, LoRA adapters modify the output\n    # You get mental health responses, not generic ones!\n    \n    model.eval()\n    # SET MODEL TO EVALUATION MODE\n    #\n    # PyTorch models have two modes:\n    #   .train() - training mode\n    #   .eval()  - evaluation/inference mode\n    #\n    # WHAT'S DIFFERENT IN EVAL MODE:\n    #   1. Dropout is DISABLED\n    #      Training: randomly drop 10% of connections (regularization)\n    #      Inference: use ALL connections (best predictions)\n    #\n    #   2. BatchNorm uses running statistics\n    #      (Not relevant for LLMs, but matters for other models)\n    #\n    #   3. Some layers behave differently\n    #      Ensures deterministic, optimal inference\n    #\n    # IMPORTANT:\n    #   We're loading for INFERENCE, not more training\n    #   .eval() ensures we get best predictions\n    #   \n    # COMMON MISTAKE:\n    #   Forgetting .eval() when loading for inference\n    #   Model still works but dropout is active\n    #   Slightly worse and non-deterministic outputs\n    #\n    # ALWAYS call .eval() when loading for inference!\n    \n    return model, tokenizer\n    # Return both model and tokenizer\n    #\n    # WHY RETURN BOTH:\n    #   You need both to generate text\n    #   tokenizer: convert question to tokens\n    #   model: generate response tokens\n    #   tokenizer: convert response back to text\n    #\n    #   Convenient to get both from one function call\n    #\n    # USAGE:\n    #   model, tokenizer = load_model(...)\n    #   response = generate_response(model, tokenizer, \"Hi!\")\n\nprint(\"To load later:\")\nprint(f\"model, tokenizer = load_model('{model_name}', './fine_tuned_lora')\")\n# Show the exact command to use later\n#\n# OUTPUT:\n#   To load later:\n#   model, tokenizer = load_model('TinyLlama/TinyLlama-1.1B-Chat-v1.0', './fine_tuned_lora')\n#\n# Copy-paste this command into a new notebook!\n# (After defining the load_model function of course)\n#\n# WHY PRINT model_name:\n#   You might forget which base model you used\n#   This saves you from guessing later\n#   The base model MUST match what you trained with!\n\n# ============================================================\n# COMPLETE WORKFLOW FOR USING SAVED MODEL\n# ============================================================\n#\n# IN A NEW NOTEBOOK OR SCRIPT:\n#\n# Step 1: Imports\n#   import torch\n#   from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n#   from peft import PeftModel\n#\n# Step 2: Define load_model function (copy from above)\n#\n# Step 3: Define generate_response function (copy from earlier)\n#\n# Step 4: Load the model\n#   model, tokenizer = load_model(\n#       \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n#       \"./fine_tuned_lora\"  # or wherever you saved it\n#   )\n#\n# Step 5: Use it!\n#   response = generate_response(model, tokenizer, \"How do I manage anxiety?\")\n#   print(response)\n#\n# ============================================================\n# LOADING FROM DIFFERENT LOCATIONS\n# ============================================================\n#\n# SAME MACHINE, SAME FOLDER:\n#   lora_path = \"./fine_tuned_lora\"\n#   (what we've been using)\n#\n# SAME MACHINE, DIFFERENT FOLDER:\n#   lora_path = \"/home/user/my_models/mental_health_lora\"\n#   (just change the path)\n#\n# DIFFERENT MACHINE:\n#   1. Copy the fine_tuned_lora folder to new machine\n#   2. Use the path where you put it\n#   lora_path = \"/path/on/new/machine/fine_tuned_lora\"\n#\n# FROM GOOGLE DRIVE (Colab):\n#   from google.colab import drive\n#   drive.mount('/content/drive')\n#   lora_path = \"/content/drive/MyDrive/fine_tuned_lora\"\n#\n# FROM HUGGINGFACE HUB (if you uploaded):\n#   lora_path = \"your-username/mental-health-tinyllama-lora\"\n#   (works just like a model name!)\n#\n# ============================================================\n# ALTERNATIVE: MERGE LORA INTO BASE MODEL\n# ============================================================\n#\n# Instead of loading base + adapters separately every time,\n# you can MERGE them into one model:\n#\n#   # Load as usual\n#   model = PeftModel.from_pretrained(base_model, lora_path)\n#   \n#   # Merge LoRA into base weights\n#   merged_model = model.merge_and_unload()\n#   \n#   # Save the merged model\n#   merged_model.save_pretrained(\"./merged_model\")\n#   tokenizer.save_pretrained(\"./merged_model\")\n#\n# NOW:\n#   - merged_model is a regular model (no LoRA wrapper)\n#   - File size: ~2.2 GB (full model, not just adapters)\n#   - Load with just AutoModelForCausalLM.from_pretrained()\n#   - No need for PeftModel\n#\n# PROS OF MERGING:\n#   - Simpler loading (one step, not two)\n#   - Slightly faster inference (no LoRA overhead)\n#   - Works with tools that don't support PEFT\n#\n# CONS OF MERGING:\n#   - Much larger file size (2.2 GB vs 200 MB)\n#   - Can't easily swap adapters anymore\n#   - Loses the flexibility of LoRA\n#\n# For most cases, keeping adapters separate is better!\n#\n# ============================================================\n# TROUBLESHOOTING\n# ============================================================\n#\n# ERROR: \"Can't find adapter_config.json\"\n#   The lora_path is wrong, or files weren't saved correctly\n#   Check: os.listdir(lora_path) - do you see the files?\n#\n# ERROR: \"Model architecture mismatch\"\n#   You're using a different base_model_name than you trained with\n#   Must use exact same model: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n#\n# ERROR: \"CUDA out of memory\"\n#   GPU doesn't have enough RAM\n#   Make sure quantization_config is included (4-bit loading)\n#   Or use device_map=\"cpu\" (slower but always works)\n#\n# GARBAGE OUTPUT:\n#   Are you using the same prompt format as training?\n#   Same </s> tokens after each section?\n#   Check generate_response function matches training format!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-30T16:19:58.187926Z","iopub.execute_input":"2025-12-30T16:19:58.188168Z","iopub.status.idle":"2025-12-30T16:19:58.193748Z","shell.execute_reply.started":"2025-12-30T16:19:58.188146Z","shell.execute_reply":"2025-12-30T16:19:58.193158Z"}},"outputs":[{"name":"stdout","text":"To load later:\nmodel, tokenizer = load_model('TinyLlama/TinyLlama-1.1B-Chat-v1.0', './fine_tuned_lora')\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Done! You Just Fine-Tuned an LLM\n\n### That's 1.1 billion parameters trained to do YOUR bidding. Not bad for a day's work.\n\n---\n\n## What You Actually Accomplished\n\nLet's take a step back and appreciate what just happened:\n```\nBEFORE THIS NOTEBOOK:\n- TinyLlama: A generic chatbot that gives Wikipedia-style responses\n- You: \"What's LoRA? What's quantization? Why is my GPU crying?\"\n\nAFTER THIS NOTEBOOK:\n- Your Model: A mental health assistant that gives empathetic, supportive responses\n- You: \"I understand every line of code and can adapt this to any dataset\"\n```\n\nYou didn't just copy-paste code. You actually learned:\n\n### 1. The Concepts\n\n| Concept | What It Is | Why It Matters |\n|---------|-----------|----------------|\n| **Fine-tuning** | Teaching a pre-trained model YOUR specific task | Makes generic models useful for YOUR use case |\n| **LoRA** | Train only ~4% of parameters (freeze the rest) | 25x less memory for gradients/optimizer |\n| **Quantization** | Compress weights from 32-bit to 4-bit | 8x smaller model fits on cheap GPUs |\n| **Prompt Format** | The exact template the model expects | Wrong format = garbage output (THE golden rule) |\n\n### 2. The Code\n\nEvery cell in this notebook, you now understand:\n- WHAT it does\n- WHY it's there\n- What BREAKS if you change it\n- How to ADAPT it for your own use case\n\nThat's the difference between \"tutorial follower\" and \"practitioner.\"\n\n### 3. The Workflow\n```\nThe Fine-Tuning Recipe (now burned into your brain):\n\n1. GET DATA\n   - Format as {'question': '...', 'answer': '...'}\n   - More data = better model (usually)\n\n2. LOAD BASE MODEL  \n   - Pick a model (TinyLlama, Mistral, Llama, etc.)\n   - Apply 4-bit quantization (BitsAndBytesConfig)\n   - Prepare for training (prepare_model_for_kbit_training)\n\n3. ADD LORA\n   - Configure: rank, alpha, target modules\n   - Apply: get_peft_model()\n   - Now only ~4% of params are trainable\n\n4. FORMAT DATA (CRITICAL!)\n   - Must match model's expected format EXACTLY\n   - TinyLlama: <|system|>...</s><|user|>...</s><|assistant|>...</s>\n   - Miss one </s> = garbage output\n\n5. TRAIN\n   - Set hyperparameters (epochs, LR, batch size)\n   - Create SFTTrainer\n   - trainer.train() and watch the loss go down\n\n6. SAVE\n   - Save LoRA adapters (~200 MB, not 2+ GB)\n   - Save tokenizer too (for convenience)\n\n7. USE\n   - Load base model + LoRA adapters\n   - Use SAME format as training\n   - Generate responses\n```\n\nThis same workflow works for ANY model, ANY dataset, ANY task.\n\n---\n\n## Quick Reference: Common Issues and Fixes\n\nBookmark this. You'll need it.\n\n### Problem: Model outputs garbage (random tokens, HTML tags, repetitive nonsense)\n```\nCause:    99% of the time, your inference format doesn't match training format\nFix:      Check your prompt template character by character\n          - </s> after system message?\n          - </s> after user message?\n          - Same special tokens?\n          - Same whitespace/newlines?\n```\n\n### Problem: Loss stays high or spikes randomly\n```\nCause:    Learning rate too high\nFix:      Lower it: 2e-4 --> 1e-4 --> 5e-5 --> 2e-5\n          Start conservative, increase if training is too slow\n```\n\n### Problem: Training loss goes down, but validation loss goes UP\n```\nCause:    Overfitting (model memorizing, not learning)\nFix:      - Reduce epochs: 5 --> 3 --> 2\n          - Increase dropout: 0.1 --> 0.15 --> 0.2\n          - Increase weight_decay: 0.05 --> 0.1\n          - Get more training data (best fix!)\n```\n\n### Problem: CUDA out of memory\n```\nCause:    GPU doesn't have enough RAM\nFix:      - Reduce batch size: 4 --> 2 --> 1\n          - Reduce max_seq_length: 512 --> 256 --> 128\n          - Make sure quantization is enabled\n          - Make sure gradient_checkpointing is True\n          - Reduce LoRA rank: 64 --> 32 --> 16\n```\n\n### Problem: Model repeats itself endlessly\n```\nCause:    Generation parameters not tuned\nFix:      - Add repetition_penalty=1.2\n          - Add no_repeat_ngram_size=3\n          - Lower temperature: 0.9 --> 0.7\n```\n\n---\n\n## Where To Go From Here\n\nYou've got the foundation. Here's how to level up:\n\n### Level 1: Try Your Own Dataset\n\nThe mental health dataset was just an example. You can fine-tune for:\n\n- **Customer Support:** Your company's support tickets\n- **Sales:** Your sales call transcripts  \n- **Legal:** Legal Q&A pairs\n- **Medical:** Medical conversations (be careful with this one)\n- **Creative Writing:** Stories in a specific style\n- **Code:** Your codebase + documentation\n- **Anything:** If you have question-answer pairs, you can fine-tune\n\nJust format your data as:\n```python\n{'question': 'user input here', 'answer': 'desired output here'}\n```\n\nUse the same code, swap the dataset. Done.\n\n### Level 2: Experiment with LoRA Settings\n\nWe used `r=64`, but you can experiment:\n```\nr=8    --> Very efficient, limited capacity\n          Good for: Simple style changes\n          Trainable params: ~12M\n\nr=16   --> Efficient, decent capacity  \n          Good for: Light adaptation\n          Trainable params: ~25M\n\nr=32   --> Balanced\n          Good for: Learning new patterns\n          Trainable params: ~35M\n\nr=64   --> High capacity (what we used)\n          Good for: Learning new domain knowledge\n          Trainable params: ~50M\n\nr=128  --> Very high capacity\n          Good for: Complex tasks\n          Trainable params: ~100M\n```\n\nStart with r=16. If results are bad, increase. Find the minimum that works.\n\n### Level 3: Try Bigger Models\n\nTinyLlama (1.1B) is great for learning, but bigger = better:\n```\nIf you have 16GB GPU (T4, P100):\n  - Phi-2 (2.7B) - surprisingly capable\n  - Mistral-7B with aggressive settings\n  \nIf you have 24GB GPU (RTX 3090/4090):\n  - Mistral-7B comfortably\n  - Llama-2-7B\n  - Llama-2-13B with quantization\n\nIf you have 40GB+ GPU (A100):\n  - Pretty much anything\n  - Llama-2-70B with quantization\n```\n\nSame code, just change:\n```python\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\n# or\nmodel_name = \"meta-llama/Llama-2-7b-hf\"\n```\n\n### Level 4: Learn More Techniques\n\nOnce you're comfortable with basic fine-tuning:\n\n- **DPO (Direct Preference Optimization):** Train on preference data (A is better than B)\n- **RLHF:** Reinforcement Learning from Human Feedback (how ChatGPT was trained)\n- **Merging:** Combine multiple LoRA adapters\n- **Quantization-Aware Training:** Train in low precision from the start\n\nBut master the basics first. Everything else builds on what you learned today.\n\n---\n\n## The Golden Rule (One Last Time)\n\nI know I've said it a hundred times, but I'll say it once more:\n```\n+------------------------------------------------------------------+\n|                                                                  |\n|   TRAINING FORMAT  ====  INFERENCE FORMAT                        |\n|                                                                  |\n|   They must match EXACTLY.                                       |\n|                                                                  |\n|   This is the #1 cause of \"my fine-tuned model doesn't work.\"    |\n|   Check it first. Check it always. Check it when in doubt.       |\n|                                                                  |\n+------------------------------------------------------------------+\n```\n\n---\n\n## Final Words\n\nYou started this notebook wondering what fine-tuning even is.\n\nYou're ending it with:\n- A working fine-tuned model\n- Understanding of LoRA and quantization\n- Knowledge of the entire fine-tuning workflow\n- Ability to adapt this to any dataset or model\n\nThat's real progress. That's a real skill.\n\nNow go build something cool with it.\n\n**If this notebook helped you, please upvote it!** It helps others find it too.\n\nGood luck out there.\n\n---\n\n*P.S. - When your fine-tuned model inevitably outputs garbage the first time you try it on your own dataset, remember: check the prompt format. It's always the prompt format.*","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}